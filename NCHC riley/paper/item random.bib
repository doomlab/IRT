@misc{Aust2017,
author = {Aust, F. and Barth, Marius},
title = {{papaja: Create APA manuscripts with R Markdown.}},
url = {https://github.com/crsh/papaja},
year = {2017}
}
@article{Babbie1982,
abstract = {The total design method (TDM) : a new approach to mail and telephone surveys Which is best : the advantages and disadvantages of mail, telephone, and face-to-face surveys Writing questions: some general principles Constructing mail questionnaires Implementing mail surveys Constructing telephone questionnaires Implementing telephone surveys Looking to the future : prospects and concerns.},
author = {Babbie, Earl and Dillman, Don A.},
doi = {10.2307/2068433},
isbn = {0471215554},
issn = {00943061},
journal = {Contemporary Sociology},
month = {sep},
number = {5},
pages = {561},
pmid = {2362428},
title = {{Mail and Telephone Surveys: The Total Design Method.}},
url = {http://www.jstor.org/stable/2068433?origin=crossref},
volume = {11},
year = {1982}
}
@article{Barak2002,
abstract = {The study examined the quality and utility of online administration of the Self-Directed Search (SDS) to high school students in a career exploration workshop. An Internet-based SDS version was designed; it included automatic scoring and immediate feedback to test takers, providing a three-letter occupational code along with verbal explanation of the results. SDS was taken by participants in their homes or in school (n = 77), and compared to traditional administration of the SDS (n = 73), either self-scored or counselor scored. The mean internal consistency coefficient for the SDS was .90, and mean test-retest reliability (over a 6- week period) was .94. Mean correlation between the same SDS scales of participants who took both versions was .77. For this latter group, the Realistic, Social, and Enterprising scales were found to be higher in the online administration than in the paper-and-pencil administration, whereas the three other scales were not statistically different. Similar to the traditional version and consistent with previous research, the online version produced the configuration of a RIASEC-order hexagon. Participants who took the online version were more satisfied with it than those who took the paper-and-pencil version. These results highly support the further use of the online version of the SDS.},
author = {Barak, A. and Cohen, L.},
doi = {10.1177/1069072702238402},
isbn = {1069072702},
issn = {1069-0727},
journal = {Journal of Career Assessment},
keywords = {along with a changing,as well as,career,culture,emerging changes have created,even revolutionary means to,have,internet,introduced new,online,positive opportunities and prospects,promote career psychology,recent technological developments,self-directed search,testing,these},
number = {4},
pages = {387--400},
pmid = {351},
title = {{Empirical examination of an online version of the self-directed search}},
url = {https://doi.org/10.1177/1069072702238402},
volume = {10},
year = {2002}
}
@article{Barger2002,
abstract = {The Marlowe-Crowne Social Desirability scale (Crowne {\&} Marlowe, 1960) is widely used to assess and control for response bias in self-report research. Several abbreviated versions of the Marlowe-Crowne scale have been proposed and adopted in psychology and medicine. In this article I evaluate the adequacy of 9 short forms using confirmatory factor analysis across 2 samples (combined N = 867). There was some evidence for the adequacy of different short forms, but model adequacy was not consistent across samples. Supplementary analyses revealed a multidimensional structure to the full Marlowe-Crowne scale and indicated that the apparent adequacy of model fit for some short forms might be a statistical artifact. Using the Marlowe-Crowne scale or its various short forms as a control for response bias is discouraged on empirical and conceptual grounds.},
author = {Barger, Steven D},
doi = {10.1207/S15327752JPA7902_11},
isbn = {0022-3891$\backslash$r1532-7752},
issn = {0022-3891},
journal = {Journal of Personality Assessment},
number = {2},
pages = {286--305},
pmid = {12425392},
title = {{The Marlowe-Crowne affair: Short forms, psychometric structure, and social desirability.}},
volume = {79},
year = {2002}
}
@article{Bargh1986,
abstract = {The immediate environmental context in which social events occur and the current processing goals of the perceiver have been found to be important determinants of attentional focus, person memory, and social judgment. However, individuals may bring their own idiosyncratic perceptual sensitivities to bear on the selection of stimuli for further processing. The present experiment was designed to test whether one's chronically accessible social constructs constitute such a long-term perceptual readiness. The Stroop color-naming paradigm was employed to present trait-related adjectives to subjects who were to name the color in which each word was presented as quickly as possible. Subjects were paired according to the accessibility of their constructs for four different trait dimensions (so that one subject's accessible constructs were the other subject's inaccessible constructs, and vice versa). Analyses of color-naming latencies revealed that it took reliably longer for subjects to name the color of adjectives corresponding to their chronically accessible constructs than of those related to their inaccessible constructs. These results are consistent with a model in which stimulus properties relevant to one's accessible constructs receive preferential treatment in the initial automatic analysis of the environment.},
author = {Bargh, John A and Pratto, Felicia},
doi = {10.1016/0022-1031(86)90016-8},
issn = {0022-1031},
journal = {Journal of Experimental Social Psychology},
month = {jul},
number = {4},
pages = {293--311},
publisher = {Academic Press},
title = {{Individual construct accessibility and perceptual selection}},
url = {https://www.sciencedirect.com/science/article/pii/0022103186900168},
volume = {22},
year = {1986}
}
@article{Benjamin2017,
abstract = {We propose to change the default P-value threshold for statistical significance for claims of new discoveries from 0.05 to 0.005.},
archivePrefix = {arXiv},
arxivId = {psyarxiv/mky9j},
author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, E.-J. and Berk, Richard and Bollen, Kenneth A. and Brembs, Bj{\"{o}}rn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and {De Boeck}, Paul and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and {Hua Ho}, Teck and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munaf{\'{o}}, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Sch{\"{o}}nbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and {Van Zandt}, Trisha and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
doi = {10.1038/s41562-017-0189-z},
eprint = {mky9j},
issn = {2397-3374},
journal = {Nature Human Behaviour},
month = {jan},
number = {1},
pages = {6--10},
primaryClass = {psyarxiv},
title = {{Redefine statistical significance}},
url = {http://www.nature.com/articles/s41562-017-0189-z},
volume = {2},
year = {2018}
}
@article{Bethlehem2010,
abstract = {At first sight, web surveys seem to be an interesting and attractive means of data collection. They provide simple, cheap, and fast access to a large group of potential respondents. However, web surveys are not without methodological problems. Specific groups in the populations are under-represented because they have less access to Internet. Furthermore, recruitment of respondents is often based on self-selection. Both under-coverage and self-selection may lead to biased estimates. This paper describes these methodological problems. It also explores the effect of various correction techniques (adjustment weighting and use of reference surveys). This all leads to the question whether properly design web surveys can be used for data collection. The paper attempts to answer this question. It concludes that under-coverage problems may solve itself in the future, but that self-selection leads to unreliable survey outcomes},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bethlehem, Jelke},
doi = {10.1111/j.1751-5823.2010.00112.x},
eprint = {arXiv:1011.1669v3},
isbn = {1472-6483 (Print)$\backslash$r1472-6483 (Linking)},
issn = {03067734},
journal = {International Statistical Review},
keywords = {Adjustment weighting,Bias,Online survey,Reference survey,Self-selection,Under-coverage,Web survey},
number = {2},
pages = {161--188},
pmid = {16417727},
title = {{Selection bias in web surveys}},
volume = {78},
year = {2010}
}
@book{Brown2006,
abstract = {Emphasizing practical and theoretical aspects of confirmatory factor analysis (CFA) rather than mathematics or formulas, Timothy A. Brown uses rich examples derived from the psychology, management, and sociology literatures to provide in-depth treatment of the concepts, procedures, pitfalls, and extensions of CFA methodology. Chock full of useful advice and tables that outline the procedures, the text shows readers how to conduct exploratory factor analysis (EFA) and understand similarities to and differences from CFA; formulate, program, and interpret CFA models using popular latent variable software packages such as LISREL, Mplus, Amos, EQS, and SAS/CALIS; and report results from a CFA study. Also covered are extensions of CFA to traditional IRT analysis, methods for determining necessary sample sizes, and new CFA modeling possibilities, including multilevel factor models and factor mixture models. Special features include a Web page offering data and program syntax files for many of the research examples so that readers can practice the procedures described in the book with real data. The Web page also includes links to additional CFA-related resources.},
address = {New York, NY},
author = {Brown, Timothy.A.},
booktitle = {Methodology in the Social Sciences},
isbn = {9781593852740},
publisher = {The Guilford Press},
title = {{Confirmatory Factor Analysis for Applied Research}},
year = {2006}
}
@incollection{Buchanan2014,
address = {London, United Kingdom},
author = {Buchanan, Erin M and Valentine, Kathrene D and Schulenberg, Stefan E},
booktitle = {SAGE Research Methods Cases},
doi = {10.4135/978144627305013517794},
editor = {Bindle, P.},
publisher = {SAGE Publications, Ltd.},
title = {{Exploratory and Confirmatory Factor Analysis: Developing the Purpose in Life Test–Short Form}},
url = {http://srmo.sagepub.com/view/methods-case-studies-2014/n232.xml},
year = {2014}
}
@misc{Buchanan2017,
author = {Buchanan, Erin M and Valentine, Kathrene D and Scofield, John E},
title = {{MOTE}},
url = {https://github.com/doomlab/MOTE},
year = {2017}
}
@article{Buchanan2002,
abstract = {Internet-mediated psychological assessment procedures can play an important role in behavioral telehealth, but their use is not unproblematic. Possible uses of World Wide Web-based tests are discussed. Published empirical evaluations of Web-based personality tests indicate that they can be reliable and valid. However, evidence exists that Web-based versions of tests may not always measure the same constructs as their traditional antecedents: Equivalence cannot be assumed. Web-based clinical assessment seems viable, but there are potential difficulties with measurement of some constructs (particularly negative affect), as well as ethical considerations. While offering great potential, online tests of clinical constructs require stringent validation and cautious use. (PsycINFO Database Record (c) 2009 APA ) (journal abstract)},
author = {Buchanan, Tom},
doi = {10.1037/0735-7028.33.2.148},
isbn = {0735-7028},
issn = {1939-1323},
journal = {Professional Psychology: Research and Practice},
keywords = {Computer Assisted Testing,Internet,Personality Measures,Statistical Reliability,Statistical Validity},
number = {2},
pages = {148--154},
title = {{Online assessment: Desirable or dangerous?}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0735-7028.33.2.148},
volume = {33},
year = {2002}
}
@article{Buchanan2003,
abstract = {Use of internet-mediated questionnaire assessment is growing in behavioural telehealth provision. Literature on web-based psychological testing indicates that--while tests used online should be validated for online use--such measures can be reliable and valid. However, use of normative data (especially data from paper-and-pencil samples) presents problems. Studies have shown score distributions may differ across testing media, even when comparisons are made between equivalent samples. Data from 3 different projects illustrate such (substantive) differences, and the errors that would result from using established norms to interpret internet-mediated assessments. Given representativeness issues with "internet norms", it is recommended that if online tests are used for clinical purposes, this should currently be done in a manner that does not rely on normative data.},
author = {Buchanan, Tom},
doi = {10.1080/16506070310000957},
isbn = {1650-6073 (Print)$\backslash$n1650-6073 (Linking)},
issn = {1650-6073},
journal = {Cognitive Behaviour Therapy},
month = {jan},
number = {3},
pages = {100--109},
pmid = {16291542},
title = {{Internet-based questionnaire assessment: Appropriate use in clinical contexts}},
url = {http://www.tandfonline.com/doi/abs/10.1080/16506070310000957},
volume = {32},
year = {2003}
}
@article{Buchanan2005,
abstract = {There is growing evidence that Internet-mediated psychological tests can have satisfactory psychometric properties and can measure the same constructs as traditional versions. However, equivalence cannot be taken for granted. The prospective memory questionnaire (PMQ; Hannon, Adams, Harrington, Fries-Dias, {\&} Gibson, 1995) was used in an on-line study exploring links between drug use and memory (Rodgers et al., 2003). The PMQ has four factor-analytically derived subscales. In a large (N763) sample tested via the Internet, only two factors could be recovered; the other two subscales were essentially meaningless. This demonstration of nonequivalence underlines the importance of on-line test validation. Without examination of its psychometric properties, one cannot be sure that a test administered via the Internet actually measures the intended construct.},
author = {Buchanan, Tom and Ali, Tarick and Heffernan, Thomas and Ling, Jonathan and Parrott, Andrew and Rodgers, Jacqui and Scholey, Andrew},
doi = {10.3758/BF03206409},
isbn = {1554-351X},
issn = {1554-351X},
journal = {Behavior Research Methods},
number = {1},
pages = {148--154},
pmid = {16097355},
title = {{Nonequivalence of on-line and paper-and-pencil psychological tests: The case of the prospective memory questionnaire}},
url = {http://www.springerlink.com.ezproxy.ugm.ac.id/content/j0395r54h0555233/abstract/{\%}5Cnhttp://www.springerlink.com.ezproxy.ugm.ac.id/content/j0395r54h0555233/fulltext.pdf},
volume = {37},
year = {2005}
}
@article{Buhrmester2011,
author = {Buhrmester, M. and Kwang, T. and Gosling, S. D.},
doi = {10.1177/1745691610393980},
isbn = {1745-6916},
issn = {1745-6916},
journal = {Perspectives on Psychological Science},
number = {1},
pages = {3--5},
pmid = {26162106},
title = {{Amazon's Mechanical Turk: A new source of inexpensive, yet high-quality, data?}},
url = {http://pps.sagepub.com/lookup/doi/10.1177/1745691610393980},
volume = {6},
year = {2011}
}
@article{Cantrell2007,
abstract = {AIM: This paper is a report of a study to evaluate the use of an online data collection method to survey early survivors of childhood cancer about their physical and psychosocial characteristics and health-related quality of life. BACKGROUND: A major advantage in conducting web-based nursing research is the ability to involve participants who are challenging to study because of their small numbers or inaccessibility because of geographic location. As paediatric oncology patients and early survivors of childhood cancer are often not easily accessible because of their small numbers at single institutions, web-based research methods have been proposed as a potentially effective approach to collect data in studies involving these clinical populations. METHOD: Guided by published literature on using the Internet for data collection, an online protocol was developed; this included construction of a website, development of a homepage and interactive HyperText Markup Language pages and the posting of the study link on various websites. Data collection occurred over a 6-month period between December 2005 and May 2006. FINDINGS: Despite using strategies in conducting online research cited in published literature, the recruitment of subjects was very prolonged and the volume of missing data among many respondents excluded them from the study and created bias within the study's results. CONCLUSION: Web-based, online data collection methods create opportunities to conduct research globally, especially among difficult to access populations. However, web-based research requires careful consideration of how the study will be advertised and how data will be collected to ensure high quality data and validity of the findings.},
author = {Cantrell, Mary Ann and Lupinacci, Paul},
doi = {10.1111/j.1365-2648.2007.04448.x},
isbn = {1365-2648},
issn = {0309-2402},
journal = {Journal of Advanced Nursing},
keywords = {Childhood cancer survivors,Empirical research report,Methodological issues,Nursing,Online questionnaire,Web-based research},
month = {dec},
number = {5},
pages = {544--549},
pmid = {17973718},
title = {{Methodological issues in online data collection}},
url = {http://doi.wiley.com/10.1111/j.1365-2648.2007.04448.x},
volume = {60},
year = {2007}
}
@article{Charters2004,
author = {Charters, Elizabeth},
journal = {College Quarterly},
number = {1},
pages = {1--13},
title = {{New perspectives on popular culture, science and technology: Web browsers and the new illiteracy}},
volume = {7},
year = {2004}
}
@article{Cohen1994,
abstract = {After 4 decades of severe criticism, the ritual of null hy-pothesis significance testing—mechanical dichotomous decisions around a sacred .05 criterion—still persists. This article reviews the problems with this practice, including its near-universal misinterpretation ofp as the probability that H o is false, the misinterpretation that its complement is the probability of successful replication, and the mis-taken assumption that if one rejects H o one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods is suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication.},
author = {Cohen, Jacob},
file = {::},
journal = {American Psychologist},
number = {12},
pages = {997--1003},
title = {{The Earth Is Round}},
url = {http://www.sjsu.edu/faculty/gerstman/misc/Cohen1994.pdf},
volume = {49},
year = {1994}
}
@article{Cohen1992a,
author = {Cohen, Jacob},
doi = {10.1037//0033-2909.112.1.155},
issn = {0033-2909},
journal = {Psychological Bulletin},
number = {1},
pages = {155--159},
title = {{A power primer.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.112.1.155},
volume = {112},
year = {1992}
}
@book{Cohen1988,
address = {Hillsdale, NJ},
author = {Cohen, Jacob},
edition = {2nd},
publisher = {Earlbaum},
title = {{Statistical power analysis for the behavioral sciences}},
year = {1988}
}
@article{Compeau1995,
abstract = {This paper discusses the role of individuals' beliefs about their abilities to competently use computers (computer self-efficacy) in the determination of computer use. A survey of Canadian managers and professionals was conducted to develop and validate a measure of computer self-efficacy and to assess both its impacts and antecedents. Computer self-efficacy was found to exert a significant influence on individuals' expectations of the outcomes of using computers, their emotional reactions to computers (affect and anxiety), as well as their actual computer use. An individual's self-efficacy and outcome expectations were found to be positively influenced by the encouragement of others in their work group, as well as others' use of computers. Thus, self-efficacy represents an important individual trait, which moderates organizational influences (such as encouragement and support) on an individual's decision to use computers. Understanding self-efficacy, then, is important to the successful implementation of systems in organizations. The existence of a reliable and valid measure of self-efficacy makes assessment possible and should have implications for organizational support, training, and implementation.},
author = {Compeau, Deborah R. and Higgins, Christopher A},
doi = {10.2307/249688},
issn = {02767783},
journal = {MIS Quarterly},
keywords = {causal models,gb02,gb03,isrl categories,ment,partial least squares,user behavior},
month = {jun},
number = {2},
pages = {189},
title = {{Computer self-efficacy: Development of a measure and initial test}},
url = {http://www.jstor.org/stable/249688?origin=crossref},
volume = {19},
year = {1995}
}
@book{Comrey1992,
abstract = {The purpose of this book is to help the reader achieve some familiarity with . . . [factor analytic] methods of analysis. This book has been written for advanced undergraduate students, graduate students, and research workers who need to develop a knowledge of factor analysis, either for the purpose of understanding the published research of others or to aid them in their own research. It is presumed that most readers will have a reasonable understanding of high school geometry, algebra, trigonometry, and a course in elementary statistics, even though the concepts in these areas may not have been reviewed recently. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
author = {Comrey, Andrew Laurence and Lee, Howard Bing},
booktitle = {A first course in factor analysis, 2nd ed.},
doi = {10.1037/0011756},
isbn = {0-8058-1062-5 (Hardcover)},
keywords = {Factor Analysis},
pages = {xii, 430--xii, 430},
pmid = {3448226},
title = {{A first course in factor analysis, 2nd ed.}},
year = {1992}
}
@article{Cook2000,
abstract = {Response representativeness is more important than response rate in survey research. However, response rate is important if it bears on representativeness. The present meta-analysis explores factors associated with higher response rates in electronic sur- veys reported in both published and unpublished research. The number of contacts, per- sonalized contacts, and precontacts are the factors most associated with higher response rates in the Web studies that are analyzed.},
author = {Cook, Colleen and Heath, Fred and Thompson, Russel L.},
doi = {10.1177/00131640021970934},
isbn = {0013-1644},
issn = {0013-1644},
journal = {Educational and Psychological Measurement},
month = {dec},
number = {6},
pages = {821--836},
pmid = {21739744},
title = {{A meta-analysis of response rates in Web- or Internet-based surveys}},
url = {http://journals.sagepub.com/doi/10.1177/00131640021970934},
volume = {60},
year = {2000}
}
@misc{Cooper2012,
abstract = {(from the book) In the pages of this handbook, you will find descriptions of many techniques that psychologists and others have developed to help them pursue a shared understanding of why humans think, feel, and behave the way they do. These are the tools that we use to conduct our rational analyses. At the broadest level, when choosing a method you make decisions about (a) what data or measurement techniques will best capture the thoughts, feelings, and behaviors that interest you; (b) what research design best fits the question that you want to answer; and (c) what strategies for data analysis best match the characteristics of your design and measurements. The simplest choice for organizing the presentation of material is the temporal sequence in which you will make these decisions. This is roughly what we have done. In Volume 2, interpretive research designs that emphasize a qualitative approach are detailed in Part I. Volume 2, Parts II through VI, introduces designs that emphasize an etic (or theory-specified), more quantitative approach to research. In Volume 2, Part V, designs are introduced that focus on theory-testing questions, rely heavily on quantification, and are used to study change in individual units. The designs in Volume 2, Part VI, are labeled "Neuropsychology" and "Biological Psychology." Here you will find designs for theory-driven research that derive largely from the more biological end of psychology's family tree. (PsycINFO Database Record (c) 2015 APA, all rights reserved)},
author = {Cooper, Harris [Ed] and Camic, Paul M [Ed] and Long, Debra L [Ed] and Panter, A T [Ed] and Rindskopf, David [Ed] and Sher, Kenneth J [Ed]},
booktitle = {APA handbook of research methods in psychology, Vol 2: Research designs: Quantitative, qualitative, neuropsychological, and biological.},
isbn = {1-4338-1005-0 (Hardcover), 978-1-43381-005-3 (Hardcover)},
keywords = {*Experimental Design,*Experimentation,*Methodology,*Psychology,Behavioral Neuroscience,Neuropsychology,Qualitative Research,Quantitative Methods},
title = {{APA handbook of research methods in psychology, Vol 2: Research designs: Quantitative, qualitative, neuropsychological, and biological.}},
url = {http://ovidsp.ovid.com/ovidweb.cgi?T=JS{\&}PAGE=reference{\&}D=psyc9{\&}NEWS=N{\&}AN=2011-23864-000},
year = {2012}
}
@article{Cribbie2004,
abstract = {Researchers in psychology reliably select traditional null hypothesis significance tests (e.g., Student's t test), regardless of whether the research hypothesis relates to whether the group means are equivalent or whether the group means are different. Tests of equivalence, which have been popular in biopharmaceutical studies for years, have recently been introduced and recommended to researchers in psychology for demonstrating the equivalence of two group means. However, very few recommendations exist for applying tests of equivalence. A Monte Carlo study was used to compare the test of equivalence proposed by Schuirmann with the traditional Student t test for deciding if two group means are equivalent. It was found that Schuirmann's test of equivalence is more effective than Student's t test at detecting population mean equivalence with large sample sizes; however, Schuirmann's test of equivalence performs poorly relative to Student's t test with small sample sizes and/or inflated variances.},
author = {Cribbie, Robert A. and Gruman, Jamie A. and Arpin-Cribbie, Chantal A.},
doi = {10.1002/jclp.10217},
issn = {0021-9762},
journal = {Journal of Clinical Psychology},
keywords = {Equivalence tests,Null hypothesis testing,Student's t,Variance heterogeneity},
month = {jan},
number = {1},
pages = {1--10},
pmid = {14692005},
title = {{Recommendations for applying tests of equivalence}},
url = {http://doi.wiley.com/10.1002/jclp.10217},
volume = {60},
year = {2004}
}
@article{Cronk2002,
abstract = {Students, faculty, and researchers have become increasingly comfortable with the Internet, and many of them are interested in using the Web to collect data. Few published studies have investigated the differences between Web-based data and data collected with more traditional methods. In order to investigate these potential differences, two important factors were crossed in this study: whether the data were collected on line or not and whether the data were collected in a group setting at a fixed time or individually at a time of the respondent's choosing. The Visions of Morality scale (Shelton {\&} McAdams, 1990) was used, and the participants were assigned to one of four conditions: in-class Web survey, in-class paper-and-pencil survey; take-home Web survey, and take-home paper-and-pencil survey. No significant differences in scores were found for any condition; however, response rates were affected by the type of survey administered, with the take-home Web-based instrument having the lowest response rate. Therefore, researchers need to be aware that different modes of administration may affect subject attrition and may, therefore, confound investigations of other independent variables.},
author = {Cronk, Brian C and West, Jamie L},
doi = {10.3758/BF03195440},
isbn = {0743-3808},
issn = {0743-3808},
journal = {Behavior Research Methods, Instruments, {\&} Computers},
month = {may},
number = {2},
pages = {177--180},
pmid = {12109009},
title = {{Personality research on the Internet: A comparison of Web-based and traditional instruments in take-home and in-class settings}},
url = {http://www.springerlink.com/index/10.3758/BF03195440},
volume = {34},
year = {2002}
}
@article{Crumbaugh1964,
author = {Crumbaugh, James C. and Maholick, Leonard T.},
doi = {10.1002/1097-4679(196404)20:2<200::AID-JCLP2270200203>3.0.CO;2-U},
isbn = {0021-9762},
issn = {00219762},
journal = {Journal of Clinical Psychology},
month = {apr},
number = {2},
pages = {200--207},
pmid = {14138376},
title = {{An experimental study in existentialism: The psychometric approach to Frankl's concept ofnoogenic neurosis}},
url = {http://doi.wiley.com/10.1002/1097-4679{\%}28196404{\%}2920{\%}3A2{\%}3C200{\%}3A{\%}3AAID-JCLP2270200203{\%}3E3.0.CO{\%}3B2-U},
volume = {20},
year = {1964}
}
@article{Cumming2014,
abstract = {We need to make substantial changes to how we conduct research. First, in response to heightened concern that our published research literature is incomplete and untrustworthy, we need new requirements to ensure research integrity. These include prespecification of studies whenever possible, avoidance of selection and other inappropriate data-analytic practices, complete reporting, and encouragement of replication. Second, in response to renewed recognition of the severe flaws of null-hypothesis significance testing (NHST), we need to shift from reliance on NHST to estimation and other preferred techniques. The new statistics refers to recommended practices, including estimation based on effect sizes, confidence intervals, and meta-analysis. The techniques are not new, but adopting them widely would be new for many researchers, as well as highly beneficial. This article explains why the new statistics are important and offers guidance for their use. It describes an eight-step new-statistics strategy for research with integrity, which starts with formulation of research questions in estimation terms, has no place for NHST, and is aimed at building a cumulative quantitative discipline.},
author = {Cumming, Geoff},
doi = {10.1177/0956797613504966},
isbn = {1467-9280 (Electronic)$\backslash$r0956-7976 (Linking)},
issn = {1467-9280},
journal = {Psychological Science},
keywords = {13,20,8,estimation,meta-analysis,received 7,replication,research integrity,research methods,revision accepted 8,statistical analysis,that most current,the new statistics,there is increasing concern},
month = {jan},
number = {1},
pages = {7--29},
pmid = {24220629},
title = {{The new statistics: Why and how}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24220629{\%}5Cnhttp://pss.sagepub.com/lookup/doi/10.1177/0956797613504966 http://pss.sagepub.com/lookup/doi/10.1177/0956797613504966},
volume = {25},
year = {2014}
}
@book{Cumming2012,
address = {New York, NY},
author = {Cumming, Geoff},
publisher = {Routledge},
title = {{Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis}},
year = {2012}
}
@article{Davis1999,
abstract = {The World-Wide Web holds great promise as a mechanism for questionnaire-based research. But are data from Web-based questionnaires comparable to data from standard paper-and-pencil questionnaires? This study assessed the equivalence of the Ruminative Responses Scale in a Web-based format and in a paper-and-pencil format among introductory psychology, upper-level psychology, and non-psychology students. Internal consistency coefficients were comparable across the groups. The participants in the Web sample reported higher levels of self-focused rumination than did the other groups. Women in the Web sample reported more self-focused rumination than did women in the other groups. In the Web sample, results did not covary with access location. These results suggest that findings from Web-based questionnaire research are comparable with results obtained using standard procedures. The computerized Web interface may also facilitate self-disclosure among research participants.},
author = {Davis, Robert N.},
doi = {10.3758/BF03200737},
isbn = {0743-3808 (Print)$\backslash$r0743-3808 (Linking)},
issn = {0743-3808},
journal = {Behavior Research Methods, Instruments, {\&} Computers},
month = {dec},
number = {4},
pages = {572--577},
pmid = {10633976},
title = {{Web-based administration of a personality questionnaire: Comparison with traditional methods}},
url = {http://www.springerlink.com/index/10.3758/BF03200737},
volume = {31},
year = {1999}
}
@article{DeLeeuw1988,
author = {{De Leeuw}, Edith D. and Hox, Joop J.},
journal = {Journal of Official Statistics},
number = {3},
pages = {241--249},
title = {{The effects of response-stimulating factors on response rates and data quality in mail surveys: A test of Dillman's total design method}},
volume = {4},
year = {1988}
}
@article{Deutskens2006,
abstract = {This article examines whether online and mail surveys produce convergent results, which would allow them to be used in mixed-mode service quality studies. In the context of a large business-to-business service quality assessment, an analysis of the accuracy and completeness of respon- dent answers to both open and closed questions suggests that online and mail surveys produce equivalent results. Composite reliability shows consistently high levels for both groups, and the means and variance-covariance matrices are equal across modes. However, minor differences occur between the two survey methods; online respondents provide more improvement suggestions, indicate more often to which competitor they want to switch, and provide lengthier answers in response to requests for examples of positive experiences with the company. This research provides important findings regarding the process for, and results of, comparing two survey modes},
author = {Deutskens, Elisabeth and de Ruyter, Ko and Wetzels, Martin},
doi = {10.1177/1094670506286323},
isbn = {1094-6705},
issn = {1094-6705},
journal = {Journal of Service Research},
number = {4},
pages = {346--355},
title = {{An Assessment of Equivalence Between Online and Mail Surveys in Service Research}},
url = {http://journals.sagepub.com/doi/10.1177/1094670506286323},
volume = {8},
year = {2006}
}
@book{DeVellis2016a,
abstract = {DeVellis walks a thin line between making his description of measurement in the social sciences accessible to a wide range of students and practitioners, while including the most recent developments, some of which are quite technical and sophisticated.},
author = {DeVellis, Robert F.},
booktitle = {Sage},
keywords = {Bibliographies},
title = {{Scale Development: Theory and Applications, 4th Edition}},
volume = {26},
year = {2016}
}
@article{Dienes2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Dienes, Zoltan},
doi = {10.3389/fpsyg.2014.00781},
eprint = {arXiv:1011.1669v3},
isbn = {1664-1078 (Electronic)},
issn = {1664-1078},
journal = {Frontiers in Psychology},
number = {July},
pages = {1--17},
pmid = {25120503},
title = {{Using Bayes to get the most out of non-significant results}},
volume = {5},
year = {2014}
}
@book{Dillman2008,
abstract = {A complete, start-to-finish guide for every researcher to successfully plan and conduct Internet, mail, and telephone surveys, Internet, Mail, and Mixed-Mode Surveys: The Tailored Design Method, Third Edition presents a succinct review of survey research methods, equipping you to increase the validity and reliability, as well as response rates, of your surveys. Now thoroughly updated and revised with information about all aspects of survey research?grounded in the most current research?the new edition provides practical ?how-to? guidelines on optimally using the Internet, mail, and phone channels to your advantage.},
address = {Hoboken, NJ},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Dillman, Don A. and Smyth, Jolene D. and Christian, Leah Melani},
booktitle = {Internet Mail and Mixed Mode Surveys The tailored design method},
doi = {10.2307/41061275},
edition = {3rd},
eprint = {arXiv:1011.1669v3},
isbn = {9780471698685},
issn = {14337851},
pmid = {763844000},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Internet, mail, and mixed-mode surveys: The tailored design method}},
url = {http://www.worldcat.org/wcpa/search?q=isbn:9780471698685{\&}qt=advanced},
year = {2008}
}
@article{Duncan2005,
abstract = {This study was designed to examine the impact of silhouette randomization on the responses to rating scales developed to rate the perceived current and ideal body shape, as well as body dissatisfaction. Seventy students (30 men and 40 women), ages 18 to 43 (M±SD = 22.1 ± 5.7) years, completed the Stunkard, Sorensen, and Schulsinger (1983) Figure Rating Scale twice, approximately 2 weeks apart in a randomized, counterbalanced order. On one occasion, the traditional scale was completed, and another scale with a randomized set of silhouettes was completed. Results indicated a significant relationship between the traditionally and randomly presented scales for perceived current and ideal body shapes and for body dissatisfaction (all p{\textless} .01). Wilcoxon signed-ranks test revealed no significant mean rank difference between the two scales for perceived current and ideal body shapes and body dissatisfaction (all p {\textgreater}.05). Randomization of figures does not appear to have an impact on results from the Figure Rating Scale, and the sequentially presented scale appears to be appropriate for body image assessment. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
author = {Duncan, Michael J and Dodd, Lorna J and Al-Nakeeb, Yahya},
doi = {10.1207/s15327841mpee0901_5},
issn = {1091-367X},
journal = {Measurement in Physical Education and Exercise Science},
keywords = {*Body Image *Dissatisfaction Rating Scales},
month = {mar},
number = {1},
pages = {61--66},
title = {{The impact of silhouette randomization on the results of Figure Rating Scales}},
url = {http://www.tandfonline.com/doi/abs/10.1207/s15327841mpee0901{\_}5},
volume = {9},
year = {2005}
}
@article{Etz2015,
archivePrefix = {arXiv},
arxivId = {1511.08180},
author = {Etz, Alexander and Wagenmakers, Eric-Jan},
eprint = {1511.08180},
keywords = {and phrases,evidence,history of statistics,induction,sir},
pages = {1--23},
title = {{J. B. S. Haldane's Contribution to the Bayes Factor Hypothesis Test}},
url = {http://arxiv.org/abs/1511.08180},
year = {2015}
}
@article{Fang2009,
abstract = {Responding to suggestions of prior research for examining the psychological constructs involved in a decision for or against participation in web surveys, this paper investigated the effects of trust in sponsor and personal innovativeness on potential respondents' participation intention. Based on the theory of planned behavior (TPB), two alternative models were empirically tested in which the roles of trust and innovativeness were theorized differently-either as moderators of the effects which perceived behavioral control and attitude have on participation intention (moderator model) or as direct determinants of the attitude, perceived behavioral control and intention (direct effects model). Data was collected from a sample of 131 university students enrolled in a computer course. The results of our study indicated that: (1) TPB could satisfactorily predict the behavioral intention with up to 44{\%} variation of the intention being predicted by the model; (2) trust in sponsor and personal web innovativeness exerted direct determinant effects rather than moderate effects on participation attitude and perceived behavioral control, which in turn significantly affected participation intention; and (3) integrating the variables of trust and personal innovativeness into TPB model enhanced the prediction effect. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Fang, Jiaming and Shao, Peiji and Lan, George},
doi = {10.1016/j.chb.2008.08.002},
isbn = {0747-5632},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Personal innovativeness,Response rate,Theory of planned behavior (TPB),Trust,Web survey},
number = {1},
pages = {144--152},
publisher = {Elsevier Ltd},
title = {{Effects of innovativeness and trust on web survey participation}},
url = {http://dx.doi.org/10.1016/j.chb.2008.08.002},
volume = {25},
year = {2009}
}
@article{Fang2012a,
abstract = {Prior research involving response rates in Web-based surveys has not adequately addressed the effect of the reputation of a sponsoring corporation that contracts with a survey provider. This study investigates the effect of two factors, namely, the reputation of a survey's provider and the reputation of a survey's sponsoring corporation, on the willingness of potential respondents to participate in a Web survey. Results of an experimental design with these two factors reveal that the sponsoring corporation's and the survey provider's strong reputations can induce potential respondents to participate in a Web survey. A sponsoring corporation's reputation has a greater effect on the participation willingness of potential respondents of a Web survey than the reputation of the survey provider. A sponsoring corporation with a weak reputation who contracts with a survey provider having a strong reputation results in increased participation willingness from potential respondents if the identity of the sponsoring corporation is disguised in a survey. This study identifies the most effective strategy to increase participation willingness for a Web-based survey by considering both the reputations of the sponsoring corporation and survey provider and whether to reveal their identities.},
author = {Fang, Jiaming and Wen, Chao and Pavur, Robert},
doi = {10.1089/cyber.2011.0411},
issn = {2152-2715},
journal = {Cyberpsychology, Behavior, and Social Networking},
month = {apr},
number = {4},
pages = {195--199},
pmid = {22304457},
title = {{Participation willingness in web surveys: Exploring effect of sponsoring corporation's and survey provider's reputation}},
url = {http://online.liebertpub.com/doi/abs/10.1089/cyber.2011.0411},
volume = {15},
year = {2012}
}
@article{Fang2012,
abstract = {Little research exists that addresses the equivalence in collectivistic cultures of paper- versus Internet-based surveys. This study addressed this gap and examined the measurement equivalence of individual innovativeness scales between Internet surveys and paper-based surveys within a collectivistic culture (with China serving as our example). The study analyzed and compared survey data from both paper and web-based surveys using confirmatory factor analysis. The assessment of invariance included the levels of configural, metric, scalar, and covariance invariance. The means and variance of latent variables were also compared. The results show that measurements are invariant at the two levels (confi- gural and metric), and the covariances between latent variables are also equivalent, but the mean and variance differences of latent variables are apparent. The results indicate thatwhen conducting research in collectivistic cultures and collecting data from distinct survey modes, researchers should concern themselves with the potential of extreme response patterns and the inclination of social desirability responding, as well as considering the measurement invariance across survey modes.},
author = {Fang, Jiaming and Wen, Chao and Prybutok, Victor R.},
doi = {10.1007/s11135-012-9783-3},
isbn = {0747-5632},
issn = {0033-5177},
journal = {Quality {\&} Quantity},
keywords = {Collectivistic cultures,Measurement invariance,Mixed-mode survey,Response biases,Web survey},
month = {jan},
number = {1},
pages = {493--506},
title = {{An assessment of equivalence between Internet and paper-based surveys: evidence from collectivistic cultures}},
url = {http://link.springer.com/10.1007/s11135-012-9783-3},
volume = {48},
year = {2012}
}
@article{Fang2014,
abstract = {Investigation of the underlying mechanisms responsible for measurement variance has received little attention. The primary objective of this study is to examine whether paper and social media surveys produce convergent results and investigate the underlying psychological mechanisms for the potential measurement nonequivalence. Particularly, we explored the role of social desirability and satisficing on the measurement results. We collected data via five different survey modes, including paper survey, ad hoc Web survey, online forum (message boards)-based, SNS-based and microblog-based surveys. The findings show that socially desirable responding does not lead to inconsistent results. Rather we found that satisficing causes inconsistent results in paper versus online surveys. Sociability reduces the possibility of engaging in satisficing that results in inconsistent results between traditional Web surveys and social media-based Web surveys. ?? 2013 Elsevier Ltd. All rights reserved.},
author = {Fang, Jiaming and Wen, Chao and Prybutok, Victor R.},
doi = {10.1016/j.chb.2013.09.019},
isbn = {0747-5632},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Measurement invariance,Satisficing,Social desirability,Social media,Social media surveys,Web survey},
pages = {335--343},
publisher = {Elsevier Ltd},
title = {{An assessment of equivalence between paper and social media surveys: The role of social desirability and satisficing}},
url = {http://dx.doi.org/10.1016/j.chb.2013.09.019},
volume = {30},
year = {2014}
}
@article{Fang2013,
abstract = {An increasing proportion of information technology (IT)/information system adoption research collects data using online surveys. However, a paucity of research assesses the equivalence of paper-based versus Internet-based surveys in collectivistic cultures. Furthermore, no theoretical or empirical research investigates how cultural differences between collectivistic and individualistic cultures influence the measurement equivalence (ME) of these survey modes. To explore these issues, online and paper-based surveys with comparable samples were carried out in both an individualistic (the USA) and a collectivistic culture (China). Confirmatory factor analysis was conducted to examine the ME across both survey modes in these different cultures. Results indicate that the relatively larger satisficing discrepancy between paper and online surveys causes respondents in collectivistic cultures to have an increased likelihood of providing responses that vary as compared to respondents in individualistic cultures. The disparate responses, in turn, result in increased measurement variance between the two survey modes. The findings of this study bridge a gap in the literature and address the question of how culture influences online satisficing behaviour and how that behaviour causes measurement invariance across survey modes. This study also explains the possible underlying mechanisms by which different national cultures exert their influence on survey results. The findings provide important implications for IT researchers, especially those in collectivistic cultures or those who need to collect data in collectivistic cultures using online surveys or mixed-mode surveys that include an online survey mode. [ABSTRACT FROM AUTHOR]},
author = {Fang, Jiaming and Wen, Chao and Prybutok, Victor R.},
doi = {10.1080/0144929X.2012.751621},
isbn = {0144-929X},
issn = {0144-929X},
journal = {Behaviour {\&} Information Technology},
keywords = {CHI-squared test,CHINA,COLLEGE students -- United States,CULTURE,DIFFUSION of innovations,FACTOR analysis,FINANCE,INFORMATION technology,INTERNET,IT adoption,PSYCHOLOGICAL tests,RESEARCH,RESEARCH -- Methodology,SEX distribution (Demography),SURVEYS,UNITED States,Web survey,measurement invariance,national culture,satisficing},
number = {5},
pages = {480--490},
title = {{The equivalence of Internet versus paper-based surveys in IT/IS adoption research in collectivistic cultures: The impact of satisficing}},
url = {http://ezproxy.rollins.edu:2048/login?url=http://search.ebscohost.com/login.aspx?direct=true{\&}db=lih{\&}AN=87373777{\&}site=ehost-live{\&}scope=site},
volume = {32},
year = {2013}
}
@article{Farmer1986,
author = {Farmer, Richard and Sundberg, Norman D.},
doi = {10.1207/s15327752jpa5001_2},
issn = {0022-3891},
journal = {Journal of Personality Assessment},
month = {mar},
number = {1},
pages = {4--17},
title = {{Boredom proneness - The development and correlates of a new scale}},
url = {http://www.tandfonline.com/doi/abs/10.1207/s15327752jpa5001{\_}2},
volume = {50},
year = {1986}
}
@article{Feldman1988,
abstract = {Drawing from recent developments in social cognition, cognitive psychology, and behavioral decision theory, we analyzed when and how the act of measuring beliefs, attitudes, intentions, and behaviors affects observed correlations among them. Belief, attitude, or intention can be created by measure-ment if the measured constructs do not already exist in long-term memory. The responses thus created can have directive effects on answers to other questions that follow in the survey. But even when counterparts to the beliefs, attitudes, and intentions measured already exist in memory, the structure of the survey researcher's questionnaire can affect observed correlations among them. The respondent may use retrieved answers to earlier survey questions as inputs to response generation to later questions. We present a simple theory predicting that an earlier response will be used as a basis for another, subsequent response if the former is accessible and if it is perceived to be more diagnostic than other accessible inputs. We outline the factors that determine both the perceived diagnosticity of a potential input, the likelihood that it will be retrieved, and the likelihood that some alternative (and potentially more diagnostic) inputs will be retrieved. This article examines the effects of measurement operations on revealed correlations among survey measures of belief, atti-tude, intention, and behavior. The potential reactivity of mea-surement has long been of concern in psychology. warn of measurement-induced dis-tortions relating to social desirability, evaluation apprehension, and sensitization to experimental treatments. Although it is true that the problem of reactivity of measurement affects work in both the social and physical sciences, in the physical sciences, measurement effects are expressed in terms of substantive the-ory. For instance, Heisenberg's uncertainty principle links basic physical processes to the consequences of measurement opera-tions. Our goal is to develop a similar linkage in psychology, relating theories of the generation of judgment and behavior to the effects of measurement, and in the process take steps toward a theory of the instrument (Cook {\&} Campbell, 1979).},
author = {Feldman, Jack M and Lynch, John G and Banks, Cristina and Feldman, Susan and Garland, Howard and Ho-Rowitz, Ira and James, Larry and Lutz, Rich and Mumford, Michael and Nedun-Gadi, Prakash and Sujan, Mita and Feldman, M},
file = {::},
journal = {Journal of Applied Psychology},
number = {3},
publisher = {Cook {\&} Campbell Runkel {\&} McGrath},
title = {{Self-Generated Validity and Other Effects of Measurement on Belief, Attitude, Intention, and Behavior}},
url = {https://pdfs.semanticscholar.org/55e6/b3ae155043601287b8e14246a21272805fd2.pdf},
volume = {73},
year = {1988}
}
@incollection{Frick2001,
abstract = {Whereas in a classical laboratory setting participants often feel obliged to stay and finish the experiment, participants in online studies can leave the session at any time. Although, from an ethical point of view, this is an advantage of online studies, complete voluntary participation might pose methodological problems. Of course, Web experimenters would like their participants to stay until the end of the experiment. To ensure this, they use special techniques. One such technique is to make Web pages shorter, more attractive, and faster loading as the participant continues further in the study. Such a technique works to increase a participants interest and enjoyment and may result in fewer drop outs during the most important part of the study. If a Web page has a long loading time at the beginning, participants with minimal interest or little time may leave right away without even starting the experiment. This high hurdle technique is particularly effective in combination with a warm-up phase (Reips, 1996, 1999, 2001) to eliminate those who may drop out up front. A second frequently used technique to prevent participants from leaving is to initially announce a lottery with prizes, in which only those who finish the experiment can take part. Whether or not this procedure is a successful method to reduce the drop out rate in online studies has never been examined experimentally. In a study on material (nonmonetary) incentives in mail surveys, Nederhof (1983) found that incentives did not increase the response rates but did increase the speed of return. Thus, one might argue that promising financial incentives is negligible in reducing drop out or might even reduce the intrinsic motivation of the potential participant (Deci, 1975). A survey among 21 Web experimenters recently conducted by Musch and Reips (2000) suggests that incentives do play a role in reducing drop out in online studies. They found a clear link between lack of financial incentives and drop out rate. A monetary prize might diminish drop out tendency whenever intrinsically motivating factors are not sufficient. In some studies on mail surveys, a similar effect could be found. For example, James and Bolstein (1992) found response rates of 79.3{\%} when participants received a 20 check, whereas in the control condition (no incentives) only 52.0{\%} returned the questionnaire. Similar results might be found if drop out were used as the dependent variable. The Web experiment at hand has been conducted to further investigate the causal nature of the relationship between financial incentives and drop out under completely voluntary conditions: In online studies. We expected a lower drop out rate when a lottery is announced at the beginning of the experiment. Another issue that should be considered is the possibility that the use of incentives could bias the participants answers. The promise of incentives might attract a certain group of participants (Rosenthal {\&} Rosnow, 1975), and it might influence participants answers. Rush, Phillips, and Panek (1978) found that two samples of volunteer subjects recruited from the same population by different means (paid/unpaid) differed significantly on ability, personality and task-related performance. The participants who had volunteered for their study and received no financial remuneration were more likely to be field dependent and could be characterized by a higher social-interpersonal orientation. Samples of paid volunteers, on the other hand, committed more errors of omission on a dichotic listening task. In analogy to different recruitment methods, presence or absence of incentives might lead to similar differences and different answering behavior. Most studies contain a few questions potentially revealing the responder's identity. Questions about gender, age, or address reduce the participants' impression of anonymity. In our view, perceived loss of anonymity leads to a higher self-commitment, in the sense of: I just gave away all this information about myself, therefore this study must be worth finishing. Compliance with all aspects of the study should be better. Consequently, we designed the present Web experiment to test the hypothesis that asking participants for personal information early in the experiment would lead to decreased drop out and a lower non-response rate. So far, a study conducted offline by Giles and Feild (1978) failed to find any effects of the placement of demographic items. In their study, however, they mailed out questionnaires and measured the response rates. Mailed questionnaires can be answered in any order, therefore one wouldn't expect effects due to the positioning of questions (cf., Schwarz {\&} Hippler, 1995; Reips, in press). The situation is different in WWW questionnaires with only one or very few questions per Web page: Participants cannot look ahead or determine their own response order. Whether demographic data should be assessed at the beginning of an experiment or at the end is an issue of high relevance in online research, as one's choice might create systematic drop out and lower data quality. Finally, beyond systematic biases introduced by drop out and non- response behaviors, asking for personal information at the beginning of a study could also directly influence answering behavior in questions that are likely to be influenced by social desirability. Participants answers might be more strongly influenced by social norms if they believed they could be identified (e. g., through their E-mail address). For example, Satow (1975) found a strong (negative) effect of privacy on helping behavior. Conclusions This experiment has shown that (1) financial incentives can reduce drop out; (2) assessing participants personal information at the beginning of an experiment can reduce drop out and may lead to more complete demographic data about the participants; (3) these positive effects can be reached without biasing the data; and (4) the language and order in which items are presented can influence answering behavior.},
author = {Frick, A and B{\"{a}}chtiger, M T and Reips, Ulf-Dietrich},
booktitle = {Dimensions of Internet Science},
editor = {Reips, Ulf-Dietrich and Bosnjak, Michael},
isbn = {3935357524},
pages = {209--219},
title = {{Financial incentives, personal information and dropout in online studies.}},
year = {2001}
}
@article{Gallistel2009,
abstract = {Null hypotheses are simple, precise, and theoretically important. Conventional statistical analysis cannot support them; Bayesian analysis can. The challenge in a Bayesian analysis is to formulate a suitably vague alternative, because the vaguer the alternative is (the more it spreads out the unit mass of prior probability), the more the null is favored. A general solution is a sensitivity analysis: Compute the odds for or against the null as a function of the limit(s) on the vagueness of the alternative. If the odds on the null approach 1 from above as the hypothesized maximum size of the possible effect approaches 0, then the data favor the null over any vaguer alternative to it. The simple computations and the intuitive graphic representation of the analysis are illustrated by the analysis of diverse examples from the current literature. They pose 3 common experimental questions: (a) Are 2 means the same? (b) Is performance at chance? (c) Are factors additive?},
author = {Gallistel, C R},
doi = {10.1037/a0015251},
file = {::},
issn = {0033-295X},
journal = {Psychological review},
month = {apr},
number = {2},
pages = {439--53},
pmid = {19348549},
publisher = {NIH Public Access},
title = {{The importance of proving the null.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19348549 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2859953},
volume = {116},
year = {2009}
}
@misc{Gramacy2017,
author = {Gramacy, Robert B.},
title = {{monomvn: Estimation for Multivariate Normal and Student-t Data with Monotone Missingness}},
url = {https://cran.r-project.org/package=monomvn},
year = {2017}
}
@article{Gramacy2010,
abstract = {Portfolio balancing requires estimates of covariance between asset returns. Returns data have histories which greatly vary in length, since assets begin public trading at different times. This can lead to a huge amount of missing data--too much for the conventional imputation-based approach. Fortunately, a well-known factorization of the MVN likelihood under the prevailing historical missingness pattern leads to a simple algorithm of OLS regressions that is much more reliable. When there are more assets than returns, however, OLS becomes unstable. Gramacy, et al. (2008), showed how classical shrinkage regression may be used instead, thus extending the state of the art to much bigger asset collections, with further accuracy and interpretation advantages. In this paper, we detail a fully Bayesian hierarchical formulation that extends the framework further by allowing for heavy-tailed errors, relaxing the historical missingness assumption, and accounting for estimation risk. We illustrate how this approach compares favorably to the classical one using synthetic data and an investment exercise with real returns. An accompanying R package is on CRAN.},
archivePrefix = {arXiv},
arxivId = {0907.2135},
author = {Gramacy, Robert B. and Pantaleoy, Ester},
doi = {10.1214/10-BA602},
eprint = {0907.2135},
isbn = {1931-6690},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Data augmentation,Double-exponential,Eavy tails,Factor model,Monotone missing data,Multivariate,Portfolio balancing,Ridge regression},
number = {2},
pages = {237--262},
title = {{Shrinkage regression for multivariate inference with missing data, and an application to portfolio balancing}},
volume = {5},
year = {2010}
}
@incollection{Hablas1982,
address = {Berkeley, CA},
author = {Hablas, R. and Hutzell, R.},
booktitle = {Analecta Frankliana: The proceedings of the First World Congress of Logotherapy: 1980},
editor = {Wawrytko, S. A.},
pages = {211--215},
publisher = {Strawberry Hill},
title = {{The Life Purpose Questionnaire: An alternative to the Purpose-in-Life test for geriatric, neuropsychiatric patients}},
year = {1982}
}
@article{Hausman1978,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hausman, J. A.},
doi = {10.2307/1913827},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {00129682},
journal = {Econometrica},
month = {nov},
number = {6},
pages = {1251},
pmid = {25246403},
title = {{Specification tests in Econometrics}},
url = {http://www.jstor.org/stable/1913827?origin=crossref},
volume = {46},
year = {1978}
}
@article{Henrich2010,
abstract = {Behavioral scientists routinely publish broad claims about human psychology and behavior in the world's top journals based on samples drawn entirely from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies. Researchers – often implicitly – assume that either there is little variation across human populations, or that these " standard subjects " are as representative of the species as any other population. Are these assumptions justified? Here, our review of the comparative database from across the behavioral sciences suggests both that there is substantial variability in experimental results across populations and that WEIRD subjects are particularly unusual compared with the rest of the species – frequent outliers. The domains reviewed include visual perception, fairness, cooperation, spatial reasoning, categorization and inferential induction, moral reasoning, reasoning styles, self-concepts and related motivations, and the heritability of IQ. The findings suggest that members of WEIRD societies, including young children, are among the least representative populations one could find for generalizing about humans. Many of these findings involve domains that are associated with fundamental aspects of psychology, motivation, and behavior – hence, there are no obvious a priori grounds for claiming that a particular behavioral phenomenon is universal based on sampling from a single subpopulation. Overall, these empirical patterns suggests that we need to be less cavalier in addressing questions of human nature on the basis of data drawn from this particularly thin, and rather unusual, slice of humanity. We close by proposing ways to structurally re-organize the behavioral sciences to best tackle these challenges.},
author = {Henrich, Joseph and Heine, Steven J and Norenzayan, Ara},
doi = {10.1017/S0140525X0999152X},
file = {::},
issn = {0140-525X},
journal = {Behavioral and Brain Sciences},
keywords = {behavioral economics,cross-cultural research,cultural psychology,culture,evolutionary psychology,experiments,external validity,generalizability,human universals,population variability},
month = {jun},
number = {2-3},
pages = {61--83},
title = {{The weirdest people in the world?}},
url = {http://www.psych.ubc.ca/henrich/home.html http://www.journals.cambridge.org/abstract{\_}S0140525X0999152X},
volume = {33},
year = {2010}
}
@article{Hewson2005,
abstract = {A growing number of studies have supported the use of unidimensional psychometric test instruments administered via the Internet; however, support for the use of multidimensional scales is weak. The present study compares paper and Internet administrations of the Multidimensional Health Locus of Control (MHLC) Scale (Wallston {\&} Wallston, 1981). In terms of reliabilities and factor structures, the Internet data were found to be at least as good as the paper data. MHLC scores were comparable for paper and Internet administrations, although the Internet sample scored significantly lower on the Powerful Others subscale. Overall, the results show that administration of the MHLC Scale via the Internet can produce data comparable to that obtained by pen-and-paper methods. However, it is concluded that generalization of these findings beyond the psychometric test instrument and sampling procedures used here is not warranted.},
author = {Hewson, Claire and Charlton, John P},
doi = {10.3758/BF03192742},
isbn = {1554351X},
issn = {1554-351X},
journal = {Behavior Research Methods},
number = {4},
pages = {691--702},
pmid = {16629304},
title = {{Measuring health beliefs on the Internet: A comparison of paper and Internet administrations of the Multidimensional Health Locus of Control Scale.}},
url = {https://link.springer.com/article/10.3758{\%}2FBF03192742},
volume = {37},
year = {2005}
}
@article{Higgins1983,
abstract = {A psycholinguistically based conception of the relation among context, categorization, and memory is tested by examining what happens to people's memory of an object when the object is initially categorized in terms of the context in which it appears, but, when the object is later recalled, this context is no longer salient. Subjects read about the sentencing decisions of a target trial judge in the context of other trial judges who consistently gave either higher sentences or lower sentences than the target judge. As predicted, subjects tended to categorize the target judge as “lenient” in the former, harsh context condition, and as “harsh” in the latter, lenient context condition. A week later, subjects read about the sentencing decisions of some additional judges, and then recalled the sentencing decisions of the target judge they had read about the week before. Across the two sessions, either a harsh, moderate, or lenient category norm for judges' sentencing decisions was established by having subjects read about decisions that involved either high, medium, or low sentences, respectively. The results indicated that subjects recalled the target judge's decisions by interpreting their prior categorization of his behavior in terms of the category norm established across the two sessions rather than the original context. Thus, subjects who were exposed to the same target in the same circumstances, and initially categorized the target in the same way, nevertheless remembered his behavior differently if their category norm was different at the moment of recall. Other types of “change of standard” and their implications for human judgment and memory are discussed.},
author = {Higgins, E.Tory and Lurie, Liora},
doi = {10.1016/0010-0285(83)90018-X},
issn = {0010-0285},
journal = {Cognitive Psychology},
month = {oct},
number = {4},
pages = {525--547},
publisher = {Academic Press},
title = {{Context, categorization, and recall: The “change-of-standard” effect}},
url = {https://www.sciencedirect.com/science/article/pii/001002858390018X},
volume = {15},
year = {1983}
}
@article{Hirschman2000,
abstract = {Children with specific language impairment (SLI) have been shown to be deficient in the use of complex sentences. In an attempt to remediate this area of weakness, two groups of such children (mean ages 9;4 and 10;6) received about 55 half-hour sessions of metalinguistic training spread over 12 months. Results showed that the use of complex sentences increased to at least normal levels in the experimental groups, and were significantly improved, at both the written and oral levels, as compared with SLI control groups, which evidenced little change over the same period. Further analysis of the data revealed the striking finding that those children who had the poorest complex sentence usage tended to benefit the most from metalinguistic training. These results are interpreted as support for the hypothesis that metalinguistic training helps to overcome the presumed neurological deficit of the language disordered child by making linguistic rules conscious, and thereby providing a 'metalinguistic bridge' via which information can bypass the damaged area.},
author = {Hirschman, Mary},
doi = {10.1080/136828200247179},
issn = {1368-2822},
journal = {International Journal of Language {\&} Communication Disorders},
keywords = {Child,Children,Female,Humans,Intelligence,Language Disorders,Language Disorders: therapy,Language Therapy,Language Therapy: methods,Learning,Lexical word formation,Linguistics,Linguistics: methods,Male,Specific language impairment,and disorders,and hearing functions,attainments,attentional capacity,central,children with sli,education,executive function,follow-up,icle,language,language comprehension,language disorders,language treatment,lexical learning,literacy,preschool children,pro les,processing,processing capacity,sli,sli critical mass,speci c language impairment,specific language impairment,speech,speech-language impairment,syntactic,verbal working memory,vocabulary expan-},
month = {jan},
number = {2},
pages = {251--268},
pmid = {10912254},
title = {{Language repair via metalinguistic means}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10912254 http://doi.wiley.com/10.1080/136828200247179},
volume = {35},
year = {2000}
}
@article{Hox1994,
author = {Hox, Joop J. and {De Leeuw}, Edith D.},
doi = {10.1007/BF01097014},
issn = {0033-5177},
journal = {Quality and Quantity},
month = {nov},
number = {4},
pages = {329--344},
publisher = {Kluwer Academic Publishers},
title = {{A comparison of nonresponse in mail, telephone, and face-to-face surveys}},
url = {http://link.springer.com/10.1007/BF01097014},
volume = {28},
year = {1994}
}
@article{Hu1999,
abstract = {This article examines the adequacy of the "rules of thumb" conventional cutoff crite- ria and several new alternatives for various fit indexes used to evaluate model fit in practice. Using a 2-index presentation strategy, which includes using the maximum likelihood (ML)-based standardized root mean squared residual (SRMR) and supple- menting it with either Tucker-Lewis Index (TLI), Bollen's (1989) Fit Index (BL89), Relative Noncentrality Index (RNI), Comparative Fit Index (CFI), Gamma Hat, Mc- Donald's Centrality Index (Mc), or root mean squared error of approximation (RMSEA), various combinations of cutoff values from selected ranges of cutoff crite- ria for the ML-based SRMR and a given supplemental fit index were used to calculate rejection rates for various types of true-population and misspecified models; that is, models with misspecified factor covariance(s) and models with misspecified factor loading(s). The results suggest that, for the ML method, a cutoff value close to .95 for TLI, BL89, CFI, RNI, and Gamma Hat; a cutoff value close to .90 for Mc; a cutoff value close to .08 for SRMR; and a cutoff value close to .06 for RMSEA are needed before we can conclude that there is a relatively good fit between the hypothesized model and the observed data. Furthermore, the 2-index presentation strategy is re- quired to reject reasonable proportions of various types of true-population and misspecified models. Finally, using the proposed cutoff criteria, the ML-based TLI, Mc, and RMSEA tend to overreject true-population models at small sample size and thus are less preferable when sample size is small.},
author = {Hu, Li‐tze and Bentler, Peter M.},
doi = {10.1080/10705519909540118},
isbn = {1070551115328},
issn = {1070-5511},
journal = {Structural Equation Modeling: A Multidisciplinary Journal},
month = {jan},
number = {1},
pages = {1--55},
pmid = {3384345},
publisher = {Taylor {\&} Francis Group},
title = {{Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10705519909540118},
volume = {6},
year = {1999}
}
@article{Hutzell1988,
abstract = {Reviews the literature concerning the Purpose in Life Test (PILT), which is used to measure the degree to which individuals experience life as meaningful. Data support the validity of the PILT as a measure of this variable. Part A, which has been used with adults and adolescents in a wide variety of settings, is well-suited for this purpose in noncompetitive situations. However, for groups that depart from middle-class American values, the generalization of the PILT is questioned. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
author = {Hutzell, R.},
isbn = {0191-3379(Print)},
issn = {0191-3379},
journal = {International Forum for Logotherapy},
keywords = {*Attitude Measures,*Literature Review,*Meaningfulness,Psychometrics},
number = {2},
pages = {89--101},
title = {{A review of the Purpose in Life Test.}},
volume = {11},
year = {1988}
}
@article{Ii1985,
abstract = {This research is a cross-validation and extension of Reynolds' (1982) short form of the Marlowe-Crowne Social Desirability Scale, using three separate groups (N = 233). Unlike Reynolds, the present researchers administered the 13 items as a separate entity, calculated Cronbach's Alpha for each sex, and also computed a test-retest correlation for one of the three groups. The authors conclude that this 13-item short form is a viable alternative to the full scale.},
author = {Ii, Avery Zook and Sipps, Gary J.},
doi = {10.1002/1097-4679(198503)41:2<236::AID-JCLP2270410217>3.0.CO;2-H},
issn = {00219762},
journal = {Journal of Clinical Psychology},
month = {mar},
number = {2},
pages = {236--238},
title = {{Cross-validation of a short form of the Marlowe-Crowne Social Desirability Scale}},
url = {http://doi.wiley.com/10.1002/1097-4679{\%}28198503{\%}2941{\%}3A2{\%}3C236{\%}3A{\%}3AAID-JCLP2270410217{\%}3E3.0.CO{\%}3B2-H},
volume = {41},
year = {1985}
}
@article{Ilieva2001,
author = {Ilieva, Janet and Baron, Steve and Healy, Nigel M},
journal = {International Journal of Market Research},
number = {3},
pages = {361--376},
title = {{On-line surveys in international marketing research: Pros and cons}},
volume = {44},
year = {2002}
}
@misc{jamovi2018,
author = {Jamovi project},
title = {{jamovi (Version 0.8)[Computer software]}},
url = {https://www.jamovi.org},
year = {2018}
}
@misc{JASP2018,
author = {{JASP Team}},
title = {{JASP (Version 0.8.6)[Computer software]}},
url = {https://jasp-stats.org/},
year = {2018}
}
@article{Joinson1999,
abstract = {It has been argued that behavior on the Internet differs from similar behavior in the "real world" (Joinson, 1998a), In the present study, participants completed measures of self-consciousness, social anxiety, self-esteem, and social desirability, using either the World-WideWeb(WWW) or pen and paper, and were assigned to either an anonyrnous or a nonanonyrnous condition. It was found that people re-ported lower social anxiety and social desirability and higher self-esteem when they were anonyrnous than when they were nonanonymous, Furthermore, participants also reported lower social anxiety and social desirability when they were using the Internet than when they were using paper-based methods, Contrast analyses supported the prediction that participants using the WWW anonyrnously would show the lowest levels of social desirability, whereas participants answering with pen and paper nonanony-mously would score highest on the same measure. Implications for the use of the Internet for the col-lection of psychological data are discussed.},
author = {Joinson, Adam},
doi = {10.3758/BF03200723},
isbn = {0743-3808 (Print)$\backslash$n0743-3808 (Linking)},
issn = {0743-3808},
journal = {Behavior Research Methods, Instruments, {\&} Computers},
number = {3},
pages = {433--438},
pmid = {10502866},
title = {{Social desirability, anonymity, and Intemet-based questionnaires}},
url = {https://doi.org/10.3758/bf03200723},
volume = {31},
year = {1999}
}
@article{Kashdan2007,
author = {Kashdan, Todd B. and Steger, Michael F.},
doi = {10.1007/s11031-007-9068-7},
issn = {0146-7239},
journal = {Motivation and Emotion},
month = {sep},
number = {3},
pages = {159--173},
title = {{Curiosity and pathways to well-being and meaning in life: Traits, states, and everyday behaviors}},
url = {http://link.springer.com/10.1007/s11031-007-9068-7},
volume = {31},
year = {2007}
}
@article{Kass1995a,
abstract = {In a 1935 paper and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this article we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology, and psychology. We emphasize the following points: From Jeffreys' Bayesian viewpoint, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory. Bayes factors offer a way of evaluating evidence in favor of a null hypothesis. Bayes factors provide a way of incorporating external information into the evaluation of evidence about a hypothesis. Bayes factors are very general and do not require alternative models to be nested. Several techniques are available for computing Bayes factors, including asymptotic approximations that are easy to compute using the output from standard packages that maximize likelihoods. In ''nonstandard'' statistical models that do not satisfy common regularity conditions, it can be technically simpler to calculate Bayes factor; than to derive non-Bayesian significance tests. The Schwarz criterion (or BIG) gives a rough approximation to the logarithm of the Bayes factor, which is easy to use and does not require evaluation of prior distributions. When one is interested in estimation or prediction, Bayes factors may be converted to weights to be attached to various models so that a composite estimate or prediction may be obtained that takes account of structural or model uncertainty. Algorithms have been proposed that allow model uncertainty to be taken into account when the class of models initially considered is very large. Bayes factors are useful for guiding an evolutionary model-building process. It is important, and feasible, to assess the sensitivity of conclusions to the prior distributions used.},
author = {Kass, Robert E. and Raftery, Adrian E.},
doi = {10.2307/2291091},
isbn = {0162-1459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
month = {jun},
number = {430},
pages = {773},
pmid = {2291091},
title = {{Bayes Factors}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476572 http://www.jstor.org/stable/2291091?origin=crossref},
volume = {90},
year = {1995}
}
@book{Keppel2004,
address = {Upper Saddle River, NJ},
author = {Keppel, Geoffrey and Wickens, Thomas},
edition = {4th},
publisher = {Prentice Hall},
title = {{Design and Analysis: A Researcher's Handbook}},
year = {2004}
}
@article{Keselman1998,
author = {Keselman, H. J. and Algina, James and Kowalchuk, R. K. and Wolfinger, R. D.},
doi = {10.1080/03610919808813497},
journal = {Communications in Statistics - Simulation and Computation},
number = {3},
pages = {591--604},
title = {{A comparison of two approaches for selecting covariance structures in the analysis of repeated measurements}},
volume = {27},
year = {1998}
}
@incollection{Knowles1992,
address = {New York},
author = {Knowles, Eric S. and Coker, Michelle C. and Cook, Deborah A. and Diercks, Steven R. and Irwin, Mary E. and Lundeen, Edward J. and Neville, John W. and Sibicky, Mark E.},
booktitle = {Context Efects in Social and Psychological Research},
chapter = {15},
editor = {Schwarz, Norbert and Sudman, Seymour},
pages = {221--236},
publisher = {Springer-Verlag},
title = {{Order Effects within Personality Measures}},
year = {1992}
}
@article{Lakens2017a,
abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
author = {Lakens, Dani{\"{e}}l},
doi = {10.1177/1948550617697177},
issn = {1948-5506},
journal = {Social Psychological and Personality Science},
keywords = {equivalence testing,null hypothesis significance testing,power analysis,research methods},
month = {may},
number = {4},
pages = {355--362},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Equivalence tests}},
url = {http://journals.sagepub.com/doi/10.1177/1948550617697177},
volume = {8},
year = {2017}
}
@article{Lakens2013,
author = {Lakens, Dani{\"{e}}l},
doi = {10.3389/fpsyg.2013.00863},
isbn = {1664-1078 (Print)},
issn = {16641078},
journal = {Frontiers in Psychology},
pmid = {24324449},
title = {{Calculating and reporting effect sizes to facilitate cumulative science: A practical primer for t-tests and ANOVAs}},
volume = {4},
year = {2013}
}
@article{Lakens2017,
author = {Lakens, Dani{\"{e}}l and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A J and Argamon, Shlomo Engelson and van Assen, Marcel A.L.M. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M and Caldwell, Aaron R and Cal, Ben Van and Zwaan, Rolf A.},
doi = {10.17605/OSF.IO/9S3Y6},
journal = {Nature Human Behaviour},
title = {{Justify Your Alpha}},
url = {https://osf.io/by2kc}
}
@article{Lee2012,
author = {Lee, Michael D and Wagenmakers, Eric-Jan},
file = {::},
title = {{Bayesian Cognitive Modeling: A Practical Course}},
url = {http://faculty.sites.uci.edu/mdlee/files/2011/03/BB{\_}Free.pdf},
year = {2012}
}
@article{Lewis2009,
abstract = {Despite experiments being increasingly conducted over the Internet, few studies have tested whether such experiments yield data equivalent to traditional methods' data. In the current study, data obtained via a traditional sampling method of undergraduate psychology students completing a paper-and-pencil survey (N¼107) were compared with data obtained from an Internet-administered survey to a sample of self-selected Internet-users (N¼94). The data examined were from a previous study that had examined the persuasiveness of health-related messages. To the extent that Internet data would be based on a sample at least as representative as data derived from a traditional student sample, it was expected that the two methodologies would yield equivalent data. Using formal tests of equivalence on persuasion outcomes, hypotheses of equivalence were generally supported. Additionally, the Internet sample was more diverse demographically than the student sample, identifying Internet samples as a valid alternative for future experimental research},
author = {Lewis, Ioni and Watson, Barry and White, Katherine Marie},
doi = {10.1080/00049530802105865},
isbn = {0004-9530},
issn = {0004-9530},
journal = {Australian Journal of Psychology},
number = {2},
pages = {107--116},
title = {{Internet versus paper-and-pencil survey methods in psychological experiments: Equivalence testing of participant responses to health-related messages}},
url = {https://doi.org/10.1080/00049530802105865},
volume = {61},
year = {2009}
}
@article{Lord1984,
author = {Lord, Charles G. and Lepper, Mark R. and Preston, Elizabeth},
doi = {10.1037/0022-3514.47.6.1231},
issn = {1939-1315},
journal = {Journal of Personality and Social Psychology},
number = {6},
pages = {1231--1243},
title = {{Considering the opposite: A corrective strategy for social judgment.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.47.6.1231},
volume = {47},
year = {1984}
}
@article{Ly2016,
abstract = {Harold Jeffreys pioneered the development of default Bayes factor hypothesis tests for standard statistical problems. Using Jeffreys's Bayes factor hypothesis tests, researchers can grade the decisiveness of the evidence that the data provide for a point null hypothesis H0 versus a composite alternative hypothesis H1. Consequently, Jeffreys's tests are of considerable theoretical and practical relevance for empirical researchers in general and for experimental psychologists in particular. To highlight this relevance and to facilitate the interpretation and use of Jeffreys's Bayes factor tests we focus on two common inferential scenarios: testing the nullity of a normal mean (i.e., the Bayesian equivalent of the t-test) and testing the nullity of a correlation. For both Bayes factor tests, we explain their development, we extend them to one-sided problems, and we apply them to concrete examples from experimental psychology.},
author = {Ly, Alexander and Verhagen, Josine},
doi = {10.1016/J.JMP.2015.06.004},
issn = {0022-2496},
journal = {Journal of Mathematical Psychology},
month = {jun},
pages = {19--32},
publisher = {Academic Press},
title = {{Harold Jeffreys's default Bayes factor hypothesis tests: Explanation, extension, and application in psychology}},
url = {https://www.sciencedirect.com/science/article/pii/S0022249615000383},
volume = {72},
year = {2016}
}
@article{MacLeod1992,
abstract = {Consistent with Tversky and Kahneman's (1973, 1974) availability heuristic hypothesis, the current study found a negative correlation between recall latency for past events and the perceived future probability of similar events. Furthermore, when the relative accessibility of memories of positive and negative events was experimentally manipulated using the Velten mood-induction procedure, the perceived future probabilities of similar events also changed in a manner consistent with the availability heuristic account. Reductions in recall latencies resulting from the mood manipulations were, as predicted, related to increases in perceived probability, and vice versa. Partial correlations indicated that this association between the observed patterns of changes in recall latencies and probability judgments could not be accounted for by the existence of independent associations between each of these effects and the magnitude of mood change.},
author = {MacLeod, C and Campbell, L},
issn = {0022-3514},
journal = {Journal of personality and social psychology},
month = {dec},
number = {6},
pages = {890--902},
pmid = {1460558},
title = {{Memory accessibility and probability judgments: an experimental evaluation of the availability heuristic.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/1460558},
volume = {63},
year = {1992}
}
@techreport{Media2016,
author = {Media},
title = {{The Total Audience Report: Q1 2016}},
year = {2016}
}
@article{Melton2008,
abstract = {Meaningful living is a central focus of several humanistic theories and therapies. Measurement of life meaning meets many obstacles, including pragmatic concerns, such as measuring subjective experiences, and theoretical objections often offered by humanistic psychologists. The purpose of this article is to summarize empirical efforts related to logotherapy, a humanistic-existential paradigm, to illustrate the utility of assessment within the larger context of humanistic psychology. An overview of five logotherapeutic measures of meaning is provided. These measures include the Purpose in Life test (PIL), the Life Purpose Questionnaire (LPQ), the Seeking of Noetic Goals test (SONG), the Meaning in Suffering Test (MIST), and the Life Attitude Profile Revised (LAP-R). Directions for use of such measures in future research are also offered.},
author = {Melton, Amanda M. A. and Schulenberg, Stefan E},
doi = {10.1080/08873260701828870},
isbn = {0887-3267},
issn = {1547-3333},
journal = {The Humanistic Psychologist},
number = {1},
pages = {31--44},
title = {{On the measurement of meaning: Logotherapy's empirical contributions to humanistic psychology.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1080/08873260701828870},
volume = {36},
year = {2008}
}
@misc{Mendoza2006,
abstract = {The article reviews the book "Design and Analysis: A Researcher's$\backslash$nHandbook," 4th ed., by Geoffrey Keppel and Thomas D. Wickens.},
author = {Mendoza, Jorge L},
booktitle = {Organizational Research Methods},
doi = {10.1198/tech.2005.s329},
isbn = {0132007754},
issn = {10944281},
keywords = {BOOKS -- Reviews,DESIGN {\&} Analysis: A Researcher's Handbook (Book),EXPERIMENTAL design,Geoffrey,KEPPEL,NONFICTION,Thomas D.,WICKENS},
number = {2},
pages = {248--251},
pmid = {1574},
title = {{Design and Analysis: A Researcher's Handbook (4th ed.).}},
url = {https://login.proxy.libraries.rutgers.edu/login?url=http://search.ebscohost.com.proxy.libraries.rutgers.edu/login.aspx?direct=true{\&}db=buh{\&}AN=22505593{\&}site=ehost-live},
volume = {9},
year = {2006}
}
@article{Meredith1993,
author = {Meredith, William},
doi = {10.1007/BF02294825},
file = {::},
issn = {0033-3123},
journal = {Psychometrika},
month = {dec},
number = {4},
pages = {525--543},
publisher = {Springer-Verlag},
title = {{Measurement invariance, factor analysis and factorial invariance}},
url = {http://link.springer.com/10.1007/BF02294825},
volume = {58},
year = {1993}
}
@article{Meyerson2003,
abstract = {This study evaluated the psychometric equivalency of Web-based research. The Sexual Boredom Scale was presented via the World-Wide Web along with five additional scales used to validate it. A subset of 533 participants that matched a previously published sample (Watt {\&} Ewing, 1996) on age, gender, and race was identified. An 8 x 8 correlation matrix from the matched Internet sample was compared via structural equation modeling with a similar 8 x 8 correlation matrix from the previously published study. The Internet and previously published samples were psychometrically equivalent. Coefficient alpha values calculated on the matched Internet sample yielded reliability coefficients almost identical to those for the previously published sample. Factors such as computer administration and uncontrollable administration settings did not appear to affect the results. Demographic data indicated an overrepresentation of males by about 6{\%} and Caucasians by about 13{\%} relative to the U.S. Census (2000). A total of 2,230 participants were obtained in about 8 months without remuneration. These results suggest that data collection on the Web is (1) reliable, (2) valid, (3) reasonably representative, (4) cost effective, and (5) efficient.},
author = {Meyerson, Paul and Tryon, Warren W},
doi = {10.3758/BF03195541},
isbn = {0743-3808},
issn = {0743-3808},
journal = {Behavior Research Methods, Instruments, {\&} Computers},
number = {4},
pages = {614--620},
pmid = {14748506},
title = {{Validating Internet research: A test of the psychometric equivalence of Internet and in-person samples}},
url = {https://doi.org/10.3758/bf03195541},
volume = {35},
year = {2003}
}
@misc{Morey2015c,
author = {Morey, Richard D},
title = {{On verbal categories for the interpretation of Bayes factors}},
url = {http://bayesfactor.blogspot.com/2015/01/on-verbal-categories-for-interpretation.html},
urldate = {2017-09-08},
year = {2015}
}
@misc{Morey2015b,
author = {Morey, Richard D and Rouder, Jeffrey N},
title = {{BayesFactor: Computation of Bayes Factors for common designs}},
url = {https://cran.r-project.org/package=BayesFactor},
year = {2015}
}
@incollection{Musch2000,
abstract = {Publisher Summary This chapter discusses the history of Web-based research experiments. The computerized experimenting was first introduced in 1970s. It revolutionized traditional laboratory research with its attractive new features such as standardized and controlled presentation of stimuli, item-branching capabilities, immediacy of data entry, elimination of missing responses, elimination of transcription costs and errors, and accurate measurements of response times. Today, most human experimental research in psychology is aided by computer automation. Usage of extending computerized experimenting beyond single PCs, local computer networks is collecting data rapidly and programs are being written in high-level languages such as C++, Pascal, or Delphi, or with program packages such as Super-Lab, Psy-Scope and MEL to optimize the data collecting tools in order to get quality data. Although, at the moment, the number of Web experiments is still small, a rapid growth can be predicted on the basis of the present result.},
author = {Musch, Jochen and Reips, Ulf-Dietrich},
booktitle = {Psychological Experiments on the Internet},
doi = {10.1016/B978-012099980-4/50004-6},
editor = {Birnbaum, M. H.},
isbn = {978-0-12-099980-4},
issn = {0120999803},
pages = {61--87},
publisher = {Elsevier},
title = {{A brief history of web experimenting}},
url = {http://iscience.deusto.es/wp-content/uploads/2010/04/ulf37.pdf{\%}5Cnhttp://www.sciencedirect.com/science/article/pii/B9780120999804500046 http://linkinghub.elsevier.com/retrieve/pii/B9780120999804500046},
year = {2000}
}
@article{Nosek2002,
abstract = {Differences between traditional laboratory research and Internet-based research require a review of basic issues of research methodology. These differences have implications for research ethics (e.g., absence of researcher, potential exposure of confidential data and/or identity to a third party, guaranteed debriefing) and security (e.g., confidentiality and anonymity, security of data transmission, security of data storage, and tracking participants over time). We also review basic design issues a researcher should consider before implementing an Internet study, including the problem of participant self-selection and loss of experimental control on the Internet laboratory. An additional challenge for Internet-based research is the increased opportunity for participant misbehavior, intentional or otherwise. We discuss methods to detect and minimize these threats to the validity of Internetbased research. [ABSTRACT FROM AUTHOR] Copyright of Journal of Social Issues is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
author = {Nosek, Brian A and Banaji, Mahzarin R and Greenwald, Anthony G},
doi = {10.1111/1540-4560.00254},
isbn = {00224537},
issn = {0022-4537},
journal = {Journal of Social Issues},
keywords = {COMPUTER network resources,ETHICS,INTERNET,SOCIAL psychology,SOCIAL science research,SOCIAL sciences -- Methodology},
number = {1},
pages = {161 -- 176},
pmid = {6194715},
title = {{E-Research: Ethics, security, design, and control in psychological research on the Internet}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=aph{\&}AN=6194715{\&}site=ehost-live{\&}scope=site},
volume = {58},
year = {2002}
}
@article{Olson2010,
abstract = {Expert reviews are frequently used as a questionnaire evaluation method but have received little empirical attention. Questions from two surveys are evaluated by six expert reviewers using a standardized evaluation form. Each of the questions has validation data available from records. Large inconsistencies in ratings across the six experts are found. Despite the lack of reliability, the average expert ratings successfully identify questions that had higher item nonresponse rates and higher levels of inaccurate reporting. This article provides empirical evidence that experts are able to discern questions that manifest data quality problems, even if individual experts vary in what they rate as being problematic. Compared to a publicly available computerized question evaluation tool, ratings by the human experts positively predict questions with data quality problems, whereas the computerized tool varies in success in identifying these questions. These results indicate that expert reviews have value in identifying question problems that result in lower survey data quality.},
author = {Olson, Kristen},
doi = {10.1177/1525822X10379795},
isbn = {1525-822X},
issn = {1525-822X},
journal = {Field Methods},
keywords = {expert reviewers,measurement error,pretesting,questionnaire design},
month = {nov},
number = {4},
pages = {295--318},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{An examination of questionnaire evaluation by expert reviewers}},
url = {http://journals.sagepub.com/doi/abs/10.1177/1525822X10379795 http://journals.sagepub.com/doi/10.1177/1525822X10379795},
volume = {22},
year = {2010}
}
@incollection{Panter1992,
address = {New York},
author = {Panter, Abigail T. and Tanaka, Jeffrey S. and Wellens, Tracy R.},
booktitle = {Context Efects in Social and Psychological Research},
chapter = {17},
editor = {Schwarz, Norbert and Sudman, Seymour},
pages = {249--264},
publisher = {Springer-Verlag},
title = {{Psychometrics of order effects}},
year = {1992}
}
@article{Pettit2002,
abstract = {Does the manifestation of response set effects in World-Wide Web (WWW) questionnaire responses differ from that in paper-and-pencil (PP) questionnaire responses? 2,649 volunteers responded to a WWW questionnaire, and 458 volunteers responded to a PP questionnaire consisting of 5 personality scales. Five response sets were examined. For 4 of them--random response, item nonresponse, extreme response, and acquiescent response--no statistically significant differences between PP and WWW data were found. The PP administration elicited a statistically higher number of errors (uncodable responses). An analysis of interitem and interscale correlations did not differentiate WWW and PP data. Data from the Marlowe-Crowne Social Desirability Scale, the Perfectionist Self-Presentation Scale, and the Computer Anxiety Scale did not differentiate WWW and PP data in terms of either means or scale reliabilities. It was concluded that WWW data may be comparable to PP data and that the WWW is a potentially useful and valid data collection tool. (PsycINFO Database Record (c) 2005 APA, all rights reserved)},
author = {Pettit, Frances Annie},
doi = {10.3758/BF03195423},
isbn = {Print 0743-3808 Electronic, Print Print},
issn = {0743-3808},
journal = {Behavior Research Methods, Instruments {\&} Computers},
keywords = {Research Methods {\&} Experimental Design [2260],WWW questionnaires,item response {\&} nonresponse,paper-and-pencil questionnaire,responses},
number = {1},
pages = {50--54},
pmid = {12060990},
title = {{A comparison of world-wide web and paper-and-pencil personality questionnaires}},
url = {http://www.psychonomic.org/},
volume = {34},
year = {2002}
}
@book{Petty1986,
address = {New York},
author = {Petty, R. E. and Cacioppo, J. T.},
publisher = {Springer-Verlag},
title = {{Communication and persuasion: Central and peripheral routes to attitude change}},
year = {1986}
}
@misc{Pinheiro2017,
author = {Pinheiro, J and Bates, Douglas and Debroy, S and Sarkar, D and Team, R Core},
title = {{nlme: Linear and nonlinear mixed effects models}},
url = {https://cran.r-project.org/package=nlme},
year = {2017}
}
@book{Posner1978,
address = {Hillsdale, NJ},
author = {Posner, M. I.},
publisher = {Erlbaum},
title = {{Chronometric explorations of mind}},
year = {1978}
}
@article{Preacher2003,
author = {Preacher, Kristopher J. and MacCallum, Robert C.},
doi = {10.1207/S15328031US0201_02},
issn = {1534-844X},
journal = {Understanding Statistics},
month = {feb},
number = {1},
pages = {13--43},
title = {{Repairing Tom Swift's Electric Factor Analysis Machine}},
url = {http://www.tandfonline.com/doi/abs/10.1207/S15328031US0201{\_}02},
volume = {2},
year = {2003}
}
@manual{R-base,
address = {Vienna, Austria},
author = {{R Core Team}},
organization = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {https://www.r-project.org/},
year = {2017}
}
@incollection{Reips2012,
abstract = {This chapter shows the major steps in collecting data on the Internet. The first section, Internet-Based Research, narrates the short history of Internet-based data-collection methods in psychological research, describes their characteristics, and presents a systematic overview of the four basic types of methods. Some notions about planning Internet based research lead to the second section, Generating a Web Experiment. The section describes an example and provides the reader with the opportunity to become active and experience Internet-based data-collection methods by creating and conducting a web experiment in a step-by-step fashion. The example introduces the important concepts of client-side versus server-side processing and illustrates a number of important techniques. The third section, Pretesting, emphasizes the need to take extra care in preparing the materials and procedure and evaluating their usability. Useful procedures in pretesting of Internet-based data collection are introduced, and the section explains how these procedures prevent methodological problems. In the fourth section, Recruitment, the pros and cons of various ways of attracting participants to Internet based studies are explained, concluding with the use of games as research environments on the Internet. The Data Analysis section explains a number of important issues such as raw data preservation, paradata, inclusion criteria, and technical variance. Furthermore, the section introduces several specific methods, including log file analysis. The concluding section looks at future trends and the continuing evolution of Internet-based methods and their use in behavioral and social research. (PsycINFO Database Record (c) 2013 APA, all rights reserved)},
address = {Washington},
author = {Reips, Ulf-Dietrich},
booktitle = {APA handbook of research methods in psychology, Vol 2: Research designs: Quantitative, qualitative, neuropsychological, and biological.},
doi = {10.1037/13620-017},
isbn = {1-4338-1079-4 (Hardcover); 978-1-43381-079-4 (Hardcover)},
issn = {1-4338-1005-0 (Hardcover); 978-1-43381-005-3 (Hardcover)},
pages = {291--310},
publisher = {American Psychological Association},
title = {{Using the Internet to collect data.}},
url = {http://psycnet.apa.org/bookcollections/13620/ http://content.apa.org/books/13620-017},
volume = {2},
year = {2012}
}
@article{Reips2002a,
abstract = {This article summarizes expertise gleaned from the first years of Internet-based experimental research and presents recommendations on: (1) ideal circumstances for conducting a study on the Internet; (2) what precautions have to be undertaken in Web experimental design; (3) which techniques have proven useful in Web experimenting; (4) which frequent errors and misconceptions need to be avoided; and (5) what should be reported. Procedures and solutions for typical challenges in Web experimenting are discussed. Topics covered include randomization, recruitment of samples, generalizability, dropout, experimental control, identity checks, multiple submissions, configuration errors, control of moti-vational confounding, and pre-testing. Several techniques are explained, including " warm-up, " " high hurdle, " password methods, " multiple site entry, " randomization, and the use of incentives. The article concludes by proposing sixteen stan-dards for Internet-based experimenting.},
author = {Reips, Ulf-Dietrich},
doi = {10.1026//1618-3169.49.4.243},
isbn = {1618-3169},
issn = {16183169},
journal = {Experimental Psychology},
keywords = {Experiment method,Internet research,Internet science,Internet-based experimenting,Methodology,Online research,Psychological experiment,Standards,Web experiment},
number = {4},
pages = {243--256},
pmid = {12455331},
title = {{Standards for Internet-based experimenting}},
volume = {49},
year = {2002}
}
@article{Reips2002,
abstract = {In the past few years, the Internet has been discovered as a new means to conduct psychological research. In particular, the World Wide Web (WWW) has increasingly been used as a promising setting for Internet-based experimenting (Musch {\&} Reips, 2000). However, Web experiments differ in fundamental aspects from traditional laboratory and field experiments. The present special issue of Experimental Psychology on Internet-based psychological experimenting provides a forum for the new methodological trends in current psychological research. It focuses on empirical examples of and methodological approaches to Web experimentation. (PsycINFO Database Record (c) 2007 APA, all rights reserved)},
author = {Reips, Ulf-Dietrich and Musch, Jochen},
doi = {10.1027//1618-3169.49.4.241},
isbn = {1618-3169},
journal = {Experimental Psychology},
keywords = {Experimental Psychology,Experimentation,Internet,Internet-based experimenting,Methodology,Scientific Communication,Trends,World Wide Web,journal Experimental Psychology,methodological trends},
number = {4},
pages = {241--242},
title = {{Special issue: Internet-based psychological experimenting}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=pdh{\&}AN=zea-49-4-241{\&}site=ehost-live{\%}5Cnhttp://psycnet.apa.org/journals/zea/49/4/241.pdf},
volume = {49},
year = {2002}
}
@manual{Revelle2017,
address = {Evanston, Illinois},
annote = {R package version 1.7.8},
author = {Revelle, William},
organization = {Northwestern University},
title = {{psych: Procedures for Psychological, Psychometric, and Personality Research}},
url = {https://cran.r-project.org/package=psych},
year = {2017}
}
@article{Reynolds1978,
abstract = {Developed, on the basis of responses from 608 undergraduate students to the 33-item Marlowe-Crowne Social Desirability Scale, three short forms of 11, 12, and 13 items. The psychometric characteristics of these three forms and three other short forms developed by Strahan and Gerbasi (1972) were investigated and comparisons made. Results, in the form of internal consistency reliability, item factor loadings, short form with Marlowe-Crowne total scale correlations, and correlations between Marlowe-Crowne short forms and the Edwards Social Desirability Scale, indicate that psychometrically sound short forms can be constructed. Comparisons made between the short forms examined in this investigation suggest the 13-item form as a viable substitute for the regular 33-item Marlowe-Crowne scale.},
author = {Reynolds, William M},
doi = {10.1002/1097-4679(198201)38:1<119::AID-JCLP2270380118>3.0.CO;2-I},
journal = {Journal of Clinical Psychology},
number = {1},
pages = {119--125},
title = {{Development of Reliable and Valid Short Forms of the Marlowe-Crowne Social Desirability Scale}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/1097-4679(198201)38:1{\%}3C119::AID-JCLP2270380118{\%}3E3.0.CO;2-I/abstract;jsessionid=A51F631724DCF60462F14A0F27E10A43.f02t02},
volume = {38},
year = {1982}
}
@article{Robitschek2012,
abstract = {The original Personal Growth Initiative Scale (PGIS; Robitschek, 1998) was unidimensional, despite theory identifying multiple components (e.g., cognition and behavior) of personal growth initiative (PGI). The present research developed a multidimensional measure of the complex process of PGI, while retaining the brief and psychometrically sound properties of the original scale. Study 1 focused on scale development, including theoretical derivation of items, assessing factor structure, reducing number of items, and refining the scale length using samples of college students. Study 2 consisted of confirmatory factor analysis with 3 independent samples of college students and community members. Lastly, Study 3 assessed test-retest reliability over 1-, 2-, 4-, and 6-week periods and tests of concurrent and discriminant validity using samples of college students. The final measure, the Personal Growth Initiative Scale-II (PGIS-II), includes 4 subscales: Readiness for Change, Planfulness, Using Resources, and Intentional Behavior. These studies provide exploratory and confirmatory evidence for the 4-factor structure, strong internal consistency for the subscales and overall score across samples, acceptable temporal stability at all assessed intervals, and concurrent and discriminant validity of the PGIS-II. Future directions for research and clinical practice are discussed.},
author = {Robitschek, Christine and Ashton, Matthew W. and Spering, Cynthia C. and Geiger, Nathaniel and Byers, Danielle and Schotts, G. Christian and Thoen, Megan a.},
doi = {10.1037/a0027310},
isbn = {1939-2168(Electronic);0022-0167(Print)},
issn = {0022-0167},
journal = {Journal of Counseling Psychology},
keywords = {come to counseling because,counseling is essentially a,either they identify something,eudaimonia,identifies this,measurement,multidimensional,people,personal growth initiative,process of personal growth,scale,themselves or someone else,they,want to change about},
number = {2},
pages = {274--287},
pmid = {22352950},
title = {{Development and psychometric evaluation of the Personal Growth Initiative Scale–II.}},
url = {http://psycnet.apa.org/index.cfm?fa=search.displayrecord{\&}uid=2012-04573-001},
volume = {59},
year = {2012}
}
@article{Rogers1993,
abstract = {Equivalency testing, a statistical method often used in biostatistics to determine the equivalence of 2 experimental drugs, is introduced to social scientists. Examples of equivalency testing are offered, and the usefulness of the method to the social scientist is discussed.},
author = {Rogers, J L and Howard, K I and Vessey, J T},
doi = {10.1037/0033-2909.113.3.553},
isbn = {0033-2909$\backslash$r1939-1455},
issn = {0033-2909},
journal = {Psychological Bulletin},
keywords = {Measurement,Methodological Studies,Research Methodology,Social Sciences,Sociometric Technics,Testing},
number = {3},
pages = {553--565},
pmid = {8316613},
title = {{Using significance tests to evaluate equivalence between two experimental groups.}},
volume = {113},
year = {1993}
}
@article{Rogers1974,
abstract = {Three experiments were done to determine the additivity of pairwise comparisons of three personality item characteristics: (1) Length, (2) Ambiguity and (3) Controversiality, using Sternberg's (1969) method. Each of these item attributes was chosen as an experimental factor to represent a stage in a postulated model of responding to personality items. Results indicated non-additivity of the experimental factors which was interpreted as supporting the postulated model.},
author = {Rogers, T.B.},
doi = {10.1016/0001-6918(74)90034-1},
issn = {0001-6918},
journal = {Acta Psychologica},
month = {jun},
number = {3},
pages = {205--213},
publisher = {North-Holland},
title = {{An analysis of the stages underlying the process of responding to personality items}},
url = {https://www.sciencedirect.com/science/article/pii/0001691874900341},
volume = {38},
year = {1974}
}
@article{Rouder2009,
author = {Rouder, Jeffrey N and Speckman, Paul L and Sun, Dongchu and Morey, Richard D and Iverson, Geoffrey},
doi = {10.3758/PBR.16.2.225},
isbn = {1069-9384(Print)},
issn = {1069-9384},
journal = {Psychonomic Bulletin {\&} Review},
number = {2},
pages = {225--237},
pmid = {19293088},
title = {{Bayesian t tests for accepting and rejecting the null hypothesis.}},
volume = {16},
year = {2009}
}
@incollection{Salancik1992,
address = {New York},
author = {Salancik, Gerald R. and Brand, Julianne F.},
booktitle = {Context Efects in Social and Psychological Research},
chapter = {16},
editor = {Schwarz, Norbert and Sudman, Seymour},
pages = {237--247},
publisher = {Springer-Verlag},
title = {{Context influences on the meaning of work}},
year = {1992}
}
@misc{Sanou2017,
abstract = {As ICTs continue to be a key enabler of economic and social development, ITU has a pivotal role in the global efforts to bridge the digital divide and in fostering an inclusive digital economy. I am confident that the discussions at the World Telecommunication Development Conference 2017, to be held in Buenos Aires, Argentina from 9 to 20 October 2017, under the theme of “ICT for Sustainable Development Goals”, will contribute to the mapping of results-oriented strategies aimed at hastening the pace of countries towards the timely attainment of the SDGs and their related target.},
author = {Sanou, Brahima},
booktitle = {Itu},
isbn = {4043295731},
issn = {14713063},
month = {jul},
pages = {1--8},
pmid = {14770351},
title = {{ICT Facts and Figures 2017}},
url = {http://www.itu.int/en/ITU-D/Statistics/Documents/facts/ICTFactsFigures2017.pdf},
year = {2017}
}
@article{Schuirmann1987,
abstract = {The statistical test of hypothesis of no difference between the average bioavailabilities of two drug formulations, usually supplemented by an assessment of what the power of the statistical test would have been if the true averages had been inequivalent, continues to be used in the statistical analysis of bioavailability/bioequivalence studies. In the present article, this Power Approach (which in practice usually consists of testing the hypothesis of no difference at level 0.05 and requiring an estimated power of 0.80) is compared to another statistical approach, the Two One-Sided Tests Procedure, which leads to the same conclusion as the approach proposed by Westlake based on the usual (shortest) 1-2 alpha confidence interval for the true average difference. It is found that for the specific choice of alpha = 0.05 as the nominal level of the one-sided tests, the two one-sided tests procedure has uniformly superior properties to the power approach in most cases. The only cases where the power approach has superior properties when the true averages are equivalent correspond to cases where the chance of concluding equivalence with the power approach when the true averages are not equivalent exceeds 0.05. With appropriate choice of the nominal level of significance of the one-sided tests, the two one-sided tests procedure always has uniformly superior properties to the power approach. The two one-sided tests procedure is compared to the procedure proposed by Hauck and Anderson.},
author = {Schuirmann, Donald J.},
doi = {10.1007/BF01068419},
isbn = {0090-466X (Print)$\backslash$r0090-466X (Linking)},
issn = {0090466X},
journal = {Journal of Pharmacokinetics and Biopharmaceutics},
keywords = {bioavailability,bioequivalence,hypothesis testing,interval hypotheses},
number = {6},
pages = {657--680},
pmid = {3450848},
title = {{A comparison of the Two One-Sided Tests Procedure and the Power Approach for assessing the equivalence of average bioavailability}},
volume = {15},
year = {1987}
}
@article{Schuldt1994,
abstract = {The article discusses the role of electronic mail surveys in marketing research, with emphasis on the benefits that may be obtained as compared with those of mail questionnaires. The author has presented an exploratory study on response rates from both surveys indicating that the response rate for electronic mail was low and confined primarily to respondents who had a greater interest in technology than the average respondent. He noted however that further study into electronic mail surveying is warranted since it may become the standard data collection method in the 21st century.},
author = {Schuldt, Barbara a. and Totten, Jeff W.},
isbn = {1040359019},
issn = {10408460},
journal = {Marketing Research},
pages = {36--39},
pmid = {7754995},
title = {{Electronic mail vs. mail survey response rates}},
volume = {6},
year = {1994}
}
@article{Schulenberg2004,
abstract = {The purpose of this study was to expand the psychometric properties of four Logotherapy measures, namely the Purpose-in-Life test (PIL), the Life Purpose Questionnaire (LPQ), the Seeking of Noetic Goals test (SONG), and the Meaning in Suffering Test (MIST) in order to better understand how they relate to one another and to a measure of psychological distress (the Outcome Questionnaire-OQ-45.2). The sample was composed of 341 undergraduate students from a medium-sized university located in the southern United States of America. The total scores of the measures were found to be internally consistent. Two subscales of the MIST were found to have questionable reliability. The PIL and the LPQ, both general measures of meaning, appear to have comparable psychometric properties. The PIL and the LPQ share 64{\%} common variance and are similarly correlated with the remaining measures of meaning and the OQ-45.2. However, the LPQ tended to be preferred by respondents in many respects.},
author = {Schulenberg, Stefan E},
issn = {15277143},
journal = {North American Journal of Psychology},
number = {3},
pages = {477--492},
title = {{A psychometric investigation of logotherapy measures and the Outcome Questionnaire (OQ-45.2)}},
volume = {6},
year = {2004}
}
@article{Schulenberg2010,
abstract = {This study's purpose was to use confirmatory factor analysis to compare published factor-analytic models of the 20-item Purpose in Life test (PIL) to identify the one that provides the best fit to the data. To date many different models have been described, with limited evidence to support whether they are replicable. This study utilized data from undergraduates (N = 620) from a medium-sized university located in the southern United States. Ten different PIL models were tested, with support found for the two-factor model (exciting life, purposeful life) of Morgan and Farsides. Recommendations and implications for research are provided.},
author = {Schulenberg, Stefan E and Melton, Amanda M. A.},
doi = {10.1007/s10902-008-9124-3},
isbn = {1389-4978},
issn = {13894978},
journal = {Journal of Happiness Studies},
keywords = {Confirmatory factor analysis,Logotherapy,Meaning,PIL,Purpose in life},
number = {1},
pages = {95--111},
title = {{A confirmatory factor-analytic evaluation of the purpose in life test: Preliminary psychometric support for a replicable two-factor model}},
volume = {11},
year = {2010}
}
@article{Schulenberg2011,
abstract = {General meaning-in-life measure. Not cancer specific.},
author = {Schulenberg, Stefan E and Schnetzer, Lindsay W. and Buchanan, Erin M},
doi = {10.1007/s10902-010-9231-9},
isbn = {1389-4978, 1389-4978},
issn = {13894978},
journal = {Journal of Happiness Studies},
keywords = {Confirmatory factor analysis,Logotherapy,Meaning,Purpose in Life test,Short form},
month = {oct},
number = {5},
pages = {861--876},
publisher = {Springer Netherlands},
title = {{The Purpose in Life Test-Short Form: Development and Psychometric Support}},
url = {http://link.springer.com/10.1007/s10902-010-9231-9},
volume = {12},
year = {2011}
}
@article{Schulenberg2001,
abstract = {This study examined the equivalence of the conventional and computerized versions of the Beck Depression Inventory-II (BDI-II), taking into account that computer aversion may negatively impact computer-administered BDI-II scores by elevating them. Participants were 180 psychology undergraduate students from a medium-sized midwestern university. Participants were divided into four experimental groups. Each group was administered the BDI-II twice in various combinations (conventional only, computerized only, conventional and computerized and vice versa). All participants completed measures of computer aversion and computer experience. Participants who received both versions of the BDI-II were also asked to specify their preference for method of administration. Independent samples t-test results indicated that the computerized and of the BDI-II may be considered equivalent in terms of measurement validity. Implications for future research are discussed.},
author = {Schulenberg, Stefan E and Yutrzenka, Barbara A.},
doi = {10.1007/s12144-001-1008-1},
isbn = {0737-8262},
issn = {0737-8262},
journal = {Current Psychology},
number = {3},
pages = {216--230},
title = {{Equivalence of computerized and conventional versions of the Beck Depression Inventory-II (BDI-II)}},
url = {http://link.springer.com/10.1007/s12144-001-1008-1},
volume = {20},
year = {2001}
}
@article{Schulenberg1999,
abstract = {The use of computerized psychological assessment is a growing practice among contemporary mental health professionals. Many popular and frequently used paper-and-pencil instruments have been adapted into computerized versions. Although equivalence for many instruments has been evaluated and supported, this issue is far from resolved. This literature review deals with recent research findings that suggest that computer aversion negatively impacts computerized assessment, particularly as it relates to measures of negative affect. There is a dearth of equivalence studies that take into account computer aversion's potential impact on the measurement of negative affect. Recommendations are offered for future research in this area.},
author = {Schulenberg, Stefan E and Yutrzenka, Barbara A.},
doi = {10.3758/BF03207726},
isbn = {0743-3808 (Print)$\backslash$r0743-3808 (Linking)},
issn = {0743-3808},
journal = {Behavior Research Methods, Instruments, {\&} Computers},
number = {2},
pages = {315--321},
pmid = {10495816},
title = {{The equivalence of computerized and paper-and-pencil psychological instruments: Implications for measures of negative affect.}},
url = {https://doi.org/10.3758/bf03207726},
volume = {31},
year = {1999}
}
@book{Schwarz1992,
address = {New York},
author = {Schwarz, Norbert and Sudman, Seymour},
isbn = {0387977058},
pages = {353},
publisher = {Springer-Verlag},
title = {{Context Effects in Social and Psychological Research}},
url = {https://books.google.com/books?hl=en{\&}lr={\&}id=SeFVBgAAQBAJ{\&}oi=fnd{\&}pg=PT11{\&}dq=context+effects+in+social+and+psychological+research{\&}ots=kw4b1qme3V{\&}sig=RVI1jARreKnpA4UiOa-ImtZDgto{\#}v=onepage{\&}q=context effects in social and psychological research{\&}f=f},
year = {1992}
}
@article{Smith1997,
author = {Smith, Michael A. and Leigh, Brant},
doi = {10.3758/BF03210601},
issn = {0743-3808},
journal = {Behavior Research Methods, Instruments, {\&} Computers},
month = {dec},
number = {4},
pages = {496--505},
title = {{Virtual subjects: Using the Internet as an alternative source of subjects and research environment}},
url = {http://www.springerlink.com/index/10.3758/BF03210601},
volume = {29},
year = {1997}
}
@article{Smithson2001,
abstract = {The advantages that confidence intervals have over null-hypothesis significance testing have been presented on many occasions to researchers in psychology. This article provides a practical introduction to methods of constructing confidence intervals for multiple and partial R2 and related parameters in multiple regression models based on "noncentral" F and $\chi$2 distributions. Until recently, these techniques have not been widely available due to their neglect in popular statistical textbooks and software. These difficulties are addressed here via freely available SPSS scripts and software and illustrations of their use. The article concludes with discussions of implications for the interpretation of findings in terms of noncentral confidence intervals, alternative measures of effect size, the relationship between noncentral confidence intervals and power analysis, and the design of studies.},
author = {Smithson, Michael},
doi = {10.1177/00131640121971392},
isbn = {0013-1644},
issn = {0013-1644},
journal = {Educational and Psychological Measurement},
month = {aug},
number = {4},
pages = {605--632},
title = {{Correct confidence intervals for various regression effect sizes and parameters: The importance of noncentral distributions in computing intervals}},
volume = {61},
year = {2001}
}
@article{Smyth2006,
abstract = {For survey researchers, it is common practice to use the check-all question format in Web and mail surveys but to convert to the forced-choice question format in telephone surveys. The assumption underlying this practice is that respondents will answer the two formats similarly. In this research note we report results from 16 experimental comparisons in two Web surveys and a paper survey conducted in 2002 and 2003 that test whether the check-all and forced-choice formats pro-duce similar results. In all 16 comparisons, we find that the two question formats do not perform similarly; respondents endorse more options and take longer to answer in the forced-choice format than in the check-all format. These findings suggest that the forced-choice question format encourages deeper processing of response options and, as such, is pref-erable to the check-all format, which may encourage a weak satisficing response strategy. Additional analyses show that neither acquiescence bias nor item nonresponse seem to pose substantial problems for use of the forced-choice question format in Web surveys.},
author = {Smyth, Jolene D.},
doi = {10.1093/poq/nfj007},
isbn = {0033-362X$\backslash$r1537-5331},
issn = {0033-362X},
journal = {Public Opinion Quarterly},
month = {mar},
number = {1},
pages = {66--77},
pmid = {20837647},
title = {{Comparing check-all and forced-choice question formats in web surveys}},
url = {https://academic.oup.com/poq/article-lookup/doi/10.1093/poq/nfj007},
volume = {70},
year = {2006}
}
@article{Steenkamp1998,
author = {Steenkamp, Jan‐Benedict E. M. and Baumgartner, Hans},
doi = {10.1086/209528},
file = {::},
issn = {0093-5301},
journal = {Journal of Consumer Research},
month = {jun},
number = {1},
pages = {78--107},
publisher = {Oxford University Press},
title = {{Assessing Measurement Invariance in Cross‐National Consumer Research}},
url = {https://academic.oup.com/jcr/article-lookup/doi/10.1086/209528},
volume = {25},
year = {1998}
}
@incollection{Strack1987,
author = {Strack, Fritz and Martin, Leonard L.},
doi = {10.1007/978-1-4612-4798-2_7},
file = {::},
pages = {123--148},
publisher = {Springer, New York, NY},
title = {{Thinking, Judging, and Communicating: A Process Account of Context Effects in Attitude Surveys}},
url = {http://link.springer.com/10.1007/978-1-4612-4798-2{\_}7},
year = {1987}
}
@article{Strack1985,
abstract = {Three experiments showed that subjects' ratings of general life satisfaction depended not only on the hedonic quality of the life experiences they happened to recall but also on the way in which they thought about them. Specifically, the hedonic quality of present life events influenced subjects' judgments of well-being in the same di-rection. The hedonic quality of past events, however, had a congruent impact on well-being judgments only when thinking about them elicited affect in the present but otherwise had a contrast effect on these judgments. Two factors were found to determine if thinking about the past elicits affect: whether subjects describe the events vividly and in detail or only mention them briefly, and whether subjects describe how the events occurred rather than why they occurred. Possible mediating mechanisms and implications of these results are discussed.},
author = {Strack, Fritz and Schwarz, Norbert and Gschneidinger, Elisabeth},
file = {::},
journal = {Journal of Personality and Social Psychology},
number = {6},
pages = {1460--1469},
title = {{Happiness and Reminiscing: The Role of Time Perspective, Affect, and Mode of Thinking}},
url = {https://jshellman-reminiscence.wiki.uml.edu/file/view/reminiscing.pdf},
volume = {49},
year = {1985}
}
@article{Strack2009,
abstract = {This study investigated the relationship among the variables of perceived coercion, psychiatric symptoms, empowerment, and meaning in an inpatient sample of individuals with serious mental illness (N=94). It was hypothesized that empowerment would be strongly related to the level of psychiatric symptoms and meaning, but not significantly related to coercion. Participants were recruited from inpatient facilities and completed the MacArthur Perceived Coercion Scale, the Empowerment Scale, the Brief Symptom Inventory, and the Life Purpose Questionnaire. The results suggested that empowerment is significantly associated with the presence of meaning in one's life as well as the level of psychiatric symptoms. Coercion was not significantly related to empowerment. Implications for clinical practice are discussed.},
author = {Strack, Kristen M. and Schulenberg, Stefan E},
doi = {10.1002/jclp.20607},
isbn = {1097-4679},
issn = {00219762},
journal = {Journal of Clinical Psychology},
keywords = {Coercion,Empowerment,Logotherapy,Meaning,SMI,Serious mental illness},
number = {10},
pages = {1137--1148},
pmid = {19670431},
title = {{Understanding empowerment, meaning, and perceived coercion in individuals with serious mental illness}},
volume = {65},
year = {2009}
}
@book{Tabachnick2012,
address = {Boston, MA},
author = {Tabachnick, B. G. and Fidell, L. S.},
edition = {6th},
isbn = {978-0205849574},
issn = {0956-7976},
pmid = {1577},
publisher = {Pearson},
title = {{Using Multivariate Statistics}},
year = {2012}
}
@article{Tesser1978,
author = {Tesser, A.},
journal = {Advances in Experimental Social Psychology},
pages = {289--338},
title = {{Self-generated attitude change}},
volume = {11},
year = {1978}
}
@article{Tourangeau1988,
abstract = {We begin this article with the assumption that attitudes are best understood as structures in long-term memory, and we look at the implications of this view for the response process in attitude surveys. More specifically, we assert that an answer to an attitude question is the product of a four-stage process. Respondents first interpret the attitude question, determining what attitude the ques-tion is about. They then retrieve relevant beliefs and feelings. Next, they apply these beliefs and feelings in rendering the appropriate judgment. Finally, they use this judgment to select a response. All four of the component processes can be affected by prior items. The prior items can provide a framework for interpreting later questions and can also make some responses appear to be redundant with earlier answers. The prior items can prime some beliefs, making them more accessible to the retrieval process. The prior items can suggest a norm or standard of comparison for making the judgment. Finally, the prior items can create consistency pressures or pressures to appear moderate. Because of the multiple processes involved, context effects are difficult to predict and sometimes difficult to replicate. We attempt to sort out when context is likely to affect later responses and include a list of the variables that affect the size and direction of the effects of context.},
author = {Tourangeau, Roger and Rasinski, Kenneth A},
file = {::},
journal = {Psychological Bulletin},
number = {3},
pages = {299--314},
title = {{Cognitive Processes Underlying Context Effects in Attitude Measurement}},
url = {http://www.jwalkonline.org/docs/Grad Classes/Fall 07/Cog Surv/project 2/articles/tourangeau rasinski cog processes context eff.pdf},
volume = {103},
year = {1988}
}
@book{Tourangeau1999,
abstract = {The question of how exactly the human mind reacts to a question and produces an answer is extremely complex. In The Psychology of Survey Response, Tourangeau, Rips, and Rasinski (TRR) present a model of how a person formulates and reports responses to survey questions. The authors use their model to examine factors that affect the interview and response process in the context of asking questions about dates, frequencies, numerical quantities, and attitudes. They compare and contrast their model to other models and provide illustrations from the literature on survey research methods, cognitive science research on memory and estimation, and several major surveys.},
address = {Cambridge, UK},
author = {Tourangeau, Roger and Rips, Lance J and Rasinski, Kenneth},
booktitle = {Response},
isbn = {0033-362X},
issn = {0033-362X},
keywords = {review},
pmid = {4090464},
publisher = {Cambridge University Press},
title = {{The Psychology of Survey Response}},
year = {1999}
}
@article{Tversky1973,
abstract = {This paper explores a judgmental heuristic in which a person evaluates the frequency of classes or the probability of events by availability, i.e., by the ease with which relevant instances come to mind. In general, availability is correlated with ecological frequency, but it is also affected by other factors. Consequently, the reliance on the availability heuristic leads to systematic biases. Such biases are demonstrated in the judged frequency of classes of words, of combinatorial outcomes, and of repeated events. The phenomenon of illusory correlation is explained as an availability bias. The effects of the availability of incidents and scenarios on subjective probability are discussed.},
author = {Tversky, Amos and Kahneman, Daniel},
doi = {10.1016/0010-0285(73)90033-9},
issn = {0010-0285},
journal = {Cognitive Psychology},
month = {sep},
number = {2},
pages = {207--232},
publisher = {Academic Press},
title = {{Availability: A heuristic for judging frequency and probability}},
url = {https://www.sciencedirect.com/science/article/pii/0010028573900339},
volume = {5},
year = {1973}
}
@article{Valentine2017,
author = {Valentine, Kathrene D and Buchanan, Erin M and Scofield, John E and Beauchamp, Marshall},
doi = {10.17605/osf.io/9hp7y},
keywords = {Bayes Factors,Observation Oriented Modeling,Psychology,Quantitative Psychology,Social Statistics,Social and Behavioral Sciences,evidence,evidentiary value,null hypothesis testing,p values,statistics},
publisher = {Open Science Framework},
title = {{Beyond p-values: Utilizing Multiple Estimates to Evaluate Evidence}},
url = {https://osf.io/9hp7y/},
year = {2017}
}
@article{VanBuuren2011,
abstract = {The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice, which extends the functionality of mice 1.0 in several ways. In mice , the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. mice adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
author = {{Van Buuren}, Stef and Groothuis-Oudshoorn, Karin},
doi = {10.18637/jss.v045.i03},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {3},
pages = {1--67},
title = {{mice: Multivariate Imputation by Chained Equations in R}},
url = {http://www.jstatsoft.org/v45/i03/},
volume = {45},
year = {2011}
}
@article{Wagenmakers2016,
abstract = {The practical advantages of Bayesian inference are demonstrated here through two concrete examples. In the first
example, we wish to learn about a criminal's IQ: a problem of parameter estimation. In the second example, we wish
to quantify and track support in favor of the null hypothesis that Adam Sandler movies are profitable regardless of their
quality: a problem of hypothesis testing. The Bayesian approach unifies both problems within a coherent predictive
framework, in which parameters and models that predict the data successfully receive a boost in plausibility, whereas
parameters and models that predict poorly suffer a decline. Our examples demonstrate how Bayesian analyses can be
more informative, more elegant, and more flexible than the orthodox methodology that remains dominant within the
field of psychology.},
author = {Wagenmakers, E M ; and Morey, R D ; and Lee, M. D.},
doi = {10.1177/0963721416643289},
file = {::},
journal = {Current Directions in Psychological Science},
number = {3},
pages = {169--176},
title = {{Bayesian Benefits for the Pragmatic Researcher}},
url = {https://pure.uva.nl/ws/files/9217360/Bayesian{\_}Benefits{\_}for{\_}the{\_}Pragmatic{\_}Researcher.pdf},
volume = {25},
year = {2016}
}
@article{Wagenmakers2007,
archivePrefix = {arXiv},
arxivId = {1105.1486},
author = {Wagenmakers, Eric-Jan},
doi = {10.3758/BF03194105},
eprint = {1105.1486},
isbn = {1069-9384},
issn = {1069-9384},
journal = {Psychonomic Bulletin {\&} Review},
month = {oct},
number = {5},
pages = {779--804},
pmid = {18087943},
title = {{A practical solution to the pervasive problems of p values}},
url = {http://www.springerlink.com/index/10.3758/BF03194105},
volume = {14},
year = {2007}
}
@book{Webb1966,
address = {Chicago},
author = {Webb, E. S. and Campbell, D. T. and Schwartz, R. D. and Sechrest, L.},
publisher = {Rand McNally},
title = {{Unobtrusive measures: Nonreactive research in the social sciences}},
year = {1966}
}
@article{Weigold2013,
abstract = {Self-report survey-based data collection is increasingly carried out using the Internet, as opposed to the traditional paper-and-pencil method. However, previous research on the equivalence of these methods has yielded inconsistent findings. This may be due to methodological and statistical issues present in much of the literature, such as nonequivalent samples in different conditions due to recruitment, participant self-selection to conditions, and data collection procedures, as well as incomplete or inappropriate statistical procedures for examining equivalence. We conducted 2 studies examining the equivalence of paper-and-pencil and Internet data collection that accounted for these issues. In both studies, we used measures of personality, social desirability, and computer self-efficacy, and, in Study 2, we used personal growth initiative to assess quantitative equivalence (i.e., mean equivalence), qualitative equivalence (i.e., internal consistency and intercorrelations), and auxiliary equivalence (i.e., response rates, missing data, completion time, and comfort completing questionnaires using paper-and-pencil and the Internet). Study 1 investigated the effects of completing surveys via paper-and-pencil or the Internet in both traditional (i.e., lab) and natural (i.e., take-home) settings. Results indicated equivalence across conditions, except for auxiliary equivalence aspects of missing data and completion time. Study 2 examined mailed paper-and-pencil and Internet surveys without contact between experimenter and participants. Results indicated equivalence between conditions, except for auxiliary equivalence aspects of response rate for providing an address and completion time. Overall, the findings show that paper-and-pencil and Internet data collection methods are generally equivalent, particularly for quantitative and qualitative equivalence, with nonequivalence only for some aspects of auxiliary equivalence.},
author = {Weigold, Arne and Weigold, Ingrid K and Russell, Elizabeth J},
doi = {10.1037/a0031607},
isbn = {1082-989X$\backslash$r1939-1463},
issn = {1939-1463},
journal = {Psychological Methods},
keywords = {auxiliary equivalence,equivalence testing,qualitative equivalence,quantitative equivalence},
number = {1},
pages = {53--70},
pmid = {23477606},
title = {{Examination of the equivalence of self-report survey-based paper-and-pencil and internet data collection methods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23477606},
volume = {18},
year = {2013}
}
@article{Wickham2007,
author = {Wickham, Hadley},
doi = {10.18637/jss.v021.i12},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {12},
title = {{Reshaping data with the reshape package}},
url = {http://www.jstatsoft.org/v21/i12/},
volume = {21},
year = {2007}
}
@article{Wiechmann2003,
abstract = {Organizations are increasingly using computerized tests (e.g., multimedia, web-based, computer adaptive testing) in selection systems. A 2 (mode of presentation: paper-and- pencil – computerized)2 (technical level of the job: high technical job–low technical job)2 (selection decision: rejected or selected) between subjects design was used to assess proposed relationships between reactions to tests, their antecedents, and their consequences. While test-takers' post-test perceptions did not significantly differ as a result of mode of administration, computer anxiety and experience with computing were important factors in performing successfully. Significant relationships were found between post-feedback reactions and test-takers' intentions. The discussion highlights implications for implementing computerized selection tools.},
author = {Wiechmann, Darin and Ryan, Ann Marie Ryan},
doi = {10.1111/1468-2389.00245},
isbn = {0965-075X$\backslash$r1468-2389},
issn = {0965-075X},
journal = {International Journal of Selection and Assessment},
number = {2-3},
pages = {215--229},
title = {{Reactions to computerized testing in selection contexts}},
url = {http://doi.wiley.com/10.1111/1468-2389.00245},
volume = {11},
year = {2003}
}
@article{Worthington2006,
abstract = {The authors conducted a content analysis on new scale development articles appearing in the Journal of Counseling Psychology during 10 years (1995 to 2004). The authors analyze and discuss characteristics of the exploratory and confirmatory factor analysis procedures in these scale development studies with respect to sample characteristics, factorability, extraction methods, rotation methods, item deletion or retention, factor retention, and model fit indexes. The authors uncovered a variety of specific practices that were at variance with the current literature on factor analysis or structural equa- tion modeling. They make recommendations for best practices in scale development research in counseling psychology using exploratory and confirmatory factor analysis.},
archivePrefix = {arXiv},
arxivId = {http://tcp.sagepub.com/cgi/content/abstract/34/6/806},
author = {Worthington, Roger L. and Whittaker, Tiffany a.},
doi = {10.1177/0011000006288127},
eprint = {/tcp.sagepub.com/cgi/content/abstract/34/6/806},
isbn = {0011-0000},
issn = {0011-0000},
journal = {The Counseling Psychologist},
number = {6},
pages = {806--838},
primaryClass = {http:},
title = {{Scale development research: A content analysis and recommendations for best practices}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.936.9780{\&}rep=rep1{\&}type=pdf},
volume = {34},
year = {2006}
}
@article{Zhang2000,
abstract = {The Internet provides opportunities to conduct surveys more efficiently and effectively than traditional means. This article reviews previous studies that use the Inter- net for survey research. It discusses the methodological issues and problems associated with this new approach. By presenting a case study, it seeks possible solutions to some of the problems, and explores the potential the Internet can offer to survey researchers.},
author = {Zhang, Yin},
doi = {10.1002/(SICI)1097-4571(2000)51:1<57::AID-ASI9>3.0.CO;2-W},
isbn = {1058460070131},
issn = {0002-8231},
journal = {Journal of the American Society for Information Science},
number = {1},
pages = {57--68},
title = {{Using the Internet for survey research: A case study}},
url = {http://doi.wiley.com/10.1002/{\%}28SICI{\%}291097-4571{\%}282000{\%}2951{\%}3A1{\%}3C57{\%}3A{\%}3AAID-ASI9{\%}3E3.0.CO{\%}3B2-W},
volume = {51},
year = {2000}
}
