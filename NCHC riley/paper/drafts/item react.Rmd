---
title             : "The Delivery Matters: Examining Reactivity in Question Answering"
shorttitle        : "Item Reactivity"

author: 
  - name          : "Erin M. Buchanan"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "901 S. National Ave"
    email         : "erinbuchanan@missouristate.edu"
  - name          : "Riley E. Forman"
    affiliation   : "1"
  - name          : "Jeffrey M. Pavlacic"
    affiliation   : "2"
  - name          : "Rachel Swadley"
    affiliation   : "1"
  - name          : "Becca N. Johnson"
    affiliation   : "1"
  - name          : "Stefan E. Schulenberg"
    affiliation   : "2"
  

affiliation:
  - id            : "1"
    institution   : "Missouri State University"
  - id            : "2"
    institution   : "University of Mississippi"

author_note: >
  Complete departmental affiliations for each author (note the indentation, if you start a new paragraph).

  Enter author note here.

abstract: >
  Scales that are psychometrically sound, meaning those that meet established standards regarding reliability and validity while measuring one or more constructs of interest - are customarily evaluated based on a set modality of delivery (i.e. via the Internet, handwritten, etc.) and administration (fixed item order). Deviating from an established administration profile could result in non-equivalent response patterns, indicating the possible evaluation of a dissimilar construct. Furthermore, item grouping may influence response patterns. Randomizing item administration may alter or eliminate these effects. Therefore, we examined the differences in scale relationships for computer versus handwritten scale delivery for two scales measuring meaning/purpose in life. These scales have questions about suicidality, depression, and life goals that may cause reactivity (i.e. a changed response to a second item based on the answer to the first item).  

  
keywords          : "scales, reactivity, item answering"
wordcount         : "X"

bibliography      : ["item react.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---
Introduction will go here after we figure it out. 

```{r libraries, include = FALSE}
library(papaja)
library(car)
library(memisc)
library(moments)
library(mice)
library(monomvn)
library(psych)
library(TOSTER)
```

# Method

## Participants
The data analyzed for this study originated from two separate institutions. The sample population consisted of undergraduate students at either a large Midwestern or large Southern university, placing the approximate age of participants at around 18-22. Table XX includes the information about all datasets. Several datasets have been used previously for larger studies, and those citations are included in the table. Only two scales were used from each dataset, as described below. Participants were generally enrolled in an introductory psychology course that served as a general education requirement for the university. As part of the curriculum, the students were encouraged to participate in psychology research programs, resulting in their involvement in this study. These participants were given course credit for their participation. 

## Materials
Of the surveys included within each larger study, two questionnaires were utilized: the Purpose in Life Questionnaire (PIL; Crumbaugh & Maholick, 1964, 1969) and the Life Purpose Questionnaire (LPQ; Hablas & Hutzell, 1982; Hutzell, 1989).

### The Purpose in Life Questionnaire
The PIL is a 20-item questionnaire that assesses perceived meaning and life purpose. Items are structured in a 7-point Likert type response format; however, each item has different anchoring points that focus on item content. Total scores are created by summing the items, resulting in a range of 20 to 140 for the overall score. The reliability for the scale is generally high, ranging from .70 to .90 (Schulenberg, 2004; Schulenberg & Melton, 2010). Previous work on validity for the PIL showed viable one- and two-factor models, albeit question loadings varied across publications (see Schulenberg & Melton, 2010 for a summary), and these fluctuating results lead to the development of a 4-item PIL short form (Schulenberg, Schnetzer, & Buchanan, 2012). 

### Life Purpose Questionnaire
The LPQ was modeled after the full 20-item PIL questionnaire, also measuring perceived meaning and purpose in life. The items are structured in a true/false response format, in contrast to the Likert response format found on the PIL. Each question is matched to the PIL with the same item content, altering the question to create binary answer format. A total score is created by summing questions, resulting in a range from 0 to 20. In both scales, higher scores indicated greater perceived meaning in life. Reliability for this scale is also correspondingly high, usually in the .80 range (Melton & Schulenberg, 2008; Schulenberg, 2004).

These two scales were selected because they contained the same item content with differing response formats, which would allow for cross comparisons between results for each scale. 

## Procedure
The form of administration was of interest to this study, and therefore, multiple formats were included: paper administration in a nonrandom question order, computerized administration in nonrandom order, and computerized administration with a randomized question order. Surveys completed on paper were administered in person in a classroom-type setting, and the questions were printed in a fixed order, consistent across all forms. This setting included a proctor to oversee the experiment and debrief the participants upon completion. Alternatively, computerized questionnaires were available for participants to access electronically, and they were allowed to complete the experiment without directly interacting with a proctor. To ensure participants were properly informed, both an introduction and a debriefing were included within the online form. One section of the computerized questionnaires followed the original scale question order that was never changed, consistent with the paper forms. A different group of participants were given each question in a randomized order. Once collected, the results were then amalgamated into a database for statistical analysis.

# Results

## Hypotheses and Data-Analytic Plan
Each hypothesis was tested using four dependent measures, which are described first, followed by specific hypothesis for group comparisons. First, we examined the variance-covariance matrix for each type of delivery and compared to each other by using root mean squared error (RMSE; CITE). RMSE estimates the difference between covariance matrices and is often used in structural equation modeling to determine if models have good fit to the data. A criterion of < .06 for good fit, .06-.08 for acceptable fit, and > .10 for bad fit was used (Hu & Bentler, 1999?). This analysis was used to determine if the change in delivery changed the structure of the item relationships to each other (i.e. if the correlation matrices are different). RMSE values were calculated using the monomvn package in R (Gramacy, CITE).

We then calculated an exploratory factor analysis on both scales using one-factor models to examine the loading of each item on its latent trait. The PIL factor structure is contested (Schulenberg Strack paper) with many suggestions as to latent structure for one- and two-factor models. The LPQ has seen less research on factor structure (something here about that?). This paper focused on loadings on one global latent trait to determine if the manipulation of delivery impacted factor loadings. We used a one-factor model and included all questions to focus on the loadings, rather than the factor structure. The analysis was performed using the *psych* package in *R* with maximum likelihood estimation and an oblique (oblimin) rotation. The loadings were then compared using a matched dependent *t*-test (i.e. item 1 to item 1, item 2 to item 2) to examine differences. 

Next, item averages were calculated across all participants for each item. These 20 items were then compared in a matched dependent *t*-test to determine if delivery changed the mean of the item. While covariance structure elucidates the varying relations between items, we may still find that item averages are pushed one direction or another by a change in delivery and still maintain the same correlation between items. If this test was significant, we examined the individual items across participants for large effect sizes, as large sample sizes would create significant t-test follow ups. 

Last, the total scores for each participant were compared across delivery type using an independent t-test. Item analyses allow a focus on specific items that may show changes, while total scores allow us to investigate if changes in delivery alter the overall score that is used in other analyses. In both the item and total score analyses, d values and Bayes Factors are provided to examine the size of effects for interpretation, instead of p-values that are biased by sample size. 

### Hypothesis 1. 

Paper forms were compared computerized non-random forms to examine the method of delivery on the relationships between items, item means, and total scores. We expected to find XX consistent with previous research by XX. 

### Hypothesis 2. 

Computer forms were then analyzed by randomized and nonrandomized groups to examine the impact of randomization on covariance structure, item means, and total scores. We expected to find that these forms would vary across covariance structure and item means, which would indicate differences in reactivity to questions (i.e. item 4 always has item 3 as a precursor on a nonrandom form, while item 4 may have a different set of answers when prefaced with other questions). We examined total scores; however, it was unclear if these values would change. A difference in item means may result in changes in total scores, but may also result in no change if some item means decrease, while others increase. 

## Data Screening 
	Each dataset was analyzed separately by splitting on both scale, delivery, and randomization (i.e. PIL-Paper, PIL-Computer-Random, PIL-Computer-Nonrandom, etc.). First, all data was screened for accuracy and missing data. Participants with more than 5% missing data (i.e. 2 or more items) were excluded.  For the PIL, we excluded 132XX (1NP, 83NCR, 48NCN) and for the LPQ, we excluded 843 (138P, 555CR, 150CN). Data was imputed using the mice package in R for participants with less than 5% of missing data. This procedure imputed 3,569XX (1,327NP, 1,378NCR, 864NCN) participants on the PIL and 65 (15P, 15CR, 35CN) participants on the LPQ. We examined each dataset for multivariate outliers using Mahalanobis distance, and 565XX (73NP, 455NCR, 37NCN) PIL and 116 (78P, 23CR, 15CN) LPQ participants were excluded for excessively high scores (i.e. p < .001, Tabachick & Fidell, 2012). Each dataset was then screened for multivariate assumptions of additivity, linearity, normality, homogeneity, and homoscedasticity. 

```{r PIL-Data-Screening, include = FALSE}
####import the files####
PIL = read.csv("PIL.csv")
week1.file.04.29.10 = read.delim("week1 file 04.29.10.csv")
meaning.working.file.5.10.10 = read.delim("meaning working file 5-10-10.csv")
SES.Mike.and.Mike.Data = read.delim("SES Mike and Mike Data.csv")
logotherapy_data_factor_analysis_study = read.delim("logotherapy_data_factor_analysis_study.csv")

##reverse coding the PIL, others have been reversed
PIL[ , c("PIL2", "PIL5", "PIL7", "PIL10", "PIL14", "PIL15", "PIL17", "PIL18", "PIL19")] = 8 - PIL[ , c("PIL2", "PIL5","PIL7", "PIL10", "PIL14", "PIL15", "PIL17", "PIL18", "PIL19")]

##subsetting the PIL dataset that has all three types 
##Zero = notrandom, One = random, Two = paper
nr = subset(PIL, Source == "not random")
r = subset(PIL, Source == "random")
p = subset(PIL, Source == "paper")

##removing all columns except PIL questions
notrandomcomputer = nr[, c(3:22)]
randomcomputer = r[, c(3:22)]
paper = p[, c(3:22)]

##removing all columns except PIL for other data sets
Paper1dropcolumns = week1.file.04.29.10[, c(8:27)]
Paper2dropcolumns = meaning.working.file.5.10.10[, c(55:74)]
Paper3dropcolumns = SES.Mike.and.Mike.Data[, c(2:21)]
Paper4dropcolumns = logotherapy_data_factor_analysis_study[, c(1:20)]

###combinging all paper data sets
allpaper = rbind(Paper1dropcolumns, Paper2dropcolumns, Paper3dropcolumns, Paper4dropcolumns, paper)

##data screening
##each dataset separately (and separate not random/random/paper)
summary(randomcomputer)
summary(notrandomcomputer)
summary(allpaper)

apply(randomcomputer, 2, table)
apply(notrandomcomputer, 2, table)
apply(allpaper, 2, table)

##All paper - PIL2 has a decimal,PIL7 has a decimal,PIL10 has a decimal, PIL13 has a decimal,PIL14, PIL15, PIL16, PIL17
##rounding the old decimals from stefan's mean replacement
allpaper$PIL2 = as.integer(allpaper$PIL2)
allpaper$PIL5 = as.integer(allpaper$PIL5)
allpaper$PIL7 = as.integer(allpaper$PIL7)
allpaper$PIL10 = as.integer(allpaper$PIL10)
allpaper$PIL13 = as.integer(allpaper$PIL13)
allpaper$PIL14 = as.integer(allpaper$PIL14)
allpaper$PIL15 = as.integer(allpaper$PIL15)
allpaper$PIL16 = as.integer(allpaper$PIL16)
allpaper$PIL17 = as.integer(allpaper$PIL17)
allpaper$PIL18 = as.integer(allpaper$PIL18)
allpaper$PIL19 = as.integer(allpaper$PIL19)

######Paper data screening#####
##missing data
percentmiss = function(x){ sum(is.na(x)/length(x)) * 100}

##participants (row)
missing = apply(allpaper, 1, percentmiss)
table(missing)
replacepeople = subset(allpaper, missing <= 5)
summary(replacepeople)

##variables (columns)
apply(replacepeople, 2, percentmiss)

##replace data
tempnomiss = mice(replacepeople)
replacedallpaper = complete(tempnomiss, 1)
summary(replacedallpaper)

##outliers
mahal = mahalanobis(replacedallpaper[ , ],
                    colMeans(replacedallpaper[ , ], na.rm = T),
                    cov(replacedallpaper[ , ], use = "pairwise.complete.obs"))

cutoff = qchisq(1-.001, ncol(replacedallpaper[ , ]))
ncol(replacedallpaper[ , ])
cutoff
summary(mahal < cutoff)
nooutpaper = subset(replacedallpaper, mahal < cutoff)

##assumptions
##additivity
correl = cor(nooutpaper[ , ])
symnum(correl)

finalpaperP = nooutpaper

##set up for assumptions
random = rchisq(nrow(finalpaperP), 7)
fake = lm(random ~ ., data = finalpaperP)
standardized = rstudent(fake)
fitted = scale(fake$fitted.values)

##normality
skewness(finalpaperP[ , ], na.rm = T)
kurtosis(finalpaperP[ , ], na.rm = T)
hist(standardized)

##linearity
qqnorm(standardized)
abline(0,1)

##homog + s
plot(fitted, standardized)
abline(0,0)
abline(v = 0)

######Random data screening#####
##missing data
##participants (row)
missingrandomcomputer = apply(randomcomputer, 1, percentmiss)
table(missingrandomcomputer)
replacepeoplerandom = subset(randomcomputer, missingrandomcomputer <= 5)
summary(replacepeoplerandom)

##variables (column)
apply(replacepeoplerandom, 2, percentmiss)
replacecolumnrandom = replacepeoplerandom[ , -15]
dontcolumnrrandom = replacepeoplerandom[ , 15]

##replace data
tempnomissrandom = mice(replacecolumnrandom)
replacedrandom_temp = complete(tempnomissrandom, 1)

replacedrandom = cbind(replacedrandom_temp[ , 1:14], 
                       PIL15 = dontcolumnrrandom, 
                       replacedrandom_temp[ , 15:19])
summary(replacedrandom)

##outliers
mahalrandom = mahalanobis(replacedrandom[ , ],
                    colMeans(replacedrandom[ , ], na.rm = T),
                    cov(replacedrandom[ , ], use = "pairwise.complete.obs"))

cutoff = qchisq(1-.001, ncol(replacedrandom[ , ]))
ncol(replacedrandom[ , ])
cutoff
summary(mahalrandom < cutoff)
nooutrandom = subset(replacedrandom, mahalrandom < cutoff)

##assumptions
##additivity
correlrandom = cor(nooutrandom[ , ])
symnum(correlrandom)

finalrandomP = nooutrandom

##set up for assumptions
randomrandom = rchisq(nrow(finalrandomP), 7)
fakerandom = lm(randomrandom ~ ., data = finalrandomP)
standardizedrandom = rstudent(fakerandom)
fittedrandom = scale(fakerandom$fitted.values)

##normality
skewness(finalrandomP[ , ], na.rm = T)
kurtosis(finalrandomP[ , ], na.rm = T)
hist(standardizedrandom)

##linearity
qqnorm(standardizedrandom)
abline(0,1)

##homog + s
plot(fittedrandom, standardizedrandom)
abline(0,0)
abline(v = 0)

######NOTRandom data screening#####
##missing data
##participants (row)
missingnotrandomcomputer = apply(notrandomcomputer, 1, percentmiss)
table(missingnotrandomcomputer)
replacepeoplenotrandom = subset(notrandomcomputer, missingnotrandomcomputer <= 5)
summary(replacepeoplenotrandom)

##variables (columns)
apply(replacepeoplenotrandom, 2, percentmiss)

##replace the data
tempnomissnotrandom = mice(replacepeoplenotrandom)
replacednotrandom = complete(tempnomissnotrandom, 1)
summary(replacednotrandom)

##outliers
mahalnotrandom = mahalanobis(replacednotrandom[ , ],
                          colMeans(replacednotrandom[ , ], na.rm = T),
                          cov(replacednotrandom[ , ], use = "pairwise.complete.obs"))

cutoff = qchisq(1-.001, ncol(replacednotrandom[ , ]))
ncol(replacednotrandom[ , ])
cutoff
summary(mahalnotrandom < cutoff)
nooutnotrandom = subset(replacednotrandom, mahalnotrandom < cutoff)

##assumptions
##additivity
correlnotrandom = cor(nooutnotrandom[ , ])
symnum(correlnotrandom)

finalnotrandomP = nooutnotrandom

##set up for assumptions
randomnotrandom = rchisq(nrow(finalnotrandomP), 7)
fakenotrandom = lm(randomnotrandom ~ ., data = finalnotrandomP)
standardizednotrandom = rstudent(fakenotrandom)
fittednotrandom = scale(fakenotrandom$fitted.values)

##normality
skewness(finalnotrandomP[ , ], na.rm = T)
kurtosis(finalnotrandomP[ , ], na.rm = T)
hist(standardizednotrandom)

##linearity
qqnorm(standardizednotrandom)
abline(0,1)

##homog + s
plot(fittednotrandom, standardizednotrandom)
abline(0,0)
abline(v = 0)
```

```{r LPQ-Data-Screening, include=FALSE}
##import the data files
LPQallcompiled = read.csv("LPQ.csv")
firstlpqfile = read.csv("firstlpq.csv")
secondlpqfile = read.csv("secondlpq.csv")
thirdlpqfile = read.csv("thirdlpq.csv")
fourthlpqfile = read.csv("fourthlpq.csv")

###changing to 0 and 1 - columns are currently 1 true, 2 false, so subtract 2
names(LPQallcompiled)
summary(LPQallcompiled)
##fix the 12 typo
LPQallcompiled$lpq16_1[ LPQallcompiled$lpq16_1 == 12] = NA

allcolumns = c("lpq1_1", "lpq2_1", "lpq3_1", "lpq4_1", "lpq5_1", "lpq6_1", "lpq7_1", "lpq8_1", "lpq9_1", "lpq10_1", "lpq11_1", "lpq12_1", "lpq13_1", "lpq14_1", "lpq15_1", "lpq16_1", "lpq17_1", "lpq18_1", "lpq19_1", "lpq20_1")
LPQallcompiled[ , allcolumns] = 2 - LPQallcompiled[ , allcolumns]

###reverse code items 1, 2, 5, 8, 9, 11, 12, 14, 15, 16, 19
reverse = c("lpq1_1", "lpq2_1", "lpq5_1", "lpq8_1", "lpq9_1", "lpq11_1", "lpq12_1", "lpq14_1", "lpq15_1", "lpq16_1", "lpq19_1")
LPQallcompiled[ , reverse] = 1 - LPQallcompiled[ , reverse]
summary(LPQallcompiled)

##make sure this dataaset doesn't have any decimals or weird numbers
apply(LPQallcompiled[ , allcolumns], 2, table)

##the csv files are a mix of computer and paper, so need to subset
##subset the csv that we have, shouldn't have to do this for others
LPQallcompiledrandom = subset(LPQallcompiled, Source == 'random')
LPQallcompilednotrandom = subset(LPQallcompiled, Source == 'not random')
LPQallcompiledpaper = subset(LPQallcompiled, Source == 'paper')
LPQallcompiledpaper = LPQallcompiledpaper[, 3:22] ##dropping the extra columns

##subset firstlpqfile so just lpq paper
names(firstlpqfile)
firstlpqfilepaper = firstlpqfile[ , c(200:219)]
apply(firstlpqfilepaper,2,table)
firstlpqfilepaper[ firstlpqfilepaper > 0 & firstlpqfilepaper < 1 ] = NA

##subset secondlpqfile so just lpq paper
names(secondlpqfile)
secondlpqfilepaper = secondlpqfile[ , c(66:85)]
apply(secondlpqfilepaper,2,table)
secondlpqfilepaper[ secondlpqfilepaper > 0 & secondlpqfilepaper < 1 ] = NA

##subset thirdlpqfile so just lpq paper
names(thirdlpqfile)
thirdlpqfilepaper = thirdlpqfile[ , c(135:154)]
apply(thirdlpqfilepaper,2,table)
thirdlpqfilepaper[ thirdlpqfilepaper > 0 & thirdlpqfilepaper < 1 ] = NA
thirdlpqfilepaper[ thirdlpqfilepaper > 1] = NA

##subset fourthlpqfile 
names(fourthlpqfile)
fourthlpqfilepaper = fourthlpqfile[ , c(78:97)]
apply(fourthlpqfilepaper,2,table)
fourthlpqfilepaper[ fourthlpqfilepaper > 0 & fourthlpqfilepaper < 1 ] = NA
##set all decimals = to NA. see code above under each dataset
  
##combine datasets into using rbind
##LPQallcompiledrandom is the dataset for random (just our compiled data)
##LPQallcompilednotrandom is the dataset for not random (just our compiled data)
allpaper = rbind(LPQallcompiledpaper, firstlpqfilepaper, secondlpqfilepaper, thirdlpqfilepaper, fourthlpqfilepaper)

####Paper data screening####
summary(allpaper)

##missing all paper by row
missingallpaper = apply(allpaper, 1, percentmiss)
table(missingallpaper)
replacepeopleallpaper = subset(allpaper, missingallpaper <= 5)

##missing by column
apply(replacepeopleallpaper, 2, percentmiss)

##replace missing
tempnomiss = mice(replacepeopleallpaper)
replacedallpaper = complete(tempnomiss, 1)
summary(replacedallpaper)

##outliers
mahal = mahalanobis(replacedallpaper, 
                    colMeans(replacedallpaper, na.rm = T), 
                    cov(replacedallpaper, use = "pairwise.complete.obs"))
cutoff = qchisq(1-.001, ncol(replacedallpaper))
cutoff
ncol(replacedallpaper)
summary(mahal < cutoff)
nooutreplacedallpaper = subset(replacedallpaper, mahal < cutoff)

##assumptions 
##additivity
correlreplacedallpaper = cor(nooutreplacedallpaper)
symnum(correlreplacedallpaper)

##set up for assumptions 
random = rchisq(nrow(nooutreplacedallpaper), 7)
fake = lm(random ~ ., data = nooutreplacedallpaper)
standardized = rstudent(fake)
fitted = scale(fake$fitted.values)

##normality 
hist(standardized) ##positive skew

##linearity 
qqnorm(standardized)
abline(0, 1) ##guess its okay 

##homog and s
plot(fitted, standardized)
abline(0, 0)
abline(v = 0) ##graph is a bit iffy

finalpaperL = nooutreplacedallpaper

####Random data screening####
LPQallcompiledrandom = LPQallcompiledrandom[ , 3:22]
summary(LPQallcompiledrandom)

##missing lpqallcompiled random 
missingallcompiledrandom = apply(LPQallcompiledrandom, 1, percentmiss)
table(missingallcompiledrandom)
replacepeopleallcompiledrandom = subset(LPQallcompiledrandom, missingallcompiledrandom <=5)

##check for columns
apply(replacepeopleallcompiledrandom, 2, percentmiss)

##replace data
tempnomiss = mice(replacepeopleallcompiledrandom)
replacedallcompiledrandom = complete(tempnomiss, 1)
summary(replacedallcompiledrandom)

##outliers
mahal = mahalanobis(replacedallcompiledrandom, 
                    colMeans(replacedallcompiledrandom, na.rm = T), 
                    cov(replacedallcompiledrandom, use = "pairwise.complete.obs"))
cutoff = qchisq(1-.001, ncol(replacedallcompiledrandom))
cutoff
ncol(replacedallcompiledrandom)
summary(mahal < cutoff)
nooutreplacedallcompiledrandom = subset(replacedallcompiledrandom, mahal < cutoff)

##assumptions 
##additivity
correlreplacedallcompiledrandom = cor(nooutreplacedallcompiledrandom)
symnum(correlreplacedallcompiledrandom) ##good

##set up for assumptions 
random = rchisq(nrow(nooutreplacedallcompiledrandom), 7)
fake = lm(random ~ ., data = nooutreplacedallcompiledrandom)
standardized = rstudent(fake)
fitted = scale(fake$fitted.values)

##normality 
hist(standardized) ##positive skew

##linearity 
qqnorm(standardized)
abline(0, 1) ##looks good

##homog and s
plot(fitted, standardized)
abline(0, 0)
abline(v = 0)##yes!

finalrandomL = nooutreplacedallcompiledrandom

####Not random data screening####
LPQallcompilednotrandom = LPQallcompilednotrandom[ , 3:22]
summary(LPQallcompilednotrandom)

##missing lpqallcompilednotrandom
missingallcompilednotrandom = apply(LPQallcompilednotrandom, 1, percentmiss)
table(missingallcompilednotrandom)
replacepeopleallcompilednotrandom = subset(LPQallcompilednotrandom, missingallcompilednotrandom <= 5)

##columns
apply(replacepeopleallcompilednotrandom,2,percentmiss)

##replace data
tempnomiss = mice(replacepeopleallcompilednotrandom)
replacedallcompilednotrandom = complete(tempnomiss, 1)
summary(replacedallcompilednotrandom)

##outliers
mahal = mahalanobis(replacedallcompilednotrandom, 
                    colMeans(replacedallcompilednotrandom, na.rm = T), 
                    cov(replacedallcompilednotrandom, use = "pairwise.complete.obs"))
cutoff = qchisq(1-.001, ncol(replacedallcompilednotrandom))
cutoff
ncol(replacedallcompilednotrandom)
summary(mahal < cutoff)
nooutreplacedallcompilednotrandom = subset(replacedallcompilednotrandom, mahal < cutoff)

##assumptions 
##additivity
correlreplacedallcompilednotrandom = cor(nooutreplacedallcompilednotrandom)
symnum(correlreplacedallcompilednotrandom) ##yay

##set up for assumptions 
random = rchisq(nrow(nooutreplacedallcompilednotrandom), 7)
fake = lm(random ~ ., data = nooutreplacedallcompilednotrandom)
standardized = rstudent(fake)
fitted = scale(fake$fitted.values)

##normality 
hist(standardized) ##positive skew still

##linearity 
qqnorm(standardized)
abline(0, 1) ##guess its okay 

##homog and s
plot(fitted, standardized)
abline(0, 0)
abline(v = 0)##meets both

finalnotrandomL = nooutreplacedallcompilednotrandom

```

```{r demo-table, results='asis', include=FALSE}

```

## PIL Analyses

### Covariance Matrices.

```{r Pcov-analysis, include=FALSE}

####covariance testing####
###make covariance tables
not_corP = cov(finalnotrandomP, use="pairwise")
rand_corP = cov(finalrandomP, use="pairwise")
paper_corP = cov(finalpaperP, use="pairwise")

#mean tables
notrandommP = unlist(sapply(finalnotrandomP, function(cl) list(means=mean(cl,na.rm=TRUE))))
randommP = unlist(sapply(finalrandomP, function(cl) list(means=mean(cl,na.rm=TRUE))))
papermP = unlist(sapply(finalpaperP, function(cl) list(means=mean(cl,na.rm=TRUE))))

##rsme mean, cov, mean, cov
rmse.muS(notrandommP, not_corP, papermP, paper_corP)
rmse.muS(notrandommP, not_corP, randommP, rand_corP)
##figure out how to do standardized residuals

##formula is cov - cov / sqrt ( var one - var other )
#do var 1 - var 1.1, then var 2 - var 2.2, then take the variance of that vector
not_vP = apply(finalnotrandomP, 2, var)
rand_vP = apply(finalrandomP, 2, var)
paper_vP = apply(finalpaperP, 2, var)

##figure out how to report these since you can't just divide by 2 - use csvs to figure out which and how many are over. 
##sum up the standardized residuals that are over 1.96
hyp1stdP = (not_corP - paper_corP) / sqrt(var(not_vP - paper_vP))
hyp1overP = sum(as.numeric(abs(hyp1stdP)) > 1.96)

hyp2stdP = (not_corP - rand_corP) / sqrt(var(not_vP - rand_vP))
hyp2overP = sum(as.numeric(abs(hyp2stdP)) > 1.96)
##print out the standardized residuals to put online
write.csv(hyp1stdP, "stdres_not_paper.csv")
write.csv(hyp2stdP, "stdres_not_random.csv")

```


Covariance tables were examined to determine model fit for the different methods of deliverance. The covariance tables from paper forms and those questions delivered not randomly by a computer indicated poor model fit, RMSE = .39. Likewise, covariance tables from questions delivered not randomly on a computer fit poorly with those delivered randomly on a computer, RMSE = .55. 

### Factor Loadings.

```{r Pfactor-load, include = FALSE}
FLpaperP = fa(finalpaperP, nfactors = 1, rotate = "oblimin", fm = "ml")
FLnotP= fa(finalnotrandomP, nfactors = 1, rotate = "oblimin", fm = "ml")
FLrandomP = fa(finalrandomP, nfactors = 1, rotate = "oblimin", fm = "ml") 

##t-tests
hyp1FLPt = t.test(FLpaperP$loadings[1:20], FLnotP$loadings[1:20],
                 paired = T)
hyp2FLPt = t.test(FLnotP$loadings[1:20], FLrandomP$loadings[1:20], 
                 paired = T)

##effect sizes
hyp1FLPd = d.dep.t.avg(mean(FLpaperP$loadings[1:20]), mean(FLnotP$loadings[1:20]),
            sd(FLpaperP$loadings[1:20]), sd(FLnotP$loadings[1:20]),
            n = length(FLpaperP$loadings[1:20]), a = .05)
hyp2FLPd = d.dep.t.avg(mean(FLrandomP$loadings[1:20]),mean(FLnotP$loadings[1:20]),
            sd(FLrandomP$loadings[1:20]), sd(FLnotP$loadings[1:20]),
            n = length(FLrandomP$loadings[1:20]), a = .05)

hyp1FLPdz = d.dep.t.diff(mean(FLpaperP$loadings[1:20] - FLnotP$loadings[1:20]),
                         sd(FLpaperP$loadings[1:20] - FLnotP$loadings[1:20]), 
                         length(FLpaperP$loadings[1:20]), 
                         a = .05)
hyp2FLPdz = d.dep.t.diff(mean(FLrandomP$loadings[1:20] - FLnotP$loadings[1:20]),
                         sd(FLrandomP$loadings[1:20] - FLnotP$loadings[1:20]), 
                         length(FLrandomP$loadings[1:20]), 
                         a = .05)
##toster
hyp1FLPtost = TOSTpaired.raw(n = length(FLpaperP$loadings[1:20]),
                         m1 = mean(FLpaperP$loadings[1:20]),
                         m2 = mean(FLnotP$loadings[1:20]),
                         sd1 = sd(FLpaperP$loadings[1:20]),
                         sd2 = sd(FLnotP$loadings[1:20]),
                         r12 = cor(FLpaperP$loadings[1:20], FLnotP$loadings[1:20]),
                         low_eqbound = -0.10,
                         high_eqbound = 0.10,
                         alpha = .05)

hyp2FLPtost = TOSTpaired.raw(n = length(FLrandomP$loadings[1:20]),
                         m1 = mean(FLrandomP$loadings[1:20]),
                         m2 = mean(FLnotP$loadings[1:20]),
                         sd1 = sd(FLrandomP$loadings[1:20]),
                         sd2 = sd(FLnotP$loadings[1:20]),
                         r12 = cor(FLrandomP$loadings[1:20], FLnotP$loadings[1:20]),
                         low_eqbound = -0.10,
                         high_eqbound = 0.10,
                         alpha = .05)

##bayes factor
ttestBF(x = FLpaperP$loadings[1:20], y = FLnotP$loadings[1:20], paired = TRUE)
ttestBF(x = FLrandomP$loadings[1:20], y = FLnotP$loadings[1:20], paired = TRUE)
```

### Item Means.

Dependent t-tests were conducted on the item mean scores for each hypothesis. Questions delivered on paper were significantly different from those delivered not randomly on a computer, t(19) = -7.27, p < .001, davg = -1.62-1.62. These results indicated that paper scores (M = 5.45, SD = 0.46) have higher item averages than non-random computer scores (M = 5.15, SD = 0.49). However, questions delivered randomly on a computer were not different from those delivered not randomly on a computer, t(19) = 1.36, p = .184, davg = 0.3130, indicating that computer questionnaires that presented the questions in a random format (M = 5.00, SD = 0.73) were similar to those presented in a non-random format (M = 5.15, SD = 0.49) on the computer. d effect sizes were calculated for each item between the paper delivery method and non-random computer delivery method to determine which items displayed the largest changes. Results are presented in Table XX.

### Total Scores. 

Independent t-tests conducted on total scores revealed that questions delivered on paper were significantly different from those delivered not randomly on a computer, t(2079) = 8.31, p < .001, d = 0.37. On average, paper total scores (M = 108.96, SD = 14.14) are higher than the computer not random scores (M = 103.04, SD = 18.24). Questions delivered not randomly on a computer also were significantly higher from those delivered not randomly on a computer, M = 100.10, SD = 15.42, t(1747) = 3.66, p < .001, d = 0.17. However, very small effect sizes were observed for both analyses.
  
```{r PIL Analyses}
####t test by item###

##i think this should be paired T because the items are paired
library(effsize)
t.test(notrandomm, 
       randomm,
       var.equal = T,
       paired = T)

cohen.d(notrandomm, 
        randomm,
        pooled = T,
        paired = T)

mean(notrandomm)
mean(randomm)
sd(notrandomm)
sd(randomm)
options(scipen = 999)
t.test(notrandomm, 
       paperm,
       var.equal = T,
       paired = T)

cohen.d(notrandomm, 
       paperm,
       pooled = T,
       paired = T)

mean(notrandomm)
mean(paperm)
sd(notrandomm)
sd(paperm)

####effect sizes####
##run effsize.R first to get this to work (put in folder)
##not random and random
d.deptavg(m1 = 5.15, sd1 = 0.48,
          m2 = 5.00, sd2 = 0.73, n = 20,
          a = .05, k = 2)

##notrandom and paper
d.deptavg(m1 = 5.15, sd1 = 0.48,
          m2 = 5.44, sd2 = 0.46, n = 20,
          a = .05, k = 2)

##if significant and effect size CI does not cross zero 
##run individual t-tests by item 

cohen.d(finalnotrandomP$PIL1, 
       finalpaperP$PIL1,
       pooled = T,
       paired = F)

cohen.d(finalnotrandomP$PIL2, 
        finalpaperP$PIL2,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL3, 
        finalpaperP$PIL3,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL4, 
        finalpaperP$PIL4,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL5, 
        finalpaperP$PIL5,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL6, 
        finalpaperP$PIL6,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL7, 
        finalpaperP$PIL7,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL8, 
        finalpaperP$PIL8,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL9, 
        finalpaperP$PIL9,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL10, 
        finalpaperP$PIL10,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL11, 
        finalpaperP$PIL11,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL12, 
        finalpaperP$PIL12,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL13, 
        finalpaperP$PIL13,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL14, 
        finalpaperP$PIL14,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL15, 
        finalpaperP$PIL15,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL16, 
        finalpaperP$PIL16,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL17, 
        finalpaperP$PIL17,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL18, 
        finalpaperP$PIL18,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL19, 
        finalpaperP$PIL19,
        pooled = T,
        paired = F)

cohen.d(finalnotrandomP$PIL20, 
        finalpaperP$PIL20,
        pooled = T,
        paired = F)


####total scores####
###No total scores - need to create these
finalpaperP$total = apply(finalpaperP, 1, sum)
finalnotrandomP$total = apply(finalnotrandomP, 1, sum)
finalrandomP$total = apply(finalrandomP, 1, sum)

t.test(finalpaperP$total, finalnotrandomP$total,
       var.equal = T,
       paired = F)

cohen.d(finalpaperP$total, finalnotrandomP$total,
       pooled = T,
       paired = F)

t.test(finalnotrandomP$total, finalrandomP$total,
       var.equal = T,
       paired = F)

cohen.d(finalnotrandomP$total, finalrandomP$total,
        pooled = T,
        paired = F)

mean(finalnotrandomP$total)
mean(finalrandomP$total)
mean(finalpaperP$total)
sd(finalnotrandomP$total)
sd(finalrandomP$total)
sd(finalpaperP$total)

####effect sizes####
###not random vs. random##

d.indt(m1 = 103.03, sd1 = 18.29, n1 = 20,
       m2 = 100.06, sd2 = 15.43, n2 =20,
       a = .05, k = 2)

###not random vs. paper##
d.indt(m1 = 103.03, sd1 = 18.29, n1 = 20,
       m2 = 108.96, sd2 = 14.14, n2 =20,
       a = .05, k = 2)

##Not random means by item##
mean(finalnotrandomP$PIL1)
mean(finalnotrandomP$PIL2)
mean(finalnotrandomP$PIL3)
mean(finalnotrandomP$PIL4)
mean(finalnotrandomP$PIL5)
mean(finalnotrandomP$PIL6)
mean(finalnotrandomP$PIL7)
mean(finalnotrandomP$PIL8)
mean(finalnotrandomP$PIL9)
mean(finalnotrandomP$PIL10)
mean(finalnotrandomP$PIL11)
mean(finalnotrandomP$PIL12)
mean(finalnotrandomP$PIL13)
mean(finalnotrandomP$PIL14)
mean(finalnotrandomP$PIL15)
mean(finalnotrandomP$PIL16)
mean(finalnotrandomP$PIL17)
mean(finalnotrandomP$PIL18)
mean(finalnotrandomP$PIL19)
mean(finalnotrandomP$PIL20)


##Not random sd's by item##
sd(finalnotrandomP$PIL1)
sd(finalnotrandomP$PIL2)
sd(finalnotrandomP$PIL3)
sd(finalnotrandomP$PIL4)
sd(finalnotrandomP$PIL5)
sd(finalnotrandomP$PIL6)
sd(finalnotrandomP$PIL7)
sd(finalnotrandomP$PIL8)
sd(finalnotrandomP$PIL9)
sd(finalnotrandomP$PIL10)
sd(finalnotrandomP$PIL11)
sd(finalnotrandomP$PIL12)
sd(finalnotrandomP$PIL13)
sd(finalnotrandomP$PIL14)
sd(finalnotrandomP$PIL15)
sd(finalnotrandomP$PIL16)
sd(finalnotrandomP$PIL17)
sd(finalnotrandomP$PIL18)
sd(finalnotrandomP$PIL19)
sd(finalnotrandomP$PIL20)

##Random means by item##
mean(finalrandomP$PIL1)
mean(finalrandomP$PIL2)
mean(finalrandomP$PIL3)
mean(finalrandomP$PIL4)
mean(finalrandomP$PIL5)
mean(finalrandomP$PIL6)
mean(finalrandomP$PIL7)
mean(finalrandomP$PIL8)
mean(finalrandomP$PIL9)
mean(finalrandomP$PIL10)
mean(finalrandomP$PIL11)
mean(finalrandomP$PIL12)
mean(finalrandomP$PIL13)
mean(finalrandomP$PIL14)
mean(finalrandomP$PIL15)
mean(finalrandomP$PIL16)
mean(finalrandomP$PIL17)
mean(finalrandomP$PIL18)
mean(finalrandomP$PIL19)
mean(finalrandomP$PIL20)

##Random sd'd by item##
sd(finalrandomP$PIL1)
sd(finalrandomP$PIL2)
sd(finalrandomP$PIL3)
sd(finalrandomP$PIL4)
sd(finalrandomP$PIL5)
sd(finalrandomP$PIL6)
sd(finalrandomP$PIL7)
sd(finalrandomP$PIL8)
sd(finalrandomP$PIL9)
sd(finalrandomP$PIL10)
sd(finalrandomP$PIL11)
sd(finalrandomP$PIL12)
sd(finalrandomP$PIL13)
sd(finalrandomP$PIL14)
sd(finalrandomP$PIL15)
sd(finalrandomP$PIL16)
sd(finalrandomP$PIL17)
sd(finalrandomP$PIL18)
sd(finalrandomP$PIL19)
sd(finalrandomP$PIL20)


##Paper means by item###
mean(finalpaperP$PIL1)
mean(finalpaperP$PIL2)
mean(finalpaperP$PIL3)
mean(finalpaperP$PIL4)
mean(finalpaperP$PIL5)
mean(finalpaperP$PIL6)
mean(finalpaperP$PIL7)
mean(finalpaperP$PIL8)
mean(finalpaperP$PIL9)
mean(finalpaperP$PIL10)
mean(finalpaperP$PIL11)
mean(finalpaperP$PIL12)
mean(finalpaperP$PIL13)
mean(finalpaperP$PIL14)
mean(finalpaperP$PIL15)
mean(finalpaperP$PIL16)
mean(finalpaperP$PIL17)
mean(finalpaperP$PIL18)
mean(finalpaperP$PIL19)
mean(finalpaperP$PIL20)

###Paper sd's by item###
sd(finalpaperP$PIL1)
sd(finalpaperP$PIL2)
sd(finalpaperP$PIL3)
sd(finalpaperP$PIL4)
sd(finalpaperP$PIL5)
sd(finalpaperP$PIL6)
sd(finalpaperP$PIL7)
sd(finalpaperP$PIL8)
sd(finalpaperP$PIL9)
sd(finalpaperP$PIL10)
sd(finalpaperP$PIL11)
sd(finalpaperP$PIL12)
sd(finalpaperP$PIL13)
sd(finalpaperP$PIL14)
sd(finalpaperP$PIL15)
sd(finalpaperP$PIL16)
sd(finalpaperP$PIL17)
sd(finalpaperP$PIL18)
sd(finalpaperP$PIL19)
sd(finalpaperP$PIL20)


mean(finalnotrandomP$total)
mean(finalrandomP$total)
mean(finalpaperP$total)
sd(finalnotrandomP$total)
sd(finalrandomP$total)
sd(finalpaperP$total)

library(BayesFactor)
##Bayes factor
library(effsize)
##run it on totals? 
ttestBF(x = finalnotrandomP$total, y = finalrandomP$total, paired = FALSE)
ttestBF(x = finalnotrandomP$total, y = finalpaperP$total, paired = FALSE)

library(TOSTER)
TOSTtwo(m1 = mean(finalnotrandomP$total), m2 = mean(finalrandomP$total),
        sd1 = sd(finalnotrandomP$total), sd2 = sd(finalrandomP$total),
        n1 = length(na.omit(finalnotrandomP$total)), n2 = length(na.omit(finalrandomP$total)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(finalnotrandomP$total), m2 = mean(finalpaperP$total),
        sd1 = sd(finalnotrandomP$total), sd2 = sd(finalpaperP$total),
        n1 = length(na.omit(finalnotrandomP$total)), n2 = length(na.omit(finalpaperP$total)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

```

## LPQ Analyses

### Covariance Matrices. 

The covariance tables from paper forms and those questions delivered not randomly by a computer indicated excellent model fit, RMSE = .04. Likewise, covariance tables from questions delivered not randomly on a computer fit excellently with those delivered randomly on a computer, RMSE = .02. These RMSE values indicate that the covariance tables were nearly identical. 

### Factor Loadings.

### Item Means. 

Dependent t-tests were conducted on the mean scores showed that questionnaires delivered on paper were significantly different from those delivered not randomly on a computer, t(19) = -10.05, p < .001, davg = 0.62. However, questions delivered randomly on a computer were not different from those delivered not randomly on paper, t(19) = 0.22, p = .83, davg = 0.01. These results indicate that paper scores (M = 0.80, SD = 0.16) have higher item averages than non-random computer scores (M = 0.71, SD = 0.15), but computer questionnaires that presented the questions in a random format (M = 0.71, SD = 0.18) were similar to those presented in a non-random (M = 0.71, SD = 0.15) format on the computer. d effect sizes were again calculated for each item between the paper delivery method and non-random computer delivery method. Results are presented in Table XX. All items indicated either a small or negligible effect size. Specifically, items 1, 5, 7, 15, 18, and 19 indicated a negligible effect size. Items 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 16, 17, and 20 indicated a small effect size. These results suggest that computer questions presented in a random were not different from those presented in a non-random format on the computer. 

### Total Scores. 

Questions delivered on paper were significantly different from those delivered not randomly on a computer, t(1795) = -11.05, p < .001, d = 0.53. On average, paper total scores (M = 16.09, SD = 3.10) are higher than the computer not random scores (M = 14.19, SD = 4.22). Questions delivered randomly on a computer were not different from those delivered not randomly on a computer, t(1631) = 0.19, p = .85, d = 0.01. Questionnaires delivered randomly on the computer (M = 14.15, SD = 4.00) and those not delivered randomly (M = 14.19, SD = 4.22) are statistically similar. 

```{r LPQ Analyses}
####covariance testing####
library(monomvn)
###make covariance tables using above datasets
notrandom_cor = cov(nooutreplacedallcompilednotrandom, use="pairwise")
rand_cor = cov(nooutreplacedallcompiledrandom, use="pairwise")
paper_cor = cov(nooutreplacedallpaper, use="pairwise")

#mean tables
notrandom = unlist(sapply(nooutreplacedallcompilednotrandom, function(cl) list(means=mean(cl,na.rm=TRUE))))
random = unlist(sapply(nooutreplacedallcompiledrandom, function(cl) list(means=mean(cl,na.rm=TRUE))))
paper = unlist(sapply(nooutreplacedallpaper, function(cl) list(means=mean(cl,na.rm=TRUE))))

##rsme mean, cov, mean, cov
rmse.muS(notrandom, notrandom_cor, paper, paper_cor) 
rmse.muS(notrandom, notrandom_cor, random, rand_cor)
##figure out how to do standardized residuals

####t test by dataset need to do items below###

t.test(notrandom, random,
       var.equal = T,
       paired = T)

t.test(notrandom, paper,
       var.equal = T,
       paired = T)

mean(notrandom) 
mean(random)
sd(notrandom)
sd(random)

mean(notrandom)
mean(paper)
sd(notrandom)
sd(paper)
####effect si0zes####
##random is 883 people 
##paper is 1042 
##not random is 750
d.deptavg(m1 = 0.7091333, sd1 = 0.1509649,  ####this is not random v random
       m2 = 0.7070136, sd2 = 0.1759948, n = 20,
       a = .05, k = 2)

d.deptavg(m1 = 0.7091333, sd1 = 0.1509649, ####this is not random v paper
       m2 = 0.8048279, sd2 = 0.1569277, n = 20,
       a = .05, k = 2)

##if significant and effect size CI does not cross zero 
##run individual t-tests by item 
##from my analyses, it looks like the CI does not cross 0 and is significant for both

####total scores####
##nooutreplacedallcompilednotrandom
##nooutreplacedallcompiledrandom
##nooutreplacedallpaper

nooutreplacedallcompilednotrandom$total = apply(nooutreplacedallcompilednotrandom, 1, sum)
nooutreplacedallcompiledrandom$total = apply(nooutreplacedallcompiledrandom, 1, sum)
nooutreplacedallpaper$total = apply(nooutreplacedallpaper, 1, sum)

t.test(nooutreplacedallcompilednotrandom$total, nooutreplacedallcompiledrandom$total,
       var.equal = T,
       paired = F) ##ran this one as paired = F for total scores 
t.test(nooutreplacedallcompilednotrandom$total, nooutreplacedallpaper$total, 
       var.equal = T, 
       paired = F)

mean(nooutreplacedallcompilednotrandom$total)
sd(nooutreplacedallcompilednotrandom$total)
mean(nooutreplacedallcompiledrandom$total)
sd(nooutreplacedallcompiledrandom$total)
mean(nooutreplacedallpaper$total)
sd(nooutreplacedallpaper$total)
####effect sizes####
d.indt(m1 = 14.8918, sd1 = 4.433239, n1 = 750, ##random v not random
       m2 = 14.84729, sd2 = 4.207219, n2 = 883,
       a = .05, k = 2)
d.indt(m1 = 14.8918, sd1 = 4.433239, n1 = 750,
       m2 = 16.90139, sd2 = 3.244741, n2 = 1042,
       a = .05, k = 2)

####t test by item#### dr b can you check this part? wasn't quite sure if I got it right
##nooutreplacedallcompilednotrandom
##nooutreplacedallcompiledrandom
##nooutreplacedallpaper
t.test(nooutreplacedallcompilednotrandom$lpq1_1, nooutreplacedallcompiledrandom$lpq1_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq2_1, nooutreplacedallcompiledrandom$lpq2_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq3_1, nooutreplacedallcompiledrandom$lpq3_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq4_1, nooutreplacedallcompiledrandom$lpq4_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq5_1, nooutreplacedallcompiledrandom$lpq5_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq6_1, nooutreplacedallcompiledrandom$lpq6_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq7_1, nooutreplacedallcompiledrandom$lpq7_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq8_1, nooutreplacedallcompiledrandom$lpq8_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq9_1, nooutreplacedallcompiledrandom$lpq9_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq10_1, nooutreplacedallcompiledrandom$lpq10_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq11_1, nooutreplacedallcompiledrandom$lpq11_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq12_1, nooutreplacedallcompiledrandom$lpq12_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq13_1, nooutreplacedallcompiledrandom$lpq13_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq14_1, nooutreplacedallcompiledrandom$lpq14_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq15_1, nooutreplacedallcompiledrandom$lpq15_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq16_1, nooutreplacedallcompiledrandom$lpq16_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq17_1, nooutreplacedallcompiledrandom$lpq17_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq18_1, nooutreplacedallcompiledrandom$lpq18_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq19_1, nooutreplacedallcompiledrandom$lpq19_1,
       var.equal = T,
       paired = F)
t.test(nooutreplacedallcompilednotrandom$lpq20_1, nooutreplacedallcompiledrandom$lpq20_1,
       var.equal = T,
       paired = F)
####not random vs paper (not sure if we needed these or not)####
library(effsize)

cohen.d(nooutreplacedallcompilednotrandom$lpq1_1, nooutreplacedallpaper$lpq1_1, 
       pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq2_1, nooutreplacedallpaper$lpq2_1, 
      pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq3_1, nooutreplacedallpaper$lpq3_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq4_1, nooutreplacedallpaper$lpq4_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq5_1, nooutreplacedallpaper$lpq5_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq6_1, nooutreplacedallpaper$lpq6_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq7_1, nooutreplacedallpaper$lpq7_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq8_1, nooutreplacedallpaper$lpq8_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq9_1, nooutreplacedallpaper$lpq9_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq10_1, nooutreplacedallpaper$lpq10_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq11_1, nooutreplacedallpaper$lpq11_1, 
       pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq12_1, nooutreplacedallpaper$lpq12_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq13_1, nooutreplacedallpaper$lpq13_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq14_1, nooutreplacedallpaper$lpq14_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq15_1, nooutreplacedallpaper$lpq15_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq16_1, nooutreplacedallpaper$lpq16_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq17_1, nooutreplacedallpaper$lpq17_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq18_1, nooutreplacedallpaper$lpq18_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq19_1, nooutreplacedallpaper$lpq19_1, 
        pooled = T, 
       paired = F)
cohen.d(nooutreplacedallcompilednotrandom$lpq20_1, nooutreplacedallpaper$lpq20_1, 
        pooled = T, 
       paired = F)

##Bayes factor
##nooutreplacedallcompilednotrandom
##nooutreplacedallcompiledrandom
##nooutreplacedallpaper
##run it on totals? 
library(BayesFactor) ##defaults to BF10
ttestBF(x = nooutreplacedallcompilednotrandom$total, y = nooutreplacedallcompiledrandom$total, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$total, y = nooutreplacedallpaper$total, paired = FALSE)
ttestBF(x = nooutreplacedallpaper$total, y = nooutreplacedallcompilednotrandom$total, paired = FALSE)

##specific Bayes Factor stuff (defaults to BF10) paper vs. nonrandom
ttestBF(x = nooutreplacedallcompilednotrandom$lpq1_1, y = nooutreplacedallpaper$lpq1_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq2_1, y = nooutreplacedallpaper$lpq2_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq3_1, y = nooutreplacedallpaper$lpq3_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq4_1, y = nooutreplacedallpaper$lpq4_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq5_1, y = nooutreplacedallpaper$lpq5_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq6_1, y = nooutreplacedallpaper$lpq6_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq7_1, y = nooutreplacedallpaper$lpq7_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq8_1, y = nooutreplacedallpaper$lpq8_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq9_1, y = nooutreplacedallpaper$lpq9_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq10_1, y = nooutreplacedallpaper$lpq10_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq11_1, y = nooutreplacedallpaper$lpq11_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq12_1, y = nooutreplacedallpaper$lpq12_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq13_1, y = nooutreplacedallpaper$lpq13_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq14_1, y = nooutreplacedallpaper$lpq14_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq15_1, y = nooutreplacedallpaper$lpq15_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq16_1, y = nooutreplacedallpaper$lpq16_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq17_1, y = nooutreplacedallpaper$lpq17_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq18_1, y = nooutreplacedallpaper$lpq18_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq19_1, y = nooutreplacedallpaper$lpq19_1, paired = FALSE)
ttestBF(x = nooutreplacedallcompilednotrandom$lpq20_1, y = nooutreplacedallpaper$lpq20_1, paired = FALSE)


##tost
install.packages("TOSTER")
library(TOSTER)

##not random v random
TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$total), m2 = mean(nooutreplacedallpaper$total),
        sd1 = sd(nooutreplacedallcompilednotrandom$total), sd2 = sd(nooutreplacedallcompiledrandom$total),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$total)), n2 = length(na.omit(nooutreplacedallcompiledrandom$total)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

##item TOST 
TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq1_1), m2 = mean(nooutreplacedallpaper$lpq1_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq1_1), sd2 = sd(nooutreplacedallpaper$lpq1_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq1_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq1_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq2_1), m2 = mean(nooutreplacedallcompiledrandom$lpq2_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq2_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq2_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq2_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq2_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq3_1), m2 = mean(nooutreplacedallcompiledrandom$lpq3_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq3_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq3_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq3_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq3_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq4_1), m2 = mean(nooutreplacedallcompiledrandom$lpq4_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq4_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq4_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq4_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq4_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq5_1), m2 = mean(nooutreplacedallcompiledrandom$lpq5_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq5_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq5_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq5_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq5_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq6_1), m2 = mean(nooutreplacedallcompiledrandom$lpq6_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq6_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq6_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq6_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq6_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq7_1), m2 = mean(nooutreplacedallcompiledrandom$lpq7_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq7_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq7_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq7_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq7_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq8_1), m2 = mean(nooutreplacedallcompiledrandom$lpq8_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq8_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq8_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq8_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq8_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq9_1), m2 = mean(nooutreplacedallcompiledrandom$lpq9_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq9_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq9_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq9_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq9_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq10_1), m2 = mean(nooutreplacedallcompiledrandom$lpq10_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq10_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq10_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq10_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq10_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq11_1), m2 = mean(nooutreplacedallcompiledrandom$lpq11_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq11_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq11_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq11_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq11_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq12_1), m2 = mean(nooutreplacedallcompiledrandom$lpq12_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq12_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq12_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq12_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq12_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq13_1), m2 = mean(nooutreplacedallcompiledrandom$lpq13_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq13_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq13_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq13_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq13_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq14_1), m2 = mean(nooutreplacedallcompiledrandom$lpq14_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq14_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq14_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq14_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq14_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq15_1), m2 = mean(nooutreplacedallcompiledrandom$lpq15_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq15_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq15_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq15_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq15_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq16_1), m2 = mean(nooutreplacedallcompiledrandom$lpq16_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq16_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq16_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq16_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq16_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq17_1), m2 = mean(nooutreplacedallcompiledrandom$lpq17_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq17_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq17_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq17_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq17_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq18_1), m2 = mean(nooutreplacedallcompiledrandom$lpq18_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq18_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq18_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq18_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq18_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq19_1), m2 = mean(nooutreplacedallcompiledrandom$lpq19_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq19_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq19_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq19_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq19_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq20_1), m2 = mean(nooutreplacedallcompiledrandom$lpq20_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq20_1), sd2 = sd(nooutreplacedallcompiledrandom$lpq20_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq20_1)), n2 = length(na.omit(nooutreplacedallcompiledrandom$lpq20_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

##paper vs. non random TOST 
TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq1_1), m2 = mean(nooutreplacedallpaper$lpq1_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq1_1), sd2 = sd(nooutreplacedallpaper$lpq1_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq1_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq1_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq2_1), m2 = mean(nooutreplacedallpaper$lpq2_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq2_1), sd2 = sd(nooutreplacedallpaper$lpq2_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq2_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq2_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq3_1), m2 = mean(nooutreplacedallpaper$lpq3_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq3_1), sd2 = sd(nooutreplacedallpaper$lpq3_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq3_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq3_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq4_1), m2 = mean(nooutreplacedallpaper$lpq4_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq4_1), sd2 = sd(nooutreplacedallpaper$lpq4_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq4_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq4_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq5_1), m2 = mean(nooutreplacedallpaper$lpq5_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq5_1), sd2 = sd(nooutreplacedallpaper$lpq5_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq5_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq5_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq6_1), m2 = mean(nooutreplacedallpaper$lpq6_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq6_1), sd2 = sd(nooutreplacedallpaper$lpq6_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq6_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq6_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq7_1), m2 = mean(nooutreplacedallpaper$lpq7_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq7_1), sd2 = sd(nooutreplacedallpaper$lpq7_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq7_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq7_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq8_1), m2 = mean(nooutreplacedallpaper$lpq8_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq8_1), sd2 = sd(nooutreplacedallpaper$lpq8_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq8_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq8_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq9_1), m2 = mean(nooutreplacedallpaper$lpq9_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq9_1), sd2 = sd(nooutreplacedallpaper$lpq9_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq9_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq9_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq10_1), m2 = mean(nooutreplacedallpaper$lpq10_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq10_1), sd2 = sd(nooutreplacedallpaper$lpq10_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq10_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq10_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq11_1), m2 = mean(nooutreplacedallpaper$lpq11_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq11_1), sd2 = sd(nooutreplacedallpaper$lpq11_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq11_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq11_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq12_1), m2 = mean(nooutreplacedallpaper$lpq12_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq12_1), sd2 = sd(nooutreplacedallpaper$lpq12_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq12_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq12_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq13_1), m2 = mean(nooutreplacedallpaper$lpq13_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq13_1), sd2 = sd(nooutreplacedallpaper$lpq13_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq13_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq13_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq14_1), m2 = mean(nooutreplacedallpaper$lpq14_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq14_1), sd2 = sd(nooutreplacedallpaper$lpq14_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq14_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq14_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq15_1), m2 = mean(nooutreplacedallpaper$lpq15_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq15_1), sd2 = sd(nooutreplacedallpaper$lpq15_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq15_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq15_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq16_1), m2 = mean(nooutreplacedallpaper$lpq16_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq16_1), sd2 = sd(nooutreplacedallpaper$lpq16_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq16_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq16_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq17_1), m2 = mean(nooutreplacedallpaper$lpq17_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq17_1), sd2 = sd(nooutreplacedallpaper$lpq17_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq17_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq17_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq18_1), m2 = mean(nooutreplacedallpaper$lpq18_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq18_1), sd2 = sd(nooutreplacedallpaper$lpq18_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq18_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq18_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq19_1), m2 = mean(nooutreplacedallpaper$lpq19_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq19_1), sd2 = sd(nooutreplacedallpaper$lpq19_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq19_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq19_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)

TOSTtwo(m1 = mean(nooutreplacedallcompilednotrandom$lpq20_1), m2 = mean(nooutreplacedallpaper$lpq20_1),
        sd1 = sd(nooutreplacedallcompilednotrandom$lpq20_1), sd2 = sd(nooutreplacedallpaper$lpq20_1),
        n1 = length(na.omit(nooutreplacedallcompilednotrandom$lpq20_1)), n2 = length(na.omit(nooutreplacedallpaper$lpq20_1)),
        low_eqbound_d = -.3, high_eqbound_d = .3,
        var.equal = TRUE, alpha = .05)






##item reactivity notes - look @ draft 
##table of all participant information - pull demographic information for the table *** need to do this
##fill in XX data for # of data that we imputed when we miced - done
##common on assumptions - done 
##add results to the table (item means) - done
##then upload so Dr. B can see the Bayes factor code - done with Bayes/Tost. added to table and commented on in discussion. still need to add to results

##means paper 
mean(nooutreplacedallpaper$lpq1_1)
sd(nooutreplacedallpaper$lpq1_1)
mean(nooutreplacedallpaper$lpq2_1)
sd(nooutreplacedallpaper$lpq2_1)
mean(nooutreplacedallpaper$lpq3_1)
sd(nooutreplacedallpaper$lpq3_1)
mean(nooutreplacedallpaper$lpq4_1)
sd(nooutreplacedallpaper$lpq4_1)
mean(nooutreplacedallpaper$lpq5_1)
sd(nooutreplacedallpaper$lpq5_1)
mean(nooutreplacedallpaper$lpq6_1)
sd(nooutreplacedallpaper$lpq6_1)
mean(nooutreplacedallpaper$lpq7_1)
sd(nooutreplacedallpaper$lpq7_1)
mean(nooutreplacedallpaper$lpq8_1)
sd(nooutreplacedallpaper$lpq8_1)
mean(nooutreplacedallpaper$lpq9_1)
sd(nooutreplacedallpaper$lpq9_1)
mean(nooutreplacedallpaper$lpq10_1)
sd(nooutreplacedallpaper$lpq10_1)
mean(nooutreplacedallpaper$lpq11_1)
sd(nooutreplacedallpaper$lpq11_1)
mean(nooutreplacedallpaper$lpq12_1)
sd(nooutreplacedallpaper$lpq12_1)
mean(nooutreplacedallpaper$lpq13_1)
sd(nooutreplacedallpaper$lpq13_1)
mean(nooutreplacedallpaper$lpq14_1)
sd(nooutreplacedallpaper$lpq14_1)
mean(nooutreplacedallpaper$lpq15_1)
sd(nooutreplacedallpaper$lpq15_1)
mean(nooutreplacedallpaper$lpq16_1)
sd(nooutreplacedallpaper$lpq16_1)
mean(nooutreplacedallpaper$lpq17_1)
sd(nooutreplacedallpaper$lpq17_1)
mean(nooutreplacedallpaper$lpq18_1)
sd(nooutreplacedallpaper$lpq18_1)
mean(nooutreplacedallpaper$lpq19_1)
sd(nooutreplacedallpaper$lpq19_1)
mean(nooutreplacedallpaper$lpq20_1)
sd(nooutreplacedallpaper$lpq20_1)
##means non-random computer 
mean(nooutreplacedallcompilednotrandom$lpq1_1)
sd(nooutreplacedallcompilednotrandom$lpq1_1)
mean(nooutreplacedallcompilednotrandom$lpq2_1)
sd(nooutreplacedallcompilednotrandom$lpq2_1)
mean(nooutreplacedallcompilednotrandom$lpq3_1)
sd(nooutreplacedallcompilednotrandom$lpq3_1)
mean(nooutreplacedallcompilednotrandom$lpq4_1)
sd(nooutreplacedallcompilednotrandom$lpq4_1)
mean(nooutreplacedallcompilednotrandom$lpq5_1)
sd(nooutreplacedallcompilednotrandom$lpq5_1)
mean(nooutreplacedallcompilednotrandom$lpq6_1)
sd(nooutreplacedallcompilednotrandom$lpq6_1)
mean(nooutreplacedallcompilednotrandom$lpq7_1)
sd(nooutreplacedallcompilednotrandom$lpq7_1)
mean(nooutreplacedallcompilednotrandom$lpq8_1)
sd(nooutreplacedallcompilednotrandom$lpq8_1)
mean(nooutreplacedallcompilednotrandom$lpq9_1)
sd(nooutreplacedallcompilednotrandom$lpq9_1)
mean(nooutreplacedallcompilednotrandom$lpq10_1)
sd(nooutreplacedallcompilednotrandom$lpq10_1)
mean(nooutreplacedallcompilednotrandom$lpq11_1)
sd(nooutreplacedallcompilednotrandom$lpq11_1)
mean(nooutreplacedallcompilednotrandom$lpq12_1)
sd(nooutreplacedallcompilednotrandom$lpq12_1)
mean(nooutreplacedallcompilednotrandom$lpq13_1)
sd(nooutreplacedallcompilednotrandom$lpq13_1)
mean(nooutreplacedallcompilednotrandom$lpq14_1)
sd(nooutreplacedallcompilednotrandom$lpq14_1)
mean(nooutreplacedallcompilednotrandom$lpq15_1)
sd(nooutreplacedallcompilednotrandom$lpq15_1)
mean(nooutreplacedallcompilednotrandom$lpq16_1)
sd(nooutreplacedallcompilednotrandom$lpq16_1)
mean(nooutreplacedallcompilednotrandom$lpq17_1)
sd(nooutreplacedallcompilednotrandom$lpq17_1)
mean(nooutreplacedallcompilednotrandom$lpq18_1)
sd(nooutreplacedallcompilednotrandom$lpq18_1)
mean(nooutreplacedallcompilednotrandom$lpq19_1)
sd(nooutreplacedallcompilednotrandom$lpq19_1)
mean(nooutreplacedallcompilednotrandom$lpq20_1)
sd(nooutreplacedallcompilednotrandom$lpq20_1)
##means random computer
mean(nooutreplacedallcompiledrandom$lpq1_1)
sd(nooutreplacedallcompiledrandom$lpq1_1)
mean(nooutreplacedallcompiledrandom$lpq2_1)
sd(nooutreplacedallcompiledrandom$lpq2_1)
mean(nooutreplacedallcompiledrandom$lpq3_1)
sd(nooutreplacedallcompiledrandom$lpq3_1)
mean(nooutreplacedallcompiledrandom$lpq4_1)
sd(nooutreplacedallcompiledrandom$lpq4_1)
mean(nooutreplacedallcompiledrandom$lpq5_1)
sd(nooutreplacedallcompiledrandom$lpq5_1)
mean(nooutreplacedallcompiledrandom$lpq6_1)
sd(nooutreplacedallcompiledrandom$lpq6_1)
mean(nooutreplacedallcompiledrandom$lpq7_1)
sd(nooutreplacedallcompiledrandom$lpq7_1)
mean(nooutreplacedallcompiledrandom$lpq8_1)
sd(nooutreplacedallcompiledrandom$lpq8_1)
mean(nooutreplacedallcompiledrandom$lpq9_1)
sd(nooutreplacedallcompiledrandom$lpq9_1)
mean(nooutreplacedallcompiledrandom$lpq10_1)
sd(nooutreplacedallcompiledrandom$lpq10_1)
mean(nooutreplacedallcompiledrandom$lpq11_1)
sd(nooutreplacedallcompiledrandom$lpq11_1)
mean(nooutreplacedallcompiledrandom$lpq12_1)
sd(nooutreplacedallcompiledrandom$lpq12_1)
mean(nooutreplacedallcompiledrandom$lpq13_1)
sd(nooutreplacedallcompiledrandom$lpq13_1)
mean(nooutreplacedallcompiledrandom$lpq14_1)
sd(nooutreplacedallcompiledrandom$lpq14_1)
mean(nooutreplacedallcompiledrandom$lpq15_1)
sd(nooutreplacedallcompiledrandom$lpq15_1)
mean(nooutreplacedallcompiledrandom$lpq16_1)
sd(nooutreplacedallcompiledrandom$lpq16_1)
mean(nooutreplacedallcompiledrandom$lpq17_1)
sd(nooutreplacedallcompiledrandom$lpq17_1)
mean(nooutreplacedallcompiledrandom$lpq18_1)
sd(nooutreplacedallcompiledrandom$lpq18_1)
mean(nooutreplacedallcompiledrandom$lpq19_1)
sd(nooutreplacedallcompiledrandom$lpq19_1)
mean(nooutreplacedallcompiledrandom$lpq20_1)
sd(nooutreplacedallcompiledrandom$lpq20_1)

```

# Discussion

Write this by hypothesis  need to figure out the bayes factors to make sure we can strengthen the it's significant but not important. 
  Rachel's PIL talk: The covariance analyses indicated that the item relationships differed dependent upon the questionnaire delivery method. Additionally, the questionnaires delivered on paper had significantly different item scores than questionnaires delivered non-randomly on the computer. After investigating differences in scores for each item delivered on paper versus each item delivered non-randomly on the computer, items 10, 16, and 5 were found to have the largest effect size, although all effect sizes were small or negligible. These results indicate that the item differences created a cumulative effect leading to significant item difference scores between the paper delivery method and non-random computer delivery method. Total scores also appeared to differ between the paper delivery method and the non-random delivery method. While the item scores did not appear to significantly differ between the random and non-random computer delivery method, there were significant total score differences between these two methods. However, total score differences had small effect sizes for both the paper versus non-random computer delivery analysis and the random versus non-random computer delivery analysis.
  Jeff's: Overall, we see that covariance structures between delivery types indicate excellent model fit, which indicates that they are similar. However, results indicated a large difference in item scores for paper questionnaires versus those delivered not randomly by computer for both mean scores and total scores. T-tests were conducted by item to examine possible differences in specific questions. Although no specific question was significantly different based on delivery method, questions appeared to be slightly different, which accumulated over each question. This resulted in the differing item scores for the questionnaires delivered by paper and those delivered not randomly on a computer. Bayes' Factors and Tests of Equivalence (citation) were utilized to determine whether or not differences in question means were actually significant. Results are presented in Table XX. Specifically, for Tests of Equivalence, significant results suggests support for the null hypothesis. For Bayes' Factor tests, a number > 100 suggests very strong support for the alternative hypothesis in reference in the null. A score of 1-3 suggests barely any evidence for the alternative hypothesis. A score of 3-10 suggests positive evidence for the alternative hypothesis, while a Bayes' Factor score of 10-100 suggests strong support for the alternative hypothesis. 
  
***Also discuss the comparison between the sets of results. Since we have the tables we shouldn't need to add them into this script***

\newpage

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
