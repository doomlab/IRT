---
title             : "Does the Delivery Matter? Examining Randomization at the Item Level"
shorttitle        : "Item Randomization"

author: 
  - name          : "Erin M. Buchanan"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "901 S. National Ave"
    email         : "erinbuchanan@missouristate.edu"
  - name          : "Riley E. Foreman"
    affiliation   : "1"
  - name          : "Becca N. Johnson"
    affiliation   : "1"
  - name          : "Jeffrey M. Pavlacic"
    affiliation   : "2"
  - name          : "Rachel L. Swadley"
    affiliation   : "1"
  - name          : "Stefan E. Schulenberg"
    affiliation   : "2"
  

affiliation:
  - id            : "1"
    institution   : "Missouri State University"
  - id            : "2"
    institution   : "University of Mississippi"

author_note: >
  Erin M. Buchanan is an Associate Professor of Quantitative Psychology at Missouri State University. Riley E. Foreman received his undergraduate degree in Psychology and Cell and Molecular Biology at Missouri State University and is currently at Kansas City University of Medicine and Biosciences. Becca N. Johnson is a masters degree candidate at Missouri State University. Jeffrey M. Pavlacic is a doctoral candidate at The University of Mississippi. Rachel N. Swadley completed her master's degree in Psychology at Missouri State University. Stefan E. Schulenberg is a Professor of Clinical Psychology at The University of Mississippi and Director of the Clinical Disaster Research Center.
  On behalf of all authors, the corresponding author states that there is no conflict of interest.

abstract: >
  Scales that are psychometrically sound, meaning those that meet established standards regarding reliability and validity when measuring one or more constructs of interest, are customarily evaluated based on a set modality (i.e., computer or paper) and administration (fixed-item order). Deviating from an established administration profile could result in non-equivalent response patterns, indicating the possible evaluation of a dissimilar construct. Randomizing item administration may alter or eliminate these effects. Therefore, we examined the differences in scale relationships for randomized and nonrandomized computer delivery for two scales measuring meaning/purpose in life. These scales have questions about suicidality, depression, and life goals that may cause item reactivity (i.e. a changed response to a second item based on the answer to the first item). Results indicated that item randomization does not alter scale psychometrics for meaning in life scales, which implies that results are comparable even if researchers implement different delivery modalities. 

  
keywords          : "scales, randomization, item analysis"

bibliography      : ["item random.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
#class             : "man"
class             : "man, mask"
output            : papaja::apa6_pdf
---

The use of the Internet has been integrated into daily life as a means of accessing information, interacting with others, and tending to required tasks. The International Telecommunication Union reports that over half the world is online, and 70% of 15-24 year olds are on the internet [@Sanou2017]. Further, the Nielson Total Audience report from 2016 indicates that Americans spend nearly 11 hours a day in media consumption [@Media2016]. Researchers discovered that online data collection can be advantageous over laboratory and paper data collection, as it is often cheaper and more efficient [@Ilieva2001; @Schuldt1994; @Reips2012]. Internet questionnaires first appeared in the early 90s when HTML scripting code integrated form elements, and the first experiments appeared soon after [@Musch2000; @Reips2002a]. The first experimental lab on the internet was the Web Experimental Psychology Lab formed by Reips (http://www.wexlab.eu), and the use of the Internet to collect data has since grown rapidly [@Reips2002a]. What started with email and HTML forms has since moved to whole communities of available participants including websites like Amazonâ€™s Mechanical Turk and Qualtrics' Participant Panels. Participants of all types and forms are easily accessible for somewhat little to no cost. 

Our ability to collect data on the Internet has inevitably lead to the question of measurement invariance between in person and online data collection methods [@Meyerson2003; @Buchanan2005]. Invariance implies that different forms, data collection procedures, or even target demographics produce comparable sets of responses, which is a desirable characteristic to ensure a minimal number of confounding variables [@Brown2006]. According to @Deutskens2006, mail surveys and online surveys produce nearly identical results regarding the accuracy of the data collected online versus by mail. Only minor differences arise between online surveys and mail in surveys when it comes to participant honesty and suggestions. For example, participants who responded to surveys online provided more suggestions, lengthier answers, and greater information about competitors in the field that they may prefer [@Deutskens2006]. The hypothesis as to why individuals may be more honest online than in person is that the individual may feel more anonymity and less social desirability effects due to the nature of the online world, therefore less concerned about responding in a socially polite way [@Joinson1999]. A trend found by @Fang2012a shows individuals are more likely to respond to surveys online with extreme scores, rather than mid-range responses on Likert scales due to the lessened social desirability factor. There may be slight cultural differences in responses online. For example, collectivistic cultures showed greater tendency toward mid-range responses on Likert scales via in-person and online due to placing greater value on how they are socially perceived; however, the trend is still the same as scores are more extreme online versus in person or by mail [@Fang2012]. 

Although work by Dillman and his group [@Frick2001; @Smyth2006; @Dillman2008], among others, has shown that many web surveys are plagued by problems of usability, display, coverage, sampling, non-response, or technology, other studies have found internet data to be reliable and almost preferable as it produces a varied demographic response compared to the traditional sample of introduction to psychology college students while also maintaining data equivalence [@Lewis2009]. However, equivalence in factor structure may be problematic, as @Buchanan2005 have shown that factor structure was not replicable in online and in person surveys. Other work has shown equivalence using a comparison of correlation matrices [@Meyerson2003] or *t*-tests [@Schulenberg1999; @Schulenberg2001], and the literature is mixed on how different methodologies impact factor structure. @Weigold2013 recently examined both quantitative and research design questions (i.e. missing data) on Internet and paper-and-pencil administration which showed that the administrations were generally equivalent for quantitative structure but research design issues showed non-equivalence. Other potential limitations to online surveys include the accessibility of different populations to the Internet [@Frick2001], selection bias [@Bethlehem2010], response rates [@Cook2000; @Hox1994; @DeLeeuw1988; @Cantrell2007], attrition [@Cronk2002], and distraction [@Tourangeau1999]. Many of these concerns have been alleviated in the years since online surveys were first developed, especially with the advent of panels and Mechanical Turk to reach a large, diverse population of participants [@Buhrmester2011]. 

With the development of advanced online survey platforms such as Qualtrics and Survey Monkey, researchers have the potential to control potentially confounding research design issues through randomization, although other issues may still be present, such as participant misbehavior [@Nosek2002]. Randomization has been a hallmark of good research practice, as the order or presentation of stimuli can be a noise variable in a study with multiple measures [@Keppel2004]. Thus, researchers have often randomized scales by rotating the order of presentation in paper format or simply clicking the randomization button for web-based studies. This practice has counterbalanced out any order effects of going from one scale to the next [@Keppel2004]. However, while scale structure has remained constant, these items are still stimuli within a larger construct. Therefore, these construct-related items have the ability to influence the items that appear later on the survey, which we call item reactivity. For example, a question about being *prepared for death* or *thoughts about suicide* might change the responses to further questions, especially if previous questions did not alert participants to be prepare for that subject matter. 

Scale development typically starts with an underlying latent variable that a researcher wishes to examine through measured items or questions [@DeVellis2016a]. Question design is a well-studied area that indicates that measurement is best achieved through questions that are direct, positively worded, and understandable to the subject [@Dillman2008]. @Olson2010 suggests researchers design a multitude of items in order to investigate and invite subject matter experts to examine these questions. Subject matter experts were found to be variable in their agreement, but excellent at identifying potentially problematic questions. After suggested edits from these experts, a large sample of participant data is collected. While item response theory is gaining traction, classical test theory has dominated this area through the use of exploratory and confirmatory factor analysis [EFA, CFA; @Worthington2006]. EFA elucidates several facets of how the measured items represent the latent trait through factor loadings and overall model fit [@Tabachnick2012]. Factor structure represents the correlation between item scores and factors, where a researcher wishes to find items that are strongly related to a given latent trait. Items that are not related to the latent trait, usually with factor loadings below .300 [@Preacher2003] are discarded. Model fit is examined when simple structure has been achieved (i.e. appropriate factor loadings for each item), and these fit indices inform if the items and factor structure model fit the data well. Well-designed scales include items that are highly related to their latent trait and have excellent fit indices. Scale development additionally includes the examination of other measures of reliability (alpha) and construct validity (relation to other phenomena) but the focus of the scale shifts to subscale or total scores [@Buchanan2014]. Published scales are then distributed for use in the form that is presented in the publication, as item order is often emphasized through important notes about reverse scoring and creating subscale scores. 

The question is no longer whether web-based surveys are reliable sources of data collection; the theory now is in need of a shift to whether or not item-randomization in survey data collection creates psychometric differences. These scale development procedures focus on items, and EFA/CFA statistically try to mimic variance-covariance structure by creating models of the data with the same variance-covariance matrix. If we imagine that stimuli in a classic experimental design can influence the outcome of a study because of their order, then certainly the stimuli on a scale (i.e., the items) can influence the pattern of responses for items. This area of study is relatively unexplored, as easily randomizing items has only recently become available for researchers. 

Therefore, this study focuses on potential differences in results based on item randomization delivery methodology. The current project examined large samples on two logotherapy-related scales, as these scales include potentially reactive items, as well as both a dichotomous True/False and traditional Likert format for the same items. Large samples were desirable to converge on a stable, representative population; however, false positives (i.e., Type I errors) can occur by using large *N*. Recent developments in the literature focusing on null hypothesis testing make it especially important to present potential alternatives to *p*-values [@Valentine2017]. While a large set of researchers have argued that the literature is full of Type I errors [@Benjamin2017], and thus, the $\alpha$ value should be shifted lower (i.e., *p* < .005 for statistical significance), an equally large set of researchers counter this argument as unfounded and weak [@Lakens2017]. We provide multiple sources of evidence (*p*-values, effect sizes, Bayes Factors, and tests of equivalence) to determine if differences found are not only statistically significant, but also practically significant. In our study, we expand to item randomization for online based surveys, examining the impact on factor loadings, variance-covariance structure, item means, and total scores again providing evidence of difference/non-difference from multiple statistical sources. Finally, we examine these scenarios with a unique set of scales that have both dichotomous True/False and traditional Likert formats to explore how the answer response options might impact any differences found between randomized and nonrandomized methodologies. 

```{r libraries, include = FALSE}
library(papaja)
library(car)
library(memisc)
library(moments)
library(mice)
library(monomvn)
library(psych)
library(TOSTER)
library(MOTE)
library(BayesFactor)
```

# Method

## Participants
The sample population consisted of undergraduate students at a large Midwestern University, placing the approximate age of participants at around 18-22. Table \@ref(tab:demo-table) includes the demographic information about all datasets. Only two scales were used from each dataset, as described below. Participants were generally enrolled in an introductory psychology course that served as a general education requirement for the university. As part of the curriculum, the students were encouraged to participate in psychology research programs, resulting in their involvement in this study. These participants were given course credit for their participation. 

## Materials
Of the surveys included within each larger study, two questionnaires were utilized: the Purpose in Life Questionnaire [PIL; @Crumbaugh1964] and the Life Purpose Questionnaire [LPQ; @Hutzell1988].

### The Purpose in Life Questionnaire
The PIL is a 20-item questionnaire that assesses perceived meaning and life purpose. Items are structured in a 7-point Likert type response format; however, each item has different anchoring points that focus on item content. Total scores are created by summing the items, resulting in a range of 20 to 140 for the overall score. The reliability for the scale is generally high, ranging from .70 to .90 [@Schulenberg2004; @Schulenberg2010]. Previous work on validity for the PIL showed viable one- and two-factor models, albeit factor loadings varied across publications [see @Schulenberg2010 for a summary], and these fluctuating results lead to the development of a 4-item PIL short form [@Schulenberg2011]. 

### Life Purpose Questionnaire
The LPQ was modeled after the full 20-item PIL questionnaire, also measuring perceived meaning and purpose in life. The items are structured in a true/false response format, in contrast to the Likert response format found on the PIL. Each question is matched to the PIL with the same item content, altering the question to create binary answer format. After reverse coding, zero on an item would indicate low meaning, while one on an item would indicate high meaning. A total score is created by summing item scores, resulting in a range from 0 to 20. In both scales, higher scores indicated greater perceived meaning in life. Reliability for this scale is also correspondingly high, usually in the .80 range [@Melton2008; @Schulenberg2004].

These two scales were selected because they contained the same item content with differing response formats, which would allow for cross comparisons between results for each scale. 

## Procedure
The form of administration was of interest to this study, and therefore, two formats were included: computerized administration in nonrandom order and computerized administration with a randomized question order. Computerized questionnaires were available for participants to access electronically, and they were allowed to complete the experiment from anywhere with the Internet through Qualtrics. To ensure participants were properly informed, both an introduction and a debriefing were included within the online form. Participants were randomly assigned to complete a nonrandomized or randomized version of the survey. Nonrandomized questionnaires followed the original scale question order, consistent with paper delivery format. A different group of participants were given each question in a randomized order within each scale (i.e. all PIL and LPQ questions will still grouped together on one page). The order of administration of the two scales was randomized across participants for both groups. Once collected, the results were then amalgamated into a database for statistical analysis.

# Results

## Hypothesis and Data-Analytic Plan
Computer forms were analyzed by randomized and nonrandomized groups to examine the impact of randomization on covariance structure, factor loadings, item means, and total scores. We expected to find that these forms may potentially vary across covariance structure and item means, which would indicate differences in reactivity to questions (i.e. item four always has item three as a precursor on a nonrandom form, while item four may have a different set of answers when prefaced with other questions). Factor loadings were assessed to determine if differences in randomization caused a change in focus, such that participant interpretation of the item changed the relationship to the latent variable. However, we did not predict if values would change, as latent trait measurement should be consistent. Last, we examined total scores; however, it was unclear if these values would change. A difference in item means may result in changes in total scores, but may also result in no change if some item means decrease, while others increase. 

Each hypothesis was therefore tested using four dependent measures. First, we examined the variance-covariance matrix for each type of delivery and compared the matrices to each other by using root mean squared error (RMSE). RMSE estimates the difference between covariance matrices and is often used in structural equation modeling to determine if models have good fit to the data. A criterion of < .06 for good fit, .06-.08 for acceptable fit, and > .10 for bad fit was used [@Hu1999]. This analysis was used to determine if the change in delivery changed the structure of the item relationships to each other (i.e. if the covariance matrices are different). RMSE values were calculated using the *monomvn* package in *R* [@Gramacy2010].

We then conducted an exploratory factor analysis on both scales using one-factor models to examine the loading of each item on its latent trait. The PIL factor structure is contested [@Strack2009] with many suggestions as to latent structure for one- and two-factor models. The LPQ has seen less research on factor structure [@Schulenberg2004]. This paper focused on loadings on one global latent trait to determine if the manipulation of delivery impacted factor loadings. We used a one-factor model and included all questions to focus on the loadings, rather than the factor structure. The analysis was performed using the *psych* package in *R* with maximum likelihood estimation and an oblique (oblimin) rotation. The LPQ factor analysis used tetrachoric correlation structure to control for the dichotomous format of the scale, rather than traditional Pearson correlation structure. The loadings were then compared using a matched dependent *t*-test (i.e. item one to item one, item two to item two) to examine differences between nonrandomized and randomized computer samples. 

Next, item averages were calculated across all participants for each item. These 20 items were then compared in a matched dependent *t*-test to determine if delivery changed the mean of the item on the PIL or LPQ. While covariance structure elucidates the varying relations between items, we may still find that item averages are pushed one direction or another by a change in delivery and still maintain the same correlation between items. If this test was significant, we examined the individual items across participants for large effect sizes, as the large sample sizes in this study would create significant *t*-test follow ups. 

Last, the total scores for each participant were compared across delivery type using an independent *t*-test. Item analyses allow a focus on specific items that may show changes, while total scores allow us to investigate if changes in delivery alter the overall score that is used in other analyses or possible clinical implications. For analyses involving *t*-tests, we provide multiple measures of evidentiary value so that researchers can weigh the effects of randomization on their own criterion. Recent research on $\alpha$ criteria has shown wide disagreement on the usefulness of *p*-values and set cut-off scores [@Benjamin2017; @Lakens2017]. Therefore, we sought to provide traditional null hypothesis testing results (*t*-tests, *p*-values) and supplement these values with effect sizes [*d* and non-central confidence intervals, @Cumming2014; @Buchanan2017; @Smithson2001], Bayes Factors [@Kass1995; @Morey2015b], and one-sided tests of equivalence [TOST, @Cribbie2004; @Lakens2017a; @Schuirmann1987; @Rogers1993]. We used the average standard deviation of each group as the denominator for *d* calculation as follows:

$$
d_{av} = \frac {(M_1 -  M_2) } { \frac{SD_1 + SD_2 } {2} }
$$
This effect size is less biased than the traditional $d_z$ formula, wherein mean differences are divided by the standard deviation of the difference scores [@Lakens2013]. The difference scores standard deviation is often much smaller than the average of the standard deviations of each level, which can create an upwardly biased effect size [@Cumming2014]. This bias can lead researchers to interpret larger effects for a psychological phenomenon than actually exist. 

Bayes Factors are calculated in opposition to a normal frequentist (NHST) approach, as a ratio of the likelihood of two models. Traditional NHST focuses on the likelihood of the data, given the null hypothesis is true, and Bayesian analysis instead posits the likelihood of a hypothesis given the data. Prior distributions are our estimation of the likelihood of our hypothesis before the data was collected, which is combined with the data collected to form a posterior belief of our hypothesis. We chose to use Bayes Factors as a middle ground to the Bayesian analysis continuum, that uses mildly uninformative priors and allows for the data to strongly impact the posterior distribution. The choice of prior distribution can heavily influence the posterior belief, in that uninformative priors allow the data to comprise the posterior distribution. However, most researchers have a background understanding of their field, thus, making completely uninformative priors a tenuous assumption. Because of the dearth of literature in this field, there is not enough previous information to create a strong prior distribution, which would suppress the effect of the data on posterior belief. The *BayesFactor* package [@Morey2015b] uses recommended default priors that cover a wide range of data [@Morey2015b; @Rouder2009] of a Jeffreys prior with a fixed rscale (0.5) and random rscale (1.0). The alternative model is generally considered a model wherein means between groups or items differ, and this model is compared to a null model of no mean differences. The resulting ratio is therefore the odds of the alternative model to the null, where BF values less than one indicate evidence for the null, values at one indicate even evidence for the null and alternative, and values larger than one indicate evidence for the alternative model. While some researchers have posed labels for BF values [@Kass1995], we present these values as a continuum to allow researchers to make their own decisions [@Morey2015b; @Morey2015c]. 

NHST has also been criticized for an inability to test the null hypothesis, and thus, show evidence of the absence of an effect. Non-significant *p*-values are often misinterpreted as evidence for the null hypothesis [@Lakens2017a]. However, we can use the traditional frequentist approach to determine if an effect is within a set of equivalence bounds. We used the two one-sided tests approach to specify a range of raw-score equivalence that would be considered supportive of the null hypothesis (i.e. no worthwhile effects or differences). TOST are then used to determine if the values found are outside of the equivalence range. Significant TOST values indicate that the effects are *within* the range of equivalence. We used the *TOSTER* package [@Lakens2017a] to calculate these values, and graphics created from this package can be found online at https://osf.io/gvx7s/. 

The equivalence ranges are often tested by computing an expected effect size of negligible range; however, the TOST for dependent *t* uses $d_z$, which can overestimate the effect size of a phenomena [@Cumming2014; @Lakens2013]. Therefore, we calculated TOST tests on raw score differences to alleviate the overestimation issues. For EFA, we used a change score of .10 in the loadings, as Comrey and Lee (1992) suggested loading estimation ranges, such as .32 (poor) to .45 (fair) to .55 (good), and the differences in these ranges are approximately .10 [as cited in @Tabachnick2012, p. 654]. Additionally, this score would amount to a small correlation change using traditional guidelines for interpretation of *r* [@Cohen1992a]. For item and total score differences, we chose a 5% change in magnitude as the raw score cut off as a modest raw score change. To calculate that change for total scores, we used the following formula:

$$
(Max*N_{Questions} - Min*N_{Questions}) * Change
$$
Minimum and maximum values indicate the lower and upper end of the answer choices (i.e. 1 and 7), and change represented the proportion magnitude change expected. Therefore, for total PIL scores, we proposed a change in 6 points to be significant, while LPQ scores would change 1 point to be a significant change. For item analyses, we divided the total score change by the number of items to determine how much each item should change to impact the total score a significant amount (PIL = 0.30, LPQ = .05).

## Data Screening 
Each dataset was analyzed separately by splitting on scale and randomization, and first, all data were screened for accuracy and missing data. Participants with more than 5% missing data (i.e. 2 or more items) were excluded. Data were imputed using the *mice* package in *R* for participants with less than 5% of missing data [@VanBuuren2011]. Next, each dataset was examined for multivariate outliers using Mahalanobis distance [@Tabachnick2012]. Each dataset was then screened for multivariate assumptions of additivity, linearity, normality, homogeneity, and homoscedasticity. While some data skew was present, large sample sizes allowed for the assumption of normality of the sampling distribution. Information about the number of excluded data points in each step is presented in Table \@ref(tab:demo-table).

```{r PIL-Data-Screening, include = FALSE}
####import the files####
PIL = read.csv("PIL.csv")

##reverse coding the PIL
PIL[ , c("PIL2", "PIL5", "PIL7", "PIL10", "PIL14", "PIL15", "PIL17", "PIL18", "PIL19")] = 8 - PIL[ , c("PIL2", "PIL5","PIL7", "PIL10", "PIL14", "PIL15", "PIL17", "PIL18", "PIL19")]

##subsetting the PIL dataset
nr = subset(PIL, Source == "not random")
r = subset(PIL, Source == "random")

######Random data screening#####
randomcomputer = r[, c(3:22)]
summary(randomcomputer)
apply(randomcomputer, 2, table)

##missing data
##participants (row)
percentmiss = function(x) { sum(is.na(x))/length(x)*100}
missingrandomcomputer = apply(randomcomputer, 1, percentmiss)
table(missingrandomcomputer)
replacepeoplerandom = subset(randomcomputer, missingrandomcomputer <= 5)
summary(replacepeoplerandom)

##variables (column)
apply(replacepeoplerandom, 2, percentmiss)
replacecolumnrandom = replacepeoplerandom[ , -15]
dontcolumnrrandom = replacepeoplerandom[ , 15]

##replace data
tempnomissrandom = mice(replacecolumnrandom)
replacedrandom_temp = complete(tempnomissrandom, 1)

replacedrandom = cbind(replacedrandom_temp[ , 1:14], 
                       PIL15 = dontcolumnrrandom, 
                       replacedrandom_temp[ , 15:19])
 summary(replacedrandom)

##outliers
mahalrandom = mahalanobis(replacedrandom[ , ],
                    colMeans(replacedrandom[ , ], na.rm = T),
                    cov(replacedrandom[ , ], use = "pairwise.complete.obs"))

cutoff = qchisq(1-.001, ncol(replacedrandom[ , ]))
ncol(replacedrandom[ , ])
cutoff
summary(mahalrandom < cutoff)
nooutrandom = subset(replacedrandom, mahalrandom < cutoff)

##assumptions
##additivity
correlrandom = cor(nooutrandom[ , ])
symnum(correlrandom)

finalrandomP = nooutrandom

##set up for assumptions
randomrandom = rchisq(nrow(finalrandomP), 7)
fakerandom = lm(randomrandom ~ ., data = finalrandomP)
standardizedrandom = rstudent(fakerandom)
fittedrandom = scale(fakerandom$fitted.values)

##normality
skewness(finalrandomP[ , ], na.rm = T)
kurtosis(finalrandomP[ , ], na.rm = T)
#hist(standardizedrandom)

##linearity
#qqnorm(standardizedrandom)
#abline(0,1)

##homog + s
#plot(fittedrandom, standardizedrandom)
#abline(0,0)
#abline(v = 0)

######NOTRandom data screening#####
notrandomcomputer = nr[, c(3:22)]
summary(notrandomcomputer)
apply(notrandomcomputer, 2, table)

##missing data
##participants (row)
missingnotrandomcomputer = apply(notrandomcomputer, 1, percentmiss)
table(missingnotrandomcomputer)
replacepeoplenotrandom = subset(notrandomcomputer, missingnotrandomcomputer <= 5)
summary(replacepeoplenotrandom)

##variables (columns)
apply(replacepeoplenotrandom, 2, percentmiss)

##replace the data
tempnomissnotrandom = mice(replacepeoplenotrandom)
replacednotrandom = complete(tempnomissnotrandom, 1)
summary(replacednotrandom)

##outliers
mahalnotrandom = mahalanobis(replacednotrandom[ , ],
                          colMeans(replacednotrandom[ , ], na.rm = T),
                          cov(replacednotrandom[ , ], use = "pairwise.complete.obs"))

cutoff = qchisq(1-.001, ncol(replacednotrandom[ , ]))
ncol(replacednotrandom[ , ])
cutoff
summary(mahalnotrandom < cutoff)
nooutnotrandom = subset(replacednotrandom, mahalnotrandom < cutoff)

##assumptions
##additivity
correlnotrandom = cor(nooutnotrandom[ , ])
symnum(correlnotrandom)

finalnotrandomP = nooutnotrandom

##set up for assumptions
randomnotrandom = rchisq(nrow(finalnotrandomP), 7)
fakenotrandom = lm(randomnotrandom ~ ., data = finalnotrandomP)
standardizednotrandom = rstudent(fakenotrandom)
fittednotrandom = scale(fakenotrandom$fitted.values)

##normality
skewness(finalnotrandomP[ , ], na.rm = T)
kurtosis(finalnotrandomP[ , ], na.rm = T)
#hist(standardizednotrandom)

##linearity
#qqnorm(standardizednotrandom)
#abline(0,1)

##homog + s
#plot(fittednotrandom, standardizednotrandom)
#abline(0,0)
#abline(v = 0)
```

```{r LPQ-Data-Screening, include=FALSE}
##import the data files
LPQallcompiled = read.csv("LPQ.csv")

###changing to 0 and 1 - columns are currently 1 true, 2 false, so subtract 2
names(LPQallcompiled)
summary(LPQallcompiled)
##fix the 12 typo
LPQallcompiled$lpq16_1[ LPQallcompiled$lpq16_1 == 12] = NA

allcolumns = c("lpq1_1", "lpq2_1", "lpq3_1", "lpq4_1", "lpq5_1", "lpq6_1", "lpq7_1", "lpq8_1", "lpq9_1", "lpq10_1", "lpq11_1", "lpq12_1", "lpq13_1", "lpq14_1", "lpq15_1", "lpq16_1", "lpq17_1", "lpq18_1", "lpq19_1", "lpq20_1")
LPQallcompiled[ , allcolumns] = 2 - LPQallcompiled[ , allcolumns]

###reverse code items 1, 2, 5, 8, 9, 11, 12, 14, 15, 16, 19
reverse = c("lpq1_1", "lpq2_1", "lpq5_1", "lpq8_1", "lpq9_1", "lpq11_1", "lpq12_1", "lpq14_1", "lpq15_1", "lpq16_1", "lpq19_1")
LPQallcompiled[ , reverse] = 1 - LPQallcompiled[ , reverse]
summary(LPQallcompiled)

##make sure this dataaset doesn't have any decimals or weird numbers
apply(LPQallcompiled[ , allcolumns], 2, table)

##the csv files are a mix of computer and paper, so need to subset
LPQallcompiledrandom = subset(LPQallcompiled, Source == 'random')
LPQallcompilednotrandom = subset(LPQallcompiled, Source == 'not random')

####Random data screening####
LPQallcompiledrandom = LPQallcompiledrandom[ , 3:22]
summary(LPQallcompiledrandom)

##missing lpqallcompiled random 
missingallcompiledrandom = apply(LPQallcompiledrandom, 1, percentmiss)
table(missingallcompiledrandom)
replacepeopleallcompiledrandom = subset(LPQallcompiledrandom, missingallcompiledrandom <=5)

##check for columns
apply(replacepeopleallcompiledrandom, 2, percentmiss)

##replace data
tempnomiss = mice(replacepeopleallcompiledrandom)
replacedallcompiledrandom = complete(tempnomiss, 1)
summary(replacedallcompiledrandom)

##outliers
mahal = mahalanobis(replacedallcompiledrandom, 
                    colMeans(replacedallcompiledrandom, na.rm = T), 
                    cov(replacedallcompiledrandom, use = "pairwise.complete.obs"))
cutoff = qchisq(1-.001, ncol(replacedallcompiledrandom))
cutoff
ncol(replacedallcompiledrandom)
summary(mahal < cutoff)
nooutreplacedallcompiledrandom = subset(replacedallcompiledrandom, mahal < cutoff)

##assumptions 
##additivity
correlreplacedallcompiledrandom = cor(nooutreplacedallcompiledrandom)
symnum(correlreplacedallcompiledrandom) ##good

##set up for assumptions 
random = rchisq(nrow(nooutreplacedallcompiledrandom), 7)
fake = lm(random ~ ., data = nooutreplacedallcompiledrandom)
standardized = rstudent(fake)
fitted = scale(fake$fitted.values)

##normality 
#hist(standardized) ##positive skew

##linearity 
#qqnorm(standardized)
#abline(0, 1) ##looks good

##homog and s
#plot(fitted, standardized)
#abline(0, 0)
#abline(v = 0)##yes!

finalrandomL = nooutreplacedallcompiledrandom

####Not random data screening####
LPQallcompilednotrandom = LPQallcompilednotrandom[ , 3:22]
summary(LPQallcompilednotrandom)

##missing lpqallcompilednotrandom
missingallcompilednotrandom = apply(LPQallcompilednotrandom, 1, percentmiss)
table(missingallcompilednotrandom)
replacepeopleallcompilednotrandom = subset(LPQallcompilednotrandom, missingallcompilednotrandom <= 5)

##columns
apply(replacepeopleallcompilednotrandom,2,percentmiss)

##replace data
tempnomiss = mice(replacepeopleallcompilednotrandom)
replacedallcompilednotrandom = complete(tempnomiss, 1)
summary(replacedallcompilednotrandom)

##outliers
mahal = mahalanobis(replacedallcompilednotrandom, 
                    colMeans(replacedallcompilednotrandom, na.rm = T), 
                    cov(replacedallcompilednotrandom, use = "pairwise.complete.obs"))
cutoff = qchisq(1-.001, ncol(replacedallcompilednotrandom))
cutoff
ncol(replacedallcompilednotrandom)
summary(mahal < cutoff)
nooutreplacedallcompilednotrandom = subset(replacedallcompilednotrandom, mahal < cutoff)

##assumptions 
##additivity
correlreplacedallcompilednotrandom = cor(nooutreplacedallcompilednotrandom)
symnum(correlreplacedallcompilednotrandom) ##yay

##set up for assumptions 
random = rchisq(nrow(nooutreplacedallcompilednotrandom), 7)
fake = lm(random ~ ., data = nooutreplacedallcompilednotrandom)
standardized = rstudent(fake)
fitted = scale(fake$fitted.values)

##normality 
#hist(standardized) ##positive skew still

##linearity 
#qqnorm(standardized)
#abline(0, 1) ##its okay 

##homog and s
#plot(fitted, standardized)
#abline(0, 0)
#abline(v = 0)##meets both

finalnotrandomL = nooutreplacedallcompilednotrandom
```

```{r demo-table, results = 'asis', echo = FALSE}
demo = read.csv("demographics.csv")
gender = tapply(demo$Gender, demo$Source, table)
ethnic = tapply(demo$Ethnicity, demo$Source, table)
Mage = tapply(demo$Age, demo$Source, mean, na.rm = T)
SDage = tapply(demo$Age, demo$Source, sd, na.rm = T)
##make a blank table
demotable = matrix(NA, nrow = 4, ncol = 7)

##create column names
colnames(demotable) = c("Group", "Female", "White", "Age (SD)", "Original N", "Missing N", "Outlier N")

##stick in the information you need
demotable[ , 1] = c("PIL Random", "PIL Not Random", "LPQ Random", "LPQ Not Random")
demotable[ , 2] = c(apa(gender$random["female"]/sum(gender$random)*100,1), 
                    apa(gender$`not random`["female"] / sum(gender$`not random`)*100, 1),
                    "-", "-")
demotable[ , 3] = c(apa(ethnic$random["White"] / sum(ethnic$random)*100, 1), 
                    apa(ethnic$`not random`["White"] / sum(ethnic$`not random`)*100, 1),
                    "-", "-")
demotable[ , 4] = c(paste( apa(Mage["random"],2), " (", apa(SDage["random"],2), ")", sep = ""),
                    paste( apa(Mage["not random"],2), " (", apa(SDage["not random"], 2), ")", sep = ""),
                    "-", "-")
demotable[ , 5] = c(nrow(randomcomputer), nrow(notrandomcomputer), 
                    nrow(LPQallcompiledrandom), nrow(LPQallcompilednotrandom))
demotable[ , 6] = as.numeric(demotable[ , 5]) - 
                  c(nrow(replacedrandom) , nrow(replacednotrandom),
                    nrow(replacedallcompiledrandom), nrow(replacedallcompilednotrandom))
demotable[ , 7] = as.numeric(demotable[ , 5]) - as.numeric(demotable[ , 6]) - 
                  c(nrow(finalrandomP), nrow(finalnotrandomP),
                    nrow(finalrandomL), nrow(finalnotrandomL))
##note for PIL random 256 of those are the bad data with one question, so added that to the missing column. 
demotable[1, 6] = as.numeric(demotable[1, 6]) + 256
demotable[1, 7] = as.numeric(demotable[1, 7]) - 256


##print it (apa)
apa_table(
  demotable
  , align = c("l", rep("c", 6))
  , caption = "Demographic and Data Screening Information",
  note = "Participants took both the PIL and LPQ scale, therefore, random and not random demographics are the same. Not every participant was given the LPQ, resulting in missing data for those subjects. Several PIL participants were removed because they were missing an item on their scale."
)
```

## PIL Analyses

### Covariance Matrices

```{r Pcov-analysis, include=FALSE}
###make covariance tables
not_corP = cov(finalnotrandomP, use="pairwise")
rand_corP = cov(finalrandomP, use="pairwise")

#mean tables
notrandommP = unlist(sapply(finalnotrandomP, function(cl) list(means=mean(cl,na.rm=TRUE))))
randommP = unlist(sapply(finalrandomP, function(cl) list(means=mean(cl,na.rm=TRUE))))

##rsme mean, cov, mean, cov
RMSEP = rmse.muS(notrandommP, not_corP, randommP, rand_corP)
##figure out how to do standardized residuals

##formula is cov - cov / sqrt ( var one - var other )
#do var 1 - var 1.1, then var 2 - var 2.2, then take the variance of that vector
not_vP = apply(finalnotrandomP, 2, var)
rand_vP = apply(finalrandomP, 2, var)

##figure out how to report these since you can't just divide by 2 - use csvs to figure out which and how many are over. 
##sum up the standardized residuals that are over 1.96
stdP = (not_corP - rand_corP) / sqrt(var(not_vP - rand_vP))
overP = sum(as.numeric(abs(stdP)) > 1.96)
##print out the standardized residuals to put online
write.csv(stdP, "stdres_not_randomP.csv")

##pil 7 rand_corP[7,7]

```

Covariance structure was considered different [i.e. above .10; @Hu1999] for the randomized and not randomized forms of item order, *RMSE* = `r apa(RMSEP, 2, F)`. Standardized residuals were calculated by dividing the difference in covariance tables by the variance of the differences [@Hausman1978]. While *RMSE* indicated partial misfit between the covariance relationships, only `r overP` values were significantly different using *Z* of 1.96 as a criterion: the variances of PIL 7 and 14. PIL 7 in a randomized form had less variance ($SD^2$ = `r apa(rand_corP[7,7], 2)`) than the nonrandomized form ($SD^2$ = `r apa(not_corP[7,7], 2)`). Likewise, PIL 14 randomized had a smaller variance ($SD^2$ = `r apa(rand_corP[14,14], 2)`) than the nonrandomized form ($SD^2$ = `r apa(not_corP[14,14], 2)`). Questions about retirement and freedom to make choices decreased in variance when they were randomly presented. 

### Factor Loadings

```{r Pfactor-load, include = FALSE}
FLnotP= fa(finalnotrandomP, nfactors = 1, rotate = "oblimin", fm = "ml")
FLrandomP = fa(finalrandomP, nfactors = 1, rotate = "oblimin", fm = "ml") 

##dependent t-test to determine if they are p value different
FLPt = t.test(FLnotP$loadings[1:20], FLrandomP$loadings[1:20], paired = T)

##effect sizes
FLPd = d.dep.t.avg(mean(FLrandomP$loadings[1:20]),mean(FLnotP$loadings[1:20]),
                   sd(FLrandomP$loadings[1:20]), sd(FLnotP$loadings[1:20]),
                   n = length(FLrandomP$loadings[1:20]), a = .05)

FLPdz = d.dep.t.diff(mean(FLrandomP$loadings[1:20] - FLnotP$loadings[1:20]),
                     sd(FLrandomP$loadings[1:20] - FLnotP$loadings[1:20]), 
                     length(FLrandomP$loadings[1:20]), 
                     a = .05)
##toster
##using a cut off score of 0.10 difference because it matches the definition of loadings
FLPtost = TOSTpaired.raw(n = length(FLrandomP$loadings[1:20]),
                         m1 = mean(FLrandomP$loadings[1:20]),
                         m2 = mean(FLnotP$loadings[1:20]),
                         sd1 = sd(FLrandomP$loadings[1:20]),
                         sd2 = sd(FLnotP$loadings[1:20]),
                         r12 = cor(FLrandomP$loadings[1:20], FLnotP$loadings[1:20]),
                         low_eqbound = -0.10,
                         high_eqbound = 0.10,
                         alpha = .05)

##bayes factor
FLPbf = ttestBF(x = FLrandomP$loadings[1:20], y = FLnotP$loadings[1:20], paired = TRUE)
##note I hard coded the p values for TOST and BF, need to change if we change things. 
```

Table \@ref(tab:Ptable) includes the factor loadings from the one-factor EFA analysis. These loadings were compared using a dependent *t*-test matched on item, and they were not significantly different, `r apa_print(FLPt)$full_result`. The effect size for this test was correspondingly negligible, $d_{av}$ = `r apa(FLPd$d, 2)` 95% CI [`r apa(FLPd$dlow, 2)`, `r apa(FLPd$dhigh, 2)`]. The TOST test was significant for both the lower, *t*(19) = `r apa(FLPtost$TOST_t1, 2)`, *p* < .001 and the upper bound, *t*(19) = `r apa(FLPtost$TOST_t2, 2)`, *p* < .001. This result indicated that the change score was within the confidence band of expected negligible changes. Lastly, the BF for this test was 0.24 Â±0.02%, which indicated support for the null model. 

### Item Means

```{r Pitem, include = FALSE}
##means and sds for the table
MnrP = apply(finalnotrandomP, 2, mean)
SDnrP = apply(finalnotrandomP, 2, sd)
MrP = apply(finalrandomP, 2, mean)
SDrP = apply(finalrandomP, 2, sd)

##t-test
IPt = t.test(MnrP, MrP, paired = T)

##effect size
IPd = d.dep.t.avg(mean(MnrP), mean(MrP), sd(MnrP), sd(MrP), length(MnrP), .05)
IPdz = d.dep.t.diff(mean(MnrP - MrP), sd(MnrP - MrP), length(MnrP), .05)

##toster
##first calculate the cut off score for a certain percent change you are interested in
minscale = 1 #min scale point
maxscale = 7 #max scale point
noq = 20 #number of questions
perchange = .05 #percent change of total you think is important

cutoff = ((maxscale*noq - minscale*noq) * perchange) / noq

IPtost = TOSTpaired.raw(n = length(MnrP),
                         m1 = mean(MnrP),
                         m2 = mean(MrP),
                         sd1 = sd(MnrP),
                         sd2 = sd(MrP),
                         r12 = cor(MnrP, MrP),
                         low_eqbound = -cutoff,
                         high_eqbound = cutoff,
                         alpha = .05)

##bayes factor
IPbf = ttestBF(x = MnrP, y = MrP, paired = T)
```

Table \@ref(tab:Ptable) includes the means and standard deviation of each item from the PIL scale. The item means were compared using a dependent *t*-test matched on item. Item means were significantly different `r apa_print(IPt)$full_result`. The effect size for this difference was small, $d_{av}$ = `r apa(IPd$d, 2)` 95% CI [`r apa(IPd$dlow, 2)`, `r apa(IPd$dhigh, 2)`]. Even though the *t*-test was significant, the TOST test indicated that the difference was within the range of a 5% percent change in item means (`r apa(cutoff, 2)`). The TOST test for lower bound, *t*(19) = `r apa(IPtost$TOST_t1, 2)`, *p* < .001 and the upper bound, *t*(19) = `r apa(IPtost$TOST_t2, 2)`, *p* < .001, suggested that the significant *t*-test may be not be interpreted as a meaningful change on the item means. The BF value for this test indicated 6.86 <0.01%, which is often considered weak evidence for the alternative model. Here, we find mixed results, indicating that randomization may change item means for the PIL. 

### Total Scores 

```{r Ptotal, include = FALSE}
##create total scores
finalnotrandomP$total = apply(finalnotrandomP, 1, sum)
finalrandomP$total = apply(finalrandomP, 1, sum)

##descriptives
MTnrP = mean(finalnotrandomP$total)
MTrP = mean(finalrandomP$total)
SDTnrP = sd(finalnotrandomP$total)
SDTrP = sd(finalrandomP$total)

##t-test
TPt = t.test(finalnotrandomP$total, finalrandomP$total,
             var.equal = T,
             paired = F)

##effect size
TPd = d.ind.t(MTnrP, MTrP, SDTnrP, SDTrP, 
              length(finalnotrandomP$total), length(finalrandomP$total), a = .05)

##toster
##change cut off to the full number of questions
totcutoff = cutoff * noq

TPtost = TOSTtwo.raw(m1 = MTnrP, m2 = MTrP, sd1 = SDTnrP, sd2 = SDTrP,
                 n1 = length(finalnotrandomP$total), n2 = length(finalrandomP$total), 
                 low_eqbound = -totcutoff, high_eqbound = totcutoff, 
                 alpha = .05, var.equal = T)


##bayes factor
TPbf = ttestBF(finalnotrandomP$total, finalrandomP$total, paired = F)
```

```{r Ptable, echo = FALSE, results = 'asis'}
##make a blank table
Ptable = matrix(NA, nrow = 20, ncol = 7)

##create column names
colnames(Ptable) = c("Item","FL-R","FL-NR","M-R","SD-R","M-NR","SD-NR")

##stick in the information you need
Ptable[ , 1] = apa(1:20, 0)
Ptable[ , 2] = apa(FLrandomP$loadings[1:20], 3, F)
Ptable[ , 3] = apa(FLnotP$loadings[1:20], 3, F)
Ptable[ , 4] = apa(MrP, 3)
Ptable[ , 5] = apa(SDrP, 3)
Ptable[ , 6] = apa(MnrP, 3)
Ptable[ , 7] = apa(SDnrP, 3)

##print it (apa)
apa_table(
  Ptable
  , align = c("l", rep("c", 6))
  , caption = "Item Statistics for the PIL Scale",
  note = "FL = Factor Loadings, M = Mean, SD = Standard Deviation, R = Random, NR = Not Random"
)

```

Total scores were created by summing the items for each participant across all twenty PIL questions. The mean total score for nonrandomized testing was *M* = `r apa(MTnrP, 2)` (*SD* = `r apa(SDTnrP, 2)`), while the mean for randomizing testing was *M* = `r apa(MTrP, 2)` (*SD* = `r apa(SDTrP, 2)`). This difference was examined with an independent *t*-test and was not significant, `r apa_print(TPt)$statistic`. The effect size for this difference was negligible, $d_{av}$ = `r apa(TPd$d, 2)` 95% CI [`r apa(TPd$dlow, 2)`, `r apa(IPd$dhigh, 2)`]. We tested if scores were changed by 5% (`r apa(totcutoff, 2)` points), and the TOST test indicated that the lower, *t*(1897) = `r apa(TPtost$TOST_t1, 2)`, *p* < .001 and the upper bound, *t*(1897) = `r apa(TPtost$TOST_t2, 2)`, *p* < .001 were within this area of null change. The BF results also supported the null model, 0.25 <0.01%.

## LPQ Analyses

### Covariance Matrices 

```{r Lcov-analysis, include = FALSE}
###make covariance tables
not_corL = cov(finalnotrandomL, use="pairwise")
rand_corL = cov(finalrandomL, use="pairwise")

#mean tables
notrandommL = unlist(sapply(finalnotrandomL, function(cl) list(means=mean(cl,na.rm=TRUE))))
randommL = unlist(sapply(finalrandomL, function(cl) list(means=mean(cl,na.rm=TRUE))))

##rsme mean, cov, mean, cov
RMSEL = rmse.muS(notrandommL, not_corL, randommL, rand_corL)
##figure out how to do standardized residuals

##formula is cov - cov / sqrt ( var one - var other )
#do var 1 - var 1.1, then var 2 - var 2.2, then take the variance of that vector
not_vL = apply(finalnotrandomL, 2, var)
rand_vL = apply(finalrandomL, 2, var)

##figure out how to report these since you can't just divide by 2 - use csvs to figure out which and how many are over. 
##sum up the standardized residuals that are over 1.96
stdL = (not_corL - rand_corL) / sqrt(var(not_vL - rand_vL))
overL = sum(as.numeric(abs(stdL)) > 1.96)
##print out the standardized residuals to put online
write.csv(stdL, "stdres_not_randomL.csv")

##correlation
CnrL = cor(finalnotrandomL$lpq9_1, finalnotrandomL$lpq11_1)
CrL = cor(finalrandomL$lpq9_1, finalrandomL$lpq11_1)

```

Covariance structure for the LPQ was found to be the same across both randomized and nonrandomized testing, *RMSE* = `r apa(RMSEL, 2, F)`. Standardized residuals indicated that the covariance between items 9 and 11 were significantly different, while item 13 included significantly different variances. The correlation between items 9 (empty life) and 11 (wondering about being alive) for randomized versions was *r* = `r apa(CrL, 2, F)` while the correlation for nonrandomized versions was *r* = `r apa(CnrL, 2, F)`. The variance for item 13 (responsibility) in a randomized version ($SD^2$ = `r apa(rand_corL[13,13], 2, F)`) was smaller than the variance in the nonrandomized version ($SD^2$ = `r apa(not_corL[13,13], 2, F)`).

### Factor Loadings

```{r Lfactor-load, include = FALSE}
FLnotL= fa(finalnotrandomL, nfactors = 1, rotate = "oblimin", fm = "ml", cor = "tet")
FLrandomL = fa(finalrandomL, nfactors = 1, rotate = "oblimin", fm = "ml", cor = "tet") 

##dependent t-test to determine if they are p value different
FLLt = t.test(FLnotL$loadings[1:20], FLrandomL$loadings[1:20], paired = T)

##effect sizes
FLLd = d.dep.t.avg(mean(FLrandomL$loadings[1:20]),mean(FLnotL$loadings[1:20]),
                   sd(FLrandomL$loadings[1:20]), sd(FLnotL$loadings[1:20]),
                   n = length(FLrandomL$loadings[1:20]), a = .05)

FLLdz = d.dep.t.diff(mean(FLrandomL$loadings[1:20] - FLnotL$loadings[1:20]),
                     sd(FLrandomL$loadings[1:20] - FLnotL$loadings[1:20]), 
                     length(FLrandomL$loadings[1:20]), 
                     a = .05)
##toster
##using a cut off score of 0.10 difference because it matches the definition of loadings
FLLtost = TOSTpaired.raw(n = length(FLrandomL$loadings[1:20]),
                         m1 = mean(FLrandomL$loadings[1:20]),
                         m2 = mean(FLnotL$loadings[1:20]),
                         sd1 = sd(FLrandomL$loadings[1:20]),
                         sd2 = sd(FLnotL$loadings[1:20]),
                         r12 = cor(FLrandomL$loadings[1:20], FLnotL$loadings[1:20]),
                         low_eqbound = -0.10,
                         high_eqbound = 0.10,
                         alpha = .05)

##bayes factor
FLLbf = ttestBF(x = FLrandomL$loadings[1:20], y = FLnotL$loadings[1:20], paired = TRUE)
```

Table \@ref(tab:Ltable) includes the factor loadings from the one-factor EFA analysis using tetrachoric correlations. The loadings from randomized and nonrandomized versions were compared using a dependent *t*-test matched on item, which indicated they were not significantly different, `r apa_print(FLLt)$full_result`. The difference found for this test was negligible, $d_{av}$ = `r apa(FLLd$d, 2)` 95% CI [`r apa(FLLd$dlow, 2)`, `r apa(FLLd$dhigh, 2)`]. The TOST test examined if any change was within .10 change, as described earlier. The lower, *t*(19) = `r apa(FLLtost$TOST_t1, 2)`, *p* < .001 and the upper bound, *t*(19) = `r apa(FLLtost$TOST_t2, 2)`, *p* < .001 were both significant, indicating that the change was within the expected change. Further, in support of the null model, the BF was 0.34 Â±0.02%. 

### Item Means

```{r Litem, include = FALSE}
##means and sds for the table
MnrL = apply(finalnotrandomL, 2, mean)
SDnrL = apply(finalnotrandomL, 2, sd)
MrL = apply(finalrandomL, 2, mean)
SDrL = apply(finalrandomL, 2, sd)

##t-test
ILt = t.test(MnrL, MrL, paired = T)

##effect size
ILd = d.dep.t.avg(mean(MnrL), mean(MrL), sd(MnrL), sd(MrL), length(MnrL), .05)
ILdz = d.dep.t.diff(mean(MnrL - MrL), sd(MnrL - MrL), length(MnrL), .05)

##toster
##first calculate the cut off score for a certain percent change you are interested in
minscale = 0 #min scale point
maxscale = 1 #max scale point
noq = 20 #number of questions
perchange = .05 #percent change of total you think is important

cutoff = ((maxscale*noq - minscale*noq) * perchange) / noq

ILtost = TOSTpaired.raw(n = length(MnrL),
                         m1 = mean(MnrL),
                         m2 = mean(MrL),
                         sd1 = sd(MnrL),
                         sd2 = sd(MrL),
                         r12 = cor(MnrL, MrL),
                         low_eqbound = -cutoff,
                         high_eqbound = cutoff,
                         alpha = .05)

##bayes factor
ILbf = ttestBF(x = MnrL, y = MrL, paired = T)
```

Means and standard deviations of each item are presented in Table \@ref(tab:Ltable). We again matched items and tested if there was a significant change using a dependent *t*-test. The test was not significant, `r apa_print(ILt)$full_result`, and the corresponding effect size reflects how little these means changed, $d_{av}$ = `r apa(ILd$d, 2)` 95% CI [`r apa(ILd$dlow, 2)`, `r apa(ILd$dhigh, 2)`]. Using a 5% change criterion, items were tested to determine if they changed less than (`r apa(cutoff, 2)`). The TOST test indicated both lower, *t*(19) = `r apa(ILtost$TOST_t1, 2)`, *p* < .001 and the upper bound, *t*(19) = `r apa(ILtost$TOST_t2, 2)`, *p* < .001, were within the null range. The BF also supported the null model, 0.24 Â±0.02%.

### Total Scores

```{r Ltotal, include = FALSE}
##create total scores
finalnotrandomL$total = apply(finalnotrandomL, 1, sum)
finalrandomL$total = apply(finalrandomL, 1, sum)

##descriptives
MTnrL = mean(finalnotrandomL$total)
MTrL = mean(finalrandomL$total)
SDTnrL = sd(finalnotrandomL$total)
SDTrL = sd(finalrandomL$total)

##t-test
TLt = t.test(finalnotrandomL$total, finalrandomL$total,
             var.equal = T,
             paired = F)

##effect size
TLd = d.ind.t(MTnrL, MTrL, SDTnrL, SDTrL, 
              length(finalnotrandomL$total), length(finalrandomL$total), a = .05)

##toster
##change cut off to the full number of questions
totcutoff = cutoff * noq

TLtost = TOSTtwo.raw(m1 = MTnrL, m2 = MTrL, sd1 = SDTnrL, sd2 = SDTrL,
                 n1 = length(finalnotrandomL$total), n2 = length(finalrandomL$total), 
                 low_eqbound = -totcutoff, high_eqbound = totcutoff, 
                 alpha = .05, var.equal = T)


##bayes factor
TLbf = ttestBF(finalnotrandomL$total, finalrandomL$total, paired = F)
```

```{r Ltable, echo = FALSE, results = 'asis'}
##make a blank table
Ltable = matrix(NA, nrow = 20, ncol = 7)

##create column names
colnames(Ltable) = c("Item","FL-R","FL-NR","M-R","SD-R","M-NR","SD-NR")

##stick in the information you need
Ltable[ , 1] = apa(1:20, 0)
Ltable[ , 2] = apa(FLrandomL$loadings[1:20], 3, F)
Ltable[ , 3] = apa(FLnotL$loadings[1:20], 3, F)
Ltable[ , 4] = apa(MrL, 3, F)
Ltable[ , 5] = apa(SDrL, 3, F)
Ltable[ , 6] = apa(MnrL, 3, F)
Ltable[ , 7] = apa(SDnrL, 3, F)

##print it (apa)
apa_table(
  Ltable
  , align = c("l", rep("c", 6))
  , caption = "Item Statistics for the LPQ Scale",
  note = "FL = Factor Loadings, M = Mean, SD = Standard Deviation, R = Random, NR = Not Random"
)

```

LPQ total scores were created by summing the items for each participant. The mean total score for randomized testing was *M* = `r apa(MTrL, 2, F)` (*SD* = `r apa(SDTrL, 2, F)`), and the mean for nonrandomized testing was *M* = `r apa(MTnrL, 2, F)` (*SD* = `r apa(SDTnrL, 2, F)`). An independent *t*-test indicated that the testing did not change total score, `r apa_print(TLt)$statistic`. The effect size for this difference was negligible, $d_{av}$ = `r apa(TLd$d, 2)` 95% CI [`r apa(TLd$dlow, 2)`, `r apa(ILd$dhigh, 2)`]. The TOST test indicated that the scores were withing a 5% (`r apa(totcutoff, 2)` points) change, lower: *t*(1627) = `r apa(TLtost$TOST_t1, 2)`, *p* < .001 and upper: *t*(1627) = `r apa(TLtost$TOST_t2, 2)`, *p* < .001. The BF results were in support of the null model, 0.06 Â±0.04%.

# Discussion

As technology has advanced, initial research questioned the validity of online assessments versus paper assessments. With further investigation, several researchers discovered measurement invariance with regard to computer surveys compared with paper surveys [@Deutskens2006; @Lewis2009]. However, with the addition of technology, @Fang2012a suggested that individuals respond with more extreme scores in online surveys than in-person surveys due to the social-desirability effect. Research on scale invariance is mixed in results for paper and computer, and our work is a first-step on examining survey equivalence on an individual item-level for different forms of computer delivery. 

The findings from the current study imply that item randomization is a viable option for controlling any potential reactivity between questions. First, as we analyzed the PIL, the covariance matrices were non-equivalent; the randomized data show decreased variance for several items compared to the nonrandomized data. Since variance provides a measure of how the data vary around the mean, decreased variance typically results in decreased measurement error; thus, randomization has the potential to decrease measurement error in data collection. The findings also support the null hypothesis in regards to factor loading differences because the item relationship to a latent variable should not change with randomization. The item means comparison resulted in significant differences between item randomization and nonrandomization using *p*-value criterion and Bayes Factor analyses. However, the effect size was small, meaning the differences were not as meaningful as the *p*-values and *BF* analyses posit, in addition to considering the evidentiary values of the two one-sided tests, which supported the null range of expected values. Finally, the total scores showed equivalence between randomization and nonrandomization which suggested that total scales were not considerably impacted with or without randomization of items. 

Analyses for the LPQ yielded somewhat similar results to those of the PIL. Pertaining to covariance structures, the randomized and nonrandomized scales resulted in equivalence, with a recapitulation of the PIL analysis in which variance was decreased in the randomized sample for at least one item. A slight correlational difference was detected for items 9 and 11 in which the nonrandomized scale shows a large association between the items, while the randomized scale shows a moderate association between the items. However, the presence of the association remained present on both randomized and nonrandomized scales. Further analyses of the factor loadings, item means, and total scores resulted in equivalence between forms. Therefore, the null hypothesis was supported. Evidentiary equivalence for item means and total scores suggested that randomization of items was not disadvantaging the overall scoring structure of the scale and provides further support for randomization as a means of methodological control. The match between results for two types of answer methodologies (i.e. Likert and True/False) implied that randomization can be applied across a variety of scale types with similar effects. 

Since the PIL and LPQ analyses predominately illustrated support for null effects of randomization, item randomization of scales is of practical use when there are potential concerns about item order. Randomization has been largely viewed as virtuous research practice in terms of sample selection and order of stimuli presentation for years; now, we must decide if item reactivity earns the same amount of caution that has been granted to existing research procedures. Since we found equivalence in terms of overall scoring of the PIL and LPQ, we advise that randomization should and can be used as a control mechanism, in addition to the ease of comparison between the scales if one researcher decided to randomize and one did not. Moreover, these results would imply that if an individual's total score on the PIL or LPQ is significantly different on randomized versus nonrandomized administrations, it is likely due to factors unrelated to delivery. Future research should investigate if this result is WEIRD (Western, Educated, Industrialized, Rich, and Democratic), as this study focused on college-age students in the Midwest [@Henrich2010]. As @Fang2012â€™s research indicates different effects for collectivistic cultures, other cultures may show different results based on randomization. Additionally, one should consider the effects of potential computer illiteracy on online surveys [@Charters2004]. 

A second benefit to using the procedures outlined in this paper to examine for differences in methodology is the simple implementation of the analyses. While our analyses were performed in *R*, nearly all of these analyses can be performed in free point and click software, such as *jamovi* and *JASP*. Multigroup confirmatory factory analyses can additionally be used to analyze a very similar set of questions [covariance matrices, latent loadings, item means, and latent means; @Brown2006]; however, multigroup analyses require a specialized skill and knowledge set. Bayes Factor and TOST analyses are included in these free programs and are easy to implement. In this paper, we have provided examples of how to test the null hypothesis, as well as ways to include multiple forms of evidentiary value to critically judge an analysis on facets other than *p*-values [@Valentine2017]. 

\newpage

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}