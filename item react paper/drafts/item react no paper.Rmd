---
title             : "The Delivery Matters: Examining Reactivity in Question Answering"
shorttitle        : "Item Reactivity"

author: 
  - name          : "Erin M. Buchanan"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "901 S. National Ave"
    email         : "erinbuchanan@missouristate.edu"
  - name          : "Riley E. Forman"
    affiliation   : "1"
  - name          : "Jeffrey M. Pavlacic"
    affiliation   : "2"
  - name          : "Rachel Swadley"
    affiliation   : "1"
  - name          : "Becca N. Johnson"
    affiliation   : "1"
  - name          : "Stefan E. Schulenberg"
    affiliation   : "2"
  

affiliation:
  - id            : "1"
    institution   : "Missouri State University"
  - id            : "2"
    institution   : "University of Mississippi"

author_note: >
  Erin M. Buchanan is an Associate Professor of Quantitive Psychology at Missouri State University. Riley E. Forman received his undergraduate degree in Psychology and Cell and Molecular Biology at Missouri State University and is currently in medical school at XXX. Jeffrey M. Pavlacic is a doctoral candidate at The Univerity of Mississippi. Rachel Swadley completed her master's degree in Psychology at Missouri State University. Becca N. Johnson is a master's degree candidate at Missouri State University. Stefan E. Schulenberg is a Professor of Clinical Psychology at The University of Mississippi.

abstract: >
  Scales that are psychometrically sound, meaning those that meet established standards regarding reliability and validity while measuring one or more constructs of interest, are customarily evaluated based on a set modality (computer administration) and administration (fixed item order). Deviating from an established administration profile could result in non-equivalent response patterns, indicating the possible evaluation of a dissimilar construct. Furthermore, item grouping may influence response patterns. Randomizing item administration may alter or eliminate these effects. Therefore, we examined the differences in scale relationships for randomized and nonrandomized computer delivery for two scales measuring meaning/purpose in life. These scales have questions about suicidality, depression, and life goals that may cause reactivity (i.e. a changed response to a second item based on the answer to the first item).  

  
keywords          : "scales, reactivity, item answering"

bibliography      : ["item react.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---
The use of the Internet has become integrated into daily life as a means of accessing information, interacting with others, and tending to required tasks. The International Telecommunication Union reports that over half the world is online, and 70% of 15-24 year olds are on the internet (ICT report). Further, the Nielson Total Audience report from 2016 indicates that Americans spend nearly 11 hours a day in media consumption (Nielson report). Researchers discovered that online data collection can be advantageous over laboratory and paper data collection, as it is often cheaper and more efficient (Illieva, Baron, and Healey 2002; Schuldt and Totten 1994, Reips chapter). Internet questionnaires first appeared in the early 90s when HTML scripting code integrated form elements, and the first experiments appeared soon after (Musch & Reips, 2000, Reips 1997). The first experimental lab on the internet was the Web Experimental Psychology Lab formed by Reips (http://www.wexlab.eu), and the use of the Internet to collect data has since grown rapidly. What started with email and HTML forms has since moved to whole communities of available participants including websites like Amazon’s Mechanical Turk, and Qualtric’s Participant Panels. Participants of all types and forms are easily accessible for somewhat little to no cost. 

Our ability to collect data on the Internet has inevitably lead to the question of measurement invariance between in person and online data collection methods (Meyerson & Tryon, 2003; Buchanan et al., 2005). Invariance implies that different forms, data collection procedures, or even target demographics produce comparable sets of responses, which is a desirable characteristic to ensure a minimal number of confounding variables (Brown CFA book). According to Deutskens, Ruyter, and Wetzels (2006), mail surveys and online surveys produce nearly identical results with regards to response characteristics. The authors suggest the number and length of responses were also similar, as well as the accuracy of the data collected online versus by mail. Only minor differences arise between online surveys and mail in surveys when it comes to participant honesty and suggestions. For example, participants who responded to surveys online provided more suggestions, lengthier answers, and greater information about competitors in the field that they may prefer (Deutskens, Ruyter, & Wetzels, 2006). The hypothesis as to why individuals may be more honest online than in person is that the individual may feel more anonymity and less social desirability effects due to the nature the online world, therefore less concerned about responding in a socially polite way (Joinson, 1999). A trend found by Fang, Wen, and Prybutok (2012a - social desirability paper) shows individuals are more likely to respond to surveys online with extreme scores, rather than mid-range responses on Likert scales due to the lessened social desirability factor.  There may be slight cultural differences in responses online.  For example, collectivistic cultures showed greater tendency toward mid-range responses on Likert scales via in-person and online due to placing greater value on how they are socially perceived; however, the trend is still the same as scores are more extreme online versus in person or by mail (Fang, Wen, & Prybutok, 2012b collectivist paper). 

Although work by Dillman and his group (Dillman & Bowker, 2001; Dillman, Smyth, & Christian, 2008; Smyth, Dillman, Christian, & Stern, 2006), among others, has shown that many web surveys are plagued by problems of usability, display, coverage, sampling, nonresponse, or technology, other studies have found internet data to be reliable and almost preferable as it produces a varied demographic response compared to the traditional sample of introduction to psychology college students while also maintaining data equivalence (Lewis, Watson, & White, 2009). However, equivalence in factor structure may be problematic, as Buchanan et al. (2005) have shown that factor structure was not replicable in online and in person surveys. Other work has shown equivalence using a comparison of correlation matrices (Meyerson & Tryon, 2003) or *t*-tests (Schulenberg & Yutrzenka, 1999 and 2001), and the literature is mixed on how different methodologies impact factor structure. Weigold et al. (2013) recently examined both quantitative and research design questions (i.e. missing data) about Internet and paper-and-pencil administration and showed that the administrations were generally equivalent for quantitative structure but research design issues showed nonequivalence. Other potential limitations to online surveys include the accessibility of different populations to the Internet (Dillman & Bowker, 2001), selection bias (Bethlehem, 2010), response rates (Cook, Heath, & Thompson, 2000; de Leeuw & Hox; Cantrell & Lupinacci, 2007), attrition (Cronk & West, 2002), and distraction (Tourangeau, Rips, & Rasinki, 2000). Many of these concerns have been alleviated in the years since online surveys were first developed, especially with the advent of panels and Mechanical Turk to reach a large, diverse population of participants (Burmeister, AMT references). 

With the development of advanced online survey platforms such as Qualtrics and Survey Monkey, researchers have the potential to control potentially confounding research design issues through randomization (although other issues may still be present, such as participant misbehavior, Nosek & Banaji, 2002). Randomization has been a hallmark of good research practice, as the order or presentation of stimuli can be a noise variable in a study with multiple measures (Keppel/Wickens book). Thus, researchers have often randomized scales by rotating the order of presentation in paper format or simply clicking the randomization button for web-based studies. This practice has counterbalanced out any order effects of going from one scale to the next. However, while scale structure has remained constant, these items are still stimuli within a larger construct. Therefore, these construct-related items have the ability to influence the items that later on the survey, which we will call item reactivity. For example, a question about being *prepared for death* or *thoughts about suicide* might change the responses to further questions, especially if previous questions did not alert participants to be prepare for that subject matter. 

Scale development typically starts with an underlying latent variable that a researcher wishes to examine through measured items or questions (DeVellis, 2016 book). Question design is a well-studied area that indicates that measurement is best achieved through questions that are direct, positively worded, and understandable to the subject (dillman book). Olson (2010) suggests that researchers design a multitude of items to investigate and invite subject matter experts to examine these questions. Subject matter experts were found to be variable in their agreement, but excellent at identifying potentially problematic questions. After suggested edits from these experts, a large sample of participant data is collected. While item response theory is gaining traction, classical test theory has dominated this area through the use of exploratory and confirmatory factor analysis (EFA, CFA; Worthington, 2006).  EFA elucidates several facets of how the measured items represent the latent trait through factor loadings and overall model fit (Tabachnick). Factor loadings represent the correlation between each item and the overall latent variable, where a researcher wishes to find items that are strongly related to the latent trait. Items that are not related to the latent trait, usually with factor loadings below .300 (Preacher) are discarded. Model fit is examined when simple structure has been achieved (i.e. appropriate factor loadings for each item), and these fit indices inform if the items and factor structure model fit the data well. Well-designed scales include items that are highly related to their latent trait and have excellent fit indices. Scale development additionally includes the examination of other measures of reliability (alpha) and construct validity (relation to other phenomena) but the focus of the scale shifts to subscale or total scores (Buchanan chapter). Published scales are then distributed for use in the form that is published, as item order is often emphasized through important notes about reverse scoring and creating subscale scores. 

The question is no longer whether web-based surveys are reliable sources of data collection; the theory now is in need of a shift to whether or not item-randomization in survey data collection creates psychometric differences. These scale development procedures focus on items, and EFA/CFA statistically try to mimic variance-covariance structure by creating models of the data with the same variance-covariance matrix. If we imagine that stimuli in a classic experimental design can influence the outcome of a study because of their order, then certainly the stimuli on a scale (i.e., the items) can influence the pattern of responses for items. This area of study is relatively unexplored, as easy randomization has only recently become available for researchers. 

Therefore, this study focuses on potential differences in results based on item randomization delivery methodology. The current project examined large samples on two logotheraphy related scales, as these scales include potentially reactive items, as well as both a dichotomous True/False and traditional Likert format for the same items. Large samples were desirable to converge on a stable, representative population; however, false positives (i.e., Type I errors) can occur by using large *N*. Recent developments in the literature focusing on null hypothesis testing make it especially important to present potential alternatives to *p*-values (cite KD paper). While a large set of researchers have argued that the literature is full of Type I errors (Bejamin), and thus, the $\alpha$ value should be shifted lower (i.e., *p* < .005 for statistical significance), an equally large set of researchers counter this argument as unfounded and weak (Lakens). We provide multiple sources of evidence (*p*-values, effect sizes, Bayes Factors, and tests of equivalence) to determine if differences found are not only statistically significant, but also practically significant. In our study, we expand to item randomization for online based surveys, examining the impact on item loadings to their latent variable, variance-covariance structure, item means, and total scores again providing evidence of difference/non-difference from multiple statistical sources. Finally, we examine these scenarios with a unique set of scales that have both dichotomous True/False and traditional Likert formats to explore how the answer response options might impact any differences found between randomized and nonrandomized methodologies. 

```{r libraries, include = FALSE}
library(papaja)
library(car)
library(memisc)
library(moments)
library(mice)
library(monomvn)
library(psych)
library(TOSTER)
library(MOTE)
library(BayesFactor)
```

# Method

## Participants
The sample population consisted of undergraduate students at a large Midwestern University, placing the approximate age of participants at around 18-22. Table \@ref(tab:demo-table) includes the demographic information about all datasets. Only two scales were used from each dataset, as described below. Participants were generally enrolled in an introductory psychology course that served as a general education requirement for the university. As part of the curriculum, the students were encouraged to participate in psychology research programs, resulting in their involvement in this study. These participants were given course credit for their participation. 

## Materials
Of the surveys included within each larger study, two questionnaires were utilized: the Purpose in Life Questionnaire (PIL; Crumbaugh & Maholick, 1964, 1969) and the Life Purpose Questionnaire (LPQ; Hablas & Hutzell, 1982; Hutzell, 1989).

### The Purpose in Life Questionnaire
The PIL is a 20-item questionnaire that assesses perceived meaning and life purpose. Items are structured in a 7-point Likert type response format; however, each item has different anchoring points that focus on item content. Total scores are created by summing the items, resulting in a range of 20 to 140 for the overall score. The reliability for the scale is generally high, ranging from .70 to .90 (Schulenberg, 2004; Schulenberg & Melton, 2010). Previous work on validity for the PIL showed viable one- and two-factor models, albeit question loadings varied across publications (see Schulenberg & Melton, 2010 for a summary), and these fluctuating results lead to the development of a 4-item PIL short form (Schulenberg, Schnetzer, & Buchanan, 2012). 

### Life Purpose Questionnaire
The LPQ was modeled after the full 20-item PIL questionnaire, also measuring perceived meaning and purpose in life. The items are structured in a true/false response format, in contrast to the Likert response format found on the PIL. Each question is matched to the PIL with the same item content, altering the question to create binary answer format. After reverse coding, zero on an item would indicate low meaning, while one on an item would indicate high meaning. A total score is created by summing questions, resulting in a range from 0 to 20. In both scales, higher scores indicated greater perceived meaning in life. Reliability for this scale is also correspondingly high, usually in the .80 range (Melton & Schulenberg, 2008; Schulenberg, 2004).

These two scales were selected because they contained the same item content with differing response formats, which would allow for cross comparisons between results for each scale. 

## Procedure
The form of administration was of interest to this study, and therefore, two formats were included: computerized administration in nonrandom order and computerized administration with a randomized question order. Computerized questionnaires were available for participants to access electronically, and they were allowed to complete the experiment from anywhere with the Internet through Qualtrics. To ensure participants were properly informed, both an introduction and a debriefing were included within the online form. Participants were randomly assigned to complete a nonrandomized or randomized version of the survey. Nonrandomized questionnaires followed the original scale question order, consistent with paper delivery format. A different group of participants were given each question in a randomized order within each scale (i.e. all PIL and LPQ questions will still grouped together on one page). Scales were randomized across participants for both groups. Once collected, the results were then amalgamated into a database for statistical analysis.

# Results

## Hypothesis and Data-Analytic Plan
Computer forms were analyzed by randomized and nonrandomized groups to examine the impact of randomization on covariance structure, factor loadings, item means, and total scores. We expected to find that these forms may potentially vary across covariance structure and item means, which would indicate differences in reactivity to questions (i.e. item 4 always has item 3 as a precursor on a nonrandom form, while item 4 may have a different set of answers when prefaced with other questions). Factor loadings were assesed to determine if differences in randomization caused a change in focus, such that participant interpretation of the item changed the relationship to the latent variable. However, we did not predict if values would change, as latent trait measurement should be consistent. Last, we examined total scores; however, it was unclear if these values would change. A difference in item means may result in changes in total scores, but may also result in no change if some item means decrease, while others increase. 

Each hypothesis was therefore tested using four dependent measures. First, we examined the variance-covariance matrix for each type of delivery and compared to each other by using root mean squared error (RMSE; CITE). RMSE estimates the difference between covariance matrices and is often used in structural equation modeling to determine if models have good fit to the data. A criterion of < .06 for good fit, .06-.08 for acceptable fit, and > .10 for bad fit was used (Hu & Bentler, 1999?). This analysis was used to determine if the change in delivery changed the structure of the item relationships to each other (i.e. if the correlation matrices are different). RMSE values were calculated using the monomvn package in R (Gramacy, CITE).

We then calculated an exploratory factor analysis on both scales using one-factor models to examine the loading of each item on its latent trait. The PIL factor structure is contested (Schulenberg Strack paper) with many suggestions as to latent structure for one- and two-factor models. The LPQ has seen less research on factor structure (something here about that?). This paper focused on loadings on one global latent trait to determine if the manipulation of delivery impacted factor loadings. We used a one-factor model and included all questions to focus on the loadings, rather than the factor structure. The analysis was performed using the *psych* package in *R* with maximum likelihood estimation and an oblique (oblimin) rotation. The LPQ factor analysis used tetrachoric correlation structure to control for the dichotomous format of the scale, rather than traditional Pearson correlation structure. The loadings were then compared using a matched dependent *t*-test (i.e. item 1 to item 1, item 2 to item 2) to examine differences between nonrandomized and randomized computer samples. 

Next, item averages were calculated across all participants for each item. These 20 items were then compared in a matched dependent *t*-test to determine if delivery changed the mean of the item on the PIL or LPQ. While covariance structure elucidates the varying relations between items, we may still find that item averages are pushed one direction or another by a change in delivery and still maintain the same correlation between items. If this test was significant, we examined the individual items across participants for large effect sizes, as the large sample sizes in this study would create significant *t*-test follow ups. 

Last, the total scores for each participant were compared across delivery type using an independent *t*-test. Item analyses allow a focus on specific items that may show changes, while total scores allow us to investigate if changes in delivery alter the overall score that is used in other analyses. For analyses involving *t*-tests, we provide multiple measures of evidentiary value so that researchers can weigh the effects of randomization on their own criterion. Recent research on $\alpha$ criterions has shown wide disagreement on the usefulness of *p*-values and set cut-off scores (Benjamin paper, Lakens paper, other stuff about how p is useless). Therefore, we sought to provide traditional null hypothesis testing results (*t*-tests, *p*-values) and supplement these values with effect sizes (*d* and non-central confidence intervals; CITEs), Bayes Factors (CITE), and one-sided tests of equivalence (TOST; Cribbie, 2004; LakensTOST; Rogers et al, 1993). We used the average standard deviation of each group as the denominator for *d* calculation as follows:

$$
d_{av} = \frac {(M_1 - M_2) } { \frac{SD_1 + SD_2 } {2} }
$$
This effect size is less biased than the traditional $d_z$ formula, wherein mean differences are divided by the standard deviation of the difference scores (Lakens, 2013). The difference scores standard deviation is often much smaller than the average of the standard deviations of each level, which can create an upwardly biased effect size. This bias can lead researchers to interpret larger effects for a psychological phenomenon than actually exist.  

Bayes Factors are calculated in opposition to a normal frequentist (NHST) approach, as a ratio of the likelihood of two models. Traditional NHST focuses on the likelihood of the data, given the null hypothesis is true, and Bayesian analysis instead posits the likelihood of a hypothesis given the data. Prior distributions are our estimation of the likelihood of our hypothesis before the data was collected, which is combined with the data collected to form a posterior belief of our hypothesis. We chose to use Bayes Factors as a middle ground to the Bayesian analysis continuum, that uses mildly uninformative priors and the data strongly impacts the posterior distribution. The choice of prior distribution can heavily influence the posterior belief, in that uninformative priors allow the data to comprise the posterior distribution. However, most researchers have a background understanding of their field, thus, making completely uninformative priors a tenuous assumption. Because of the dearth of literature in this field, there is not enough previous information to create a strong prior distribution, which would suppress the effect of the data on posterior belief. The Bayes Factor package (CITE) uses recommended default priors that cover a wide range of data (Rouder et al. 2012, Rouder et al., 2009) of a Jeffreys prior with a fixed rscale (0.5) and random rscale (1.0). The alternative model is generally considered a model wherein means between groups or items differ, and this model is compared to a null model of no mean differences. The resulting ratio is therefore the odds of the alternative model to the null, where BF values less than one indicate evidence for the null, values at one indicate even evidence for the null and alternative, and values larger than one indicate evidence for the alternative model. While some researchers have posed labels for BF values (Kass and Raftery, 95), we present these values as a continuum to allow researchers to make their decisions (Mourey blog). 

NHST has also been criticized for an inability to test the null hypothesis, and thus show evidence of the absence of an effect. Non-significant *p*-values are often misinterpreted as evidence for the null hypothesis (CITE). However, we can use the traditional frequentist approach to determine if an effect is within a set of equivalence bounds. We used the two one-sided tests approach (TOST, Schuirmann, 1987) to specify a range of raw-score equivalence that would be considered supportive of the null hypothesis (i.e. no worthwhile effects or differences). TOST are then used to determine if the values found are outside of the equivalence range. Significant TOST values indicate that the effects are *within* the range of equivalence. We used the TOSTER package (Lakens, cite) to calculate these values, and graphics created from this package can be found online at OSFLINK. 

The equivalence ranges are often tested by computing an expected effect size of neglible range; however, the TOST for dependent *t* uses $d_z$, which can overestimate the effect size of a phenomena (cumming, 14; lakens 13). Therefore, we calculated TOST tests on raw score differences to allievate the overestimation issues. For EFA, we used a change score of .10 in the loadings, as CITE PEOPLE suggested loading estimation ranges, such as STUFF, and the differences in these ranges are approximately .10. Additionally, this score would amount to a small correlation change using traditional guidelines for interpretation of *r* (Cohen, 92). For item and total score differences, we chose a 5% change in magnitude as the raw score cut off. To calculate that change for total scores, we used the following formula:

$$
(Max*N_{Questions} - Min*N_{Questions}) * Change
$$
Minimum and maximum values indicate the lower and upper end of the answer choices (i.e. 1 and 7), and change represented the proportion magnitude change expected. Therefore, for total PIL scores, we proposed a change in 6 points to be signficant, while LPQ scores would change 1 point to be a significant change. For item analyses, we divided the total score change by the number of items to determine how much each item should change to impact the total score a significant amount (PIL = 0.30, LPQ = .05).

## Data Screening 
Each dataset was analyzed separately by splitting on scale and randomization, and first, all data were screened for accuracy and missing data. Participants with more than 5% missing data (i.e. 2 or more items) were excluded. Data were imputed using the mice package in R for participants with less than 5% of missing data (CITE). Next, each dataset was examined for multivariate outliers using Mahalanobis distance (Tabachick & Fidell, 2012). Each dataset was then screened for multivariate assumptions of additivity, linearity, normality, homogeneity, and homoscedasticity. While some data skew was present, large sample sizes allowed for the assumption of normality of the sampling distribution. Information about the number of excluded data points in each step is presented in Table \@ref(tab:demo-table).

```{r PIL-Data-Screening, include = FALSE}
####import the files####
PIL = read.csv("PIL.csv")

##reverse coding the PIL
PIL[ , c("PIL2", "PIL5", "PIL7", "PIL10", "PIL14", "PIL15", "PIL17", "PIL18", "PIL19")] = 8 - PIL[ , c("PIL2", "PIL5","PIL7", "PIL10", "PIL14", "PIL15", "PIL17", "PIL18", "PIL19")]

##subsetting the PIL dataset
nr = subset(PIL, Source == "not random")
r = subset(PIL, Source == "random")

######Random data screening#####
randomcomputer = r[, c(3:22)]
summary(randomcomputer)
apply(randomcomputer, 2, table)

##missing data
##participants (row)
percentmiss = function(x) { sum(is.na(x))/length(x)*100}
missingrandomcomputer = apply(randomcomputer, 1, percentmiss)
table(missingrandomcomputer)
replacepeoplerandom = subset(randomcomputer, missingrandomcomputer <= 5)
summary(replacepeoplerandom)

##variables (column)
apply(replacepeoplerandom, 2, percentmiss)
replacecolumnrandom = replacepeoplerandom[ , -15]
dontcolumnrrandom = replacepeoplerandom[ , 15]

##replace data
tempnomissrandom = mice(replacecolumnrandom)
replacedrandom_temp = complete(tempnomissrandom, 1)

replacedrandom = cbind(replacedrandom_temp[ , 1:14], 
                       PIL15 = dontcolumnrrandom, 
                       replacedrandom_temp[ , 15:19])
 summary(replacedrandom)

##outliers
mahalrandom = mahalanobis(replacedrandom[ , ],
                    colMeans(replacedrandom[ , ], na.rm = T),
                    cov(replacedrandom[ , ], use = "pairwise.complete.obs"))

cutoff = qchisq(1-.001, ncol(replacedrandom[ , ]))
ncol(replacedrandom[ , ])
cutoff
summary(mahalrandom < cutoff)
nooutrandom = subset(replacedrandom, mahalrandom < cutoff)

##assumptions
##additivity
correlrandom = cor(nooutrandom[ , ])
symnum(correlrandom)

finalrandomP = nooutrandom

##set up for assumptions
randomrandom = rchisq(nrow(finalrandomP), 7)
fakerandom = lm(randomrandom ~ ., data = finalrandomP)
standardizedrandom = rstudent(fakerandom)
fittedrandom = scale(fakerandom$fitted.values)

##normality
skewness(finalrandomP[ , ], na.rm = T)
kurtosis(finalrandomP[ , ], na.rm = T)
#hist(standardizedrandom)

##linearity
#qqnorm(standardizedrandom)
#abline(0,1)

##homog + s
#plot(fittedrandom, standardizedrandom)
#abline(0,0)
#abline(v = 0)

######NOTRandom data screening#####
notrandomcomputer = nr[, c(3:22)]
summary(notrandomcomputer)
apply(notrandomcomputer, 2, table)

##missing data
##participants (row)
missingnotrandomcomputer = apply(notrandomcomputer, 1, percentmiss)
table(missingnotrandomcomputer)
replacepeoplenotrandom = subset(notrandomcomputer, missingnotrandomcomputer <= 5)
summary(replacepeoplenotrandom)

##variables (columns)
apply(replacepeoplenotrandom, 2, percentmiss)

##replace the data
tempnomissnotrandom = mice(replacepeoplenotrandom)
replacednotrandom = complete(tempnomissnotrandom, 1)
summary(replacednotrandom)

##outliers
mahalnotrandom = mahalanobis(replacednotrandom[ , ],
                          colMeans(replacednotrandom[ , ], na.rm = T),
                          cov(replacednotrandom[ , ], use = "pairwise.complete.obs"))

cutoff = qchisq(1-.001, ncol(replacednotrandom[ , ]))
ncol(replacednotrandom[ , ])
cutoff
summary(mahalnotrandom < cutoff)
nooutnotrandom = subset(replacednotrandom, mahalnotrandom < cutoff)

##assumptions
##additivity
correlnotrandom = cor(nooutnotrandom[ , ])
symnum(correlnotrandom)

finalnotrandomP = nooutnotrandom

##set up for assumptions
randomnotrandom = rchisq(nrow(finalnotrandomP), 7)
fakenotrandom = lm(randomnotrandom ~ ., data = finalnotrandomP)
standardizednotrandom = rstudent(fakenotrandom)
fittednotrandom = scale(fakenotrandom$fitted.values)

##normality
skewness(finalnotrandomP[ , ], na.rm = T)
kurtosis(finalnotrandomP[ , ], na.rm = T)
#hist(standardizednotrandom)

##linearity
#qqnorm(standardizednotrandom)
#abline(0,1)

##homog + s
#plot(fittednotrandom, standardizednotrandom)
#abline(0,0)
#abline(v = 0)
```

```{r LPQ-Data-Screening, include=FALSE}
##import the data files
LPQallcompiled = read.csv("LPQ.csv")

###changing to 0 and 1 - columns are currently 1 true, 2 false, so subtract 2
names(LPQallcompiled)
summary(LPQallcompiled)
##fix the 12 typo
LPQallcompiled$lpq16_1[ LPQallcompiled$lpq16_1 == 12] = NA

allcolumns = c("lpq1_1", "lpq2_1", "lpq3_1", "lpq4_1", "lpq5_1", "lpq6_1", "lpq7_1", "lpq8_1", "lpq9_1", "lpq10_1", "lpq11_1", "lpq12_1", "lpq13_1", "lpq14_1", "lpq15_1", "lpq16_1", "lpq17_1", "lpq18_1", "lpq19_1", "lpq20_1")
LPQallcompiled[ , allcolumns] = 2 - LPQallcompiled[ , allcolumns]

###reverse code items 1, 2, 5, 8, 9, 11, 12, 14, 15, 16, 19
reverse = c("lpq1_1", "lpq2_1", "lpq5_1", "lpq8_1", "lpq9_1", "lpq11_1", "lpq12_1", "lpq14_1", "lpq15_1", "lpq16_1", "lpq19_1")
LPQallcompiled[ , reverse] = 1 - LPQallcompiled[ , reverse]
summary(LPQallcompiled)

##make sure this dataaset doesn't have any decimals or weird numbers
apply(LPQallcompiled[ , allcolumns], 2, table)

##the csv files are a mix of computer and paper, so need to subset
LPQallcompiledrandom = subset(LPQallcompiled, Source == 'random')
LPQallcompilednotrandom = subset(LPQallcompiled, Source == 'not random')

####Random data screening####
LPQallcompiledrandom = LPQallcompiledrandom[ , 3:22]
summary(LPQallcompiledrandom)

##missing lpqallcompiled random 
missingallcompiledrandom = apply(LPQallcompiledrandom, 1, percentmiss)
table(missingallcompiledrandom)
replacepeopleallcompiledrandom = subset(LPQallcompiledrandom, missingallcompiledrandom <=5)

##check for columns
apply(replacepeopleallcompiledrandom, 2, percentmiss)

##replace data
tempnomiss = mice(replacepeopleallcompiledrandom)
replacedallcompiledrandom = complete(tempnomiss, 1)
summary(replacedallcompiledrandom)

##outliers
mahal = mahalanobis(replacedallcompiledrandom, 
                    colMeans(replacedallcompiledrandom, na.rm = T), 
                    cov(replacedallcompiledrandom, use = "pairwise.complete.obs"))
cutoff = qchisq(1-.001, ncol(replacedallcompiledrandom))
cutoff
ncol(replacedallcompiledrandom)
summary(mahal < cutoff)
nooutreplacedallcompiledrandom = subset(replacedallcompiledrandom, mahal < cutoff)

##assumptions 
##additivity
correlreplacedallcompiledrandom = cor(nooutreplacedallcompiledrandom)
symnum(correlreplacedallcompiledrandom) ##good

##set up for assumptions 
random = rchisq(nrow(nooutreplacedallcompiledrandom), 7)
fake = lm(random ~ ., data = nooutreplacedallcompiledrandom)
standardized = rstudent(fake)
fitted = scale(fake$fitted.values)

##normality 
#hist(standardized) ##positive skew

##linearity 
#qqnorm(standardized)
#abline(0, 1) ##looks good

##homog and s
#plot(fitted, standardized)
#abline(0, 0)
#abline(v = 0)##yes!

finalrandomL = nooutreplacedallcompiledrandom

####Not random data screening####
LPQallcompilednotrandom = LPQallcompilednotrandom[ , 3:22]
summary(LPQallcompilednotrandom)

##missing lpqallcompilednotrandom
missingallcompilednotrandom = apply(LPQallcompilednotrandom, 1, percentmiss)
table(missingallcompilednotrandom)
replacepeopleallcompilednotrandom = subset(LPQallcompilednotrandom, missingallcompilednotrandom <= 5)

##columns
apply(replacepeopleallcompilednotrandom,2,percentmiss)

##replace data
tempnomiss = mice(replacepeopleallcompilednotrandom)
replacedallcompilednotrandom = complete(tempnomiss, 1)
summary(replacedallcompilednotrandom)

##outliers
mahal = mahalanobis(replacedallcompilednotrandom, 
                    colMeans(replacedallcompilednotrandom, na.rm = T), 
                    cov(replacedallcompilednotrandom, use = "pairwise.complete.obs"))
cutoff = qchisq(1-.001, ncol(replacedallcompilednotrandom))
cutoff
ncol(replacedallcompilednotrandom)
summary(mahal < cutoff)
nooutreplacedallcompilednotrandom = subset(replacedallcompilednotrandom, mahal < cutoff)

##assumptions 
##additivity
correlreplacedallcompilednotrandom = cor(nooutreplacedallcompilednotrandom)
symnum(correlreplacedallcompilednotrandom) ##yay

##set up for assumptions 
random = rchisq(nrow(nooutreplacedallcompilednotrandom), 7)
fake = lm(random ~ ., data = nooutreplacedallcompilednotrandom)
standardized = rstudent(fake)
fitted = scale(fake$fitted.values)

##normality 
#hist(standardized) ##positive skew still

##linearity 
#qqnorm(standardized)
#abline(0, 1) ##its okay 

##homog and s
#plot(fitted, standardized)
#abline(0, 0)
#abline(v = 0)##meets both

finalnotrandomL = nooutreplacedallcompilednotrandom
```

```{r demo-table, results = 'asis', echo = FALSE}
demo = read.csv("demographics.csv")
gender = tapply(demo$Gender, demo$Source, table)
ethnic = tapply(demo$Ethnicity, demo$Source, table)
Mage = tapply(demo$Age, demo$Source, mean, na.rm = T)
SDage = tapply(demo$Age, demo$Source, sd, na.rm = T)
##make a blank table
demotable = matrix(NA, nrow = 4, ncol = 7)

##create column names
colnames(demotable) = c("Group", "Female", "White", "Age (SD)", "Original N", "Missing N", "Outlier N")

##stick in the information you need
demotable[ , 1] = c("PIL Random", "PIL Not Random", "LPQ Random", "LPQ Not Random")
demotable[ , 2] = c(apa(gender$random["female"]/sum(gender$random)*100,1), 
                    apa(gender$`not random`["female"] / sum(gender$`not random`)*100, 1),
                    "-", "-")
demotable[ , 3] = c(apa(ethnic$random["White"] / sum(ethnic$random)*100, 1), 
                    apa(ethnic$`not random`["White"] / sum(ethnic$`not random`)*100, 1),
                    "-", "-")
demotable[ , 4] = c(paste( apa(Mage["random"],2), " (", apa(SDage["random"],2), ")", sep = ""),
                    paste( apa(Mage["not random"],2), " (", apa(SDage["not random"], 2), ")", sep = ""),
                    "-", "-")
demotable[ , 5] = c(nrow(randomcomputer), nrow(notrandomcomputer), 
                    nrow(LPQallcompiledrandom), nrow(LPQallcompilednotrandom))
demotable[ , 6] = as.numeric(demotable[ , 5]) - 
                  c(nrow(replacedrandom) , nrow(replacednotrandom),
                    nrow(replacedallcompiledrandom), nrow(replacedallcompilednotrandom))
demotable[ , 7] = as.numeric(demotable[ , 5]) - as.numeric(demotable[ , 6]) - 
                  c(nrow(finalrandomP), nrow(finalnotrandomP),
                    nrow(finalrandomL), nrow(finalnotrandomL))
##note for PIL random 256 of those are the bad data with one question, so added that to the missing column. 
demotable[1, 6] = as.numeric(demotable[1, 6]) + 256
demotable[1, 7] = as.numeric(demotable[1, 7]) - 256


##print it (apa)
apa_table(
  demotable
  , align = c("l", rep("c", 6))
  , caption = "Demographic and Data Screening Information",
  note = "Participants took both the PIL and LPQ scale, therefore, random and not random demographics are the same. Not every participant was given the LPQ, resulting in missing data for those subjects. Several PIL participants were removed because they were missing an item on their scale."
)
```

## PIL Analyses

### Covariance Matrices

```{r Pcov-analysis, include=FALSE}
###make covariance tables
not_corP = cov(finalnotrandomP, use="pairwise")
rand_corP = cov(finalrandomP, use="pairwise")

#mean tables
notrandommP = unlist(sapply(finalnotrandomP, function(cl) list(means=mean(cl,na.rm=TRUE))))
randommP = unlist(sapply(finalrandomP, function(cl) list(means=mean(cl,na.rm=TRUE))))

##rsme mean, cov, mean, cov
RMSEP = rmse.muS(notrandommP, not_corP, randommP, rand_corP)
##figure out how to do standardized residuals

##formula is cov - cov / sqrt ( var one - var other )
#do var 1 - var 1.1, then var 2 - var 2.2, then take the variance of that vector
not_vP = apply(finalnotrandomP, 2, var)
rand_vP = apply(finalrandomP, 2, var)

##figure out how to report these since you can't just divide by 2 - use csvs to figure out which and how many are over. 
##sum up the standardized residuals that are over 1.96
stdP = (not_corP - rand_corP) / sqrt(var(not_vP - rand_vP))
overP = sum(as.numeric(abs(stdP)) > 1.96)
##print out the standardized residuals to put online
write.csv(stdP, "stdres_not_randomP.csv")

##pil 7 rand_corP[7,7]

```

Covariance structure was considered different (i.e. above .10) for the randomized and not randomized forms of item order, *RMSE* = `r apa(RMSEP, 2, F)`. Standardized residuals were calculated by dividing the difference in covariance tables by the variance of the differences (CITE). While *RMSE* indicated partial misfit between the covariance relationships, only `r overP` values were significantly different using *Z* of 1.96 as a criterion: the variances of PIL 7 and 14. PIL 7 in a randomized form had less variance ($SD^2$ = `r apa(rand_corP[7,7], 2)`) than the non-randomized form ($SD^2$ = `r apa(not_corP[7,7], 2)`). Likewise, PIL 14 randomized had a smaller variance ($SD^2$ = `r apa(rand_corP[14,14], 2)`) than the non-randomized form ($SD^2$ = `r apa(not_corP[14,14], 2)`). Questions about retirement and freedom to make choices decreased in variance when they were randomly presented. 

### Factor Loadings

```{r Pfactor-load, include = FALSE}
FLnotP= fa(finalnotrandomP, nfactors = 1, rotate = "oblimin", fm = "ml")
FLrandomP = fa(finalrandomP, nfactors = 1, rotate = "oblimin", fm = "ml") 

##dependent t-test to determine if they are p value different
FLPt = t.test(FLnotP$loadings[1:20], FLrandomP$loadings[1:20], paired = T)

##effect sizes
FLPd = d.dep.t.avg(mean(FLrandomP$loadings[1:20]),mean(FLnotP$loadings[1:20]),
                   sd(FLrandomP$loadings[1:20]), sd(FLnotP$loadings[1:20]),
                   n = length(FLrandomP$loadings[1:20]), a = .05)

FLPdz = d.dep.t.diff(mean(FLrandomP$loadings[1:20] - FLnotP$loadings[1:20]),
                     sd(FLrandomP$loadings[1:20] - FLnotP$loadings[1:20]), 
                     length(FLrandomP$loadings[1:20]), 
                     a = .05)
##toster
##using a cut off score of 0.10 difference because it matches the definition of loadings
FLPtost = TOSTpaired.raw(n = length(FLrandomP$loadings[1:20]),
                         m1 = mean(FLrandomP$loadings[1:20]),
                         m2 = mean(FLnotP$loadings[1:20]),
                         sd1 = sd(FLrandomP$loadings[1:20]),
                         sd2 = sd(FLnotP$loadings[1:20]),
                         r12 = cor(FLrandomP$loadings[1:20], FLnotP$loadings[1:20]),
                         low_eqbound = -0.10,
                         high_eqbound = 0.10,
                         alpha = .05)

##bayes factor
FLPbf = ttestBF(x = FLrandomP$loadings[1:20], y = FLnotP$loadings[1:20], paired = TRUE)
##note I hard coded the p values for TOST and BF, need to change if we change things. 
```

Table \@ref(tab:Ptable) includes the factor loadings from the one-factor EFA analysis. These loadings were compared using a dependent *t*-test matched on item, and they were not significantly different, `r apa_print(FLPt)$full_result`. The effect size for this test was correspondingly negligible, $d_{av}$ = `r apa(FLPd$d, 2)` 95% CI [`r apa(FLPd$dlow, 2)`, `r apa(FLPd$dhigh, 2)`]. The TOST test was significant for both the lower, *t*(19) = `r apa(FLPtost$TOST_t1, 2)`, *p* < .001 and the upper bound, *t*(19) = `r apa(FLPtost$TOST_t2, 2)`, *p* < .001. This result indicated that the change score was within the confidence band of expected neglible changes. Lastly, the BF for this test was 0.24 ±0.02%, which indicated support for the null model. 

### Item Means

```{r Pitem, include = FALSE}
##means and sds for the table
MnrP = apply(finalnotrandomP, 2, mean)
SDnrP = apply(finalnotrandomP, 2, sd)
MrP = apply(finalrandomP, 2, mean)
SDrP = apply(finalrandomP, 2, sd)

##t-test
IPt = t.test(MnrP, MrP, paired = T)

##effect size
IPd = d.dep.t.avg(mean(MnrP), mean(MrP), sd(MnrP), sd(MrP), length(MnrP), .05)
IPdz = d.dep.t.diff(mean(MnrP - MrP), sd(MnrP - MrP), length(MnrP), .05)

##toster
##first calculate the cut off score for a certain percent change you are interested in
minscale = 1 #min scale point
maxscale = 7 #max scale point
noq = 20 #number of questions
perchange = .05 #percent change of total you think is important

cutoff = ((maxscale*noq - minscale*noq) * perchange) / noq

IPtost = TOSTpaired.raw(n = length(MnrP),
                         m1 = mean(MnrP),
                         m2 = mean(MrP),
                         sd1 = sd(MnrP),
                         sd2 = sd(MrP),
                         r12 = cor(MnrP, MrP),
                         low_eqbound = -cutoff,
                         high_eqbound = cutoff,
                         alpha = .05)

##bayes factor
IPbf = ttestBF(x = MnrP, y = MrP, paired = T)
```

Table \@ref(tab:Ptable) includes the means and standard deviation of each item from the PIL scale. The item means were compared using a dependent *t*-test matched on item. Item means were significantly different `r apa_print(IPt)$full_result`. The effect size for this difference was small, $d_{av}$ = `r apa(IPd$d, 2)` 95% CI [`r apa(IPd$dlow, 2)`, `r apa(IPd$dhigh, 2)`]. Even though the *t*-test was significant, the TOST test indicated that the difference was within the range of a 5% percent change in item means (`r apa(cutoff, 2)`). The TOST test for lower bound, *t*(19) = `r apa(IPtost$TOST_t1, 2)`, *p* < .001 and the upper bound, *t*(19) = `r apa(IPtost$TOST_t2, 2)`, *p* < .001, suggested that the significant *t*-test may be not be interpreted as a meaningful change on the item means. The BF value for this test indicated 6.86 <0.01%, which is often considered weak evidence for the alternative model. Here, we find mixed results, indicating that randomization may change item means for the PIL. 

### Total Scores 

```{r Ptotal, include = FALSE}
##create total scores
finalnotrandomP$total = apply(finalnotrandomP, 1, sum)
finalrandomP$total = apply(finalrandomP, 1, sum)

##descriptives
MTnrP = mean(finalnotrandomP$total)
MTrP = mean(finalrandomP$total)
SDTnrP = sd(finalnotrandomP$total)
SDTrP = sd(finalrandomP$total)

##t-test
TPt = t.test(finalnotrandomP$total, finalrandomP$total,
             var.equal = T,
             paired = F)

##effect size
TPd = d.ind.t(MTnrP, MTrP, SDTnrP, SDTrP, 
              length(finalnotrandomP$total), length(finalrandomP$total), a = .05)

##toster
##change cut off to the full number of questions
totcutoff = cutoff * noq

TPtost = TOSTtwo.raw(m1 = MTnrP, m2 = MTrP, sd1 = SDTnrP, sd2 = SDTrP,
                 n1 = length(finalnotrandomP$total), n2 = length(finalrandomP$total), 
                 low_eqbound = -totcutoff, high_eqbound = totcutoff, 
                 alpha = .05, var.equal = T)


##bayes factor
TPbf = ttestBF(finalnotrandomP$total, finalrandomP$total, paired = F)
```

```{r Ptable, echo = FALSE, results = 'asis'}
##make a blank table
Ptable = matrix(NA, nrow = 20, ncol = 7)

##create column names
colnames(Ptable) = c("Item","FL-R","FL-NR","M-R","SD-R","M-NR","SD-NR")

##stick in the information you need
Ptable[ , 1] = apa(1:20, 0)
Ptable[ , 2] = apa(FLrandomP$loadings[1:20], 3, F)
Ptable[ , 3] = apa(FLnotP$loadings[1:20], 3, F)
Ptable[ , 4] = apa(MrP, 3)
Ptable[ , 5] = apa(SDrP, 3)
Ptable[ , 6] = apa(MnrP, 3)
Ptable[ , 7] = apa(SDnrP, 3)

##print it (apa)
apa_table(
  Ptable
  , align = c("l", rep("c", 6))
  , caption = "Item Statistics for the PIL Scale",
  note = "FL = Factor Loadings, M = Mean, SD = Standard Deviation, R = Random, NR = Not Random"
)

```

Total scores were created by summing the items for each participant across all twenty PIL questions. The mean total score for nonrandomized testing was *M* = `r apa(MTnrP, 2)` (*SD* = `r apa(SDTnrP, 2)`), while the mean for randomizing testing was *M* = `r apa(MTrP, 2)` (*SD* = `r apa(SDTrP, 2)`). This difference was examined with an independent *t*-test and was not significant, `r apa_print(TPt)$statistic`. The effect size for this difference was negligible, $d_{av}$ = `r apa(TPd$d, 2)` 95% CI [`r apa(TPd$dlow, 2)`, `r apa(IPd$dhigh, 2)`]. We tested if scores were changed by 5% (`r apa(totcutoff, 2)` points), and the TOST test indicated that the lower, *t*(1897) = `r apa(TPtost$TOST_t1, 2)`, *p* < .001 and the upper bound, *t*(1897) = `r apa(TPtost$TOST_t2, 2)`, *p* < .001 were within this area of null change. The BF results also supported the null model, 0.25 <0.01%.

## LPQ Analyses

### Covariance Matrices 

```{r Lcov-analysis, include = FALSE}
###make covariance tables
not_corL = cov(finalnotrandomL, use="pairwise")
rand_corL = cov(finalrandomL, use="pairwise")

#mean tables
notrandommL = unlist(sapply(finalnotrandomL, function(cl) list(means=mean(cl,na.rm=TRUE))))
randommL = unlist(sapply(finalrandomL, function(cl) list(means=mean(cl,na.rm=TRUE))))

##rsme mean, cov, mean, cov
RMSEL = rmse.muS(notrandommL, not_corL, randommL, rand_corL)
##figure out how to do standardized residuals

##formula is cov - cov / sqrt ( var one - var other )
#do var 1 - var 1.1, then var 2 - var 2.2, then take the variance of that vector
not_vL = apply(finalnotrandomL, 2, var)
rand_vL = apply(finalrandomL, 2, var)

##figure out how to report these since you can't just divide by 2 - use csvs to figure out which and how many are over. 
##sum up the standardized residuals that are over 1.96
stdL = (not_corL - rand_corL) / sqrt(var(not_vL - rand_vL))
overL = sum(as.numeric(abs(stdL)) > 1.96)
##print out the standardized residuals to put online
write.csv(stdL, "stdres_not_randomL.csv")

##correlation
CnrL = cor(finalnotrandomL$lpq9_1, finalnotrandomL$lpq11_1)
CrL = cor(finalrandomL$lpq9_1, finalrandomL$lpq11_1)

```

Covariance structure for the LPQ was found to be the same across both randomized and nonrandomized testing, *RMSE* = `r apa(RMSEL, 2, F)`. Standardized residuals indicated that the covariance between items 9 and 11 were significantly different, while item 13 included significantly different variances. The correlation between items 9 (empty life) and 11 (wondering about being alive) for randomized versions was *r* = `r apa(CrL, 2, F)` while the correlation for nonrandomized versions was *r* = `r apa(CnrL, 2, F)`. The variance for item 13 (responsibility) in a randomized version ($SD^2$ = `r apa(rand_corL[13,13], 2, F)`) was smaller than the variance in the nonrandomized version ($SD^2$ = `r apa(not_corL[13,13], 2, F)`).

### Factor Loadings

```{r Lfactor-load, include = FALSE}
FLnotL= fa(finalnotrandomL, nfactors = 1, rotate = "oblimin", fm = "ml", cor = "tet")
FLrandomL = fa(finalrandomL, nfactors = 1, rotate = "oblimin", fm = "ml", cor = "tet") 

##dependent t-test to determine if they are p value different
FLLt = t.test(FLnotL$loadings[1:20], FLrandomL$loadings[1:20], paired = T)

##effect sizes
FLLd = d.dep.t.avg(mean(FLrandomL$loadings[1:20]),mean(FLnotL$loadings[1:20]),
                   sd(FLrandomL$loadings[1:20]), sd(FLnotL$loadings[1:20]),
                   n = length(FLrandomL$loadings[1:20]), a = .05)

FLLdz = d.dep.t.diff(mean(FLrandomL$loadings[1:20] - FLnotL$loadings[1:20]),
                     sd(FLrandomL$loadings[1:20] - FLnotL$loadings[1:20]), 
                     length(FLrandomL$loadings[1:20]), 
                     a = .05)
##toster
##using a cut off score of 0.10 difference because it matches the definition of loadings
FLLtost = TOSTpaired.raw(n = length(FLrandomL$loadings[1:20]),
                         m1 = mean(FLrandomL$loadings[1:20]),
                         m2 = mean(FLnotL$loadings[1:20]),
                         sd1 = sd(FLrandomL$loadings[1:20]),
                         sd2 = sd(FLnotL$loadings[1:20]),
                         r12 = cor(FLrandomL$loadings[1:20], FLnotL$loadings[1:20]),
                         low_eqbound = -0.10,
                         high_eqbound = 0.10,
                         alpha = .05)

##bayes factor
FLLbf = ttestBF(x = FLrandomL$loadings[1:20], y = FLnotL$loadings[1:20], paired = TRUE)
```

Table \@ref(tab:Ltable) includes the factor loadings from the one-factor EFA analysis using tetrachoric correlations. The loadings from randomized and nonrandomized versions were compared using a dependent *t*-test matched on item, which indicated they were not significantly different, `r apa_print(FLLt)$full_result`. The difference found for this test was negligible, $d_{av}$ = `r apa(FLLd$d, 2)` 95% CI [`r apa(FLLd$dlow, 2)`, `r apa(FLLd$dhigh, 2)`]. The TOST test examined if any change was within .10 change, as described earlier. The lower, *t*(19) = `r apa(FLLtost$TOST_t1, 2)`, *p* < .001 and the upper bound, *t*(19) = `r apa(FLLtost$TOST_t2, 2)`, *p* < .001 were both significant, indicating that the change was within the expected change. Further, in support of the null model, the BF was 0.34 ±0.02%. 

### Item Means

```{r Litem, include = FALSE}
##means and sds for the table
MnrL = apply(finalnotrandomL, 2, mean)
SDnrL = apply(finalnotrandomL, 2, sd)
MrL = apply(finalrandomL, 2, mean)
SDrL = apply(finalrandomL, 2, sd)

##t-test
ILt = t.test(MnrL, MrL, paired = T)

##effect size
ILd = d.dep.t.avg(mean(MnrL), mean(MrL), sd(MnrL), sd(MrL), length(MnrL), .05)
ILdz = d.dep.t.diff(mean(MnrL - MrL), sd(MnrL - MrL), length(MnrL), .05)

##toster
##first calculate the cut off score for a certain percent change you are interested in
minscale = 0 #min scale point
maxscale = 1 #max scale point
noq = 20 #number of questions
perchange = .05 #percent change of total you think is important

cutoff = ((maxscale*noq - minscale*noq) * perchange) / noq

ILtost = TOSTpaired.raw(n = length(MnrL),
                         m1 = mean(MnrL),
                         m2 = mean(MrL),
                         sd1 = sd(MnrL),
                         sd2 = sd(MrL),
                         r12 = cor(MnrL, MrL),
                         low_eqbound = -cutoff,
                         high_eqbound = cutoff,
                         alpha = .05)

##bayes factor
ILbf = ttestBF(x = MnrL, y = MrL, paired = T)
```

Means and standard deviations of each item are presented in Table \@ref(tab:Ltable). We again matched items and tested if there was a significant change using a dependent *t*-test. The test was not significant, `r apa_print(ILt)$full_result`, and the corresponding effect size reflects how little these means changed, $d_{av}$ = `r apa(ILd$d, 2)` 95% CI [`r apa(ILd$dlow, 2)`, `r apa(ILd$dhigh, 2)`]. Using a 5% change criterion, items were tested to determine if they changed less than (`r apa(cutoff, 2)`). The TOST test indicated both lower, *t*(19) = `r apa(ILtost$TOST_t1, 2)`, *p* < .001 and the upper bound, *t*(19) = `r apa(ILtost$TOST_t2, 2)`, *p* < .001, were within the null range. The BF also supported the null model, 0.24 ±0.02%.

### Total Scores

```{r Ltotal, include = FALSE}
##create total scores
finalnotrandomL$total = apply(finalnotrandomL, 1, sum)
finalrandomL$total = apply(finalrandomL, 1, sum)

##descriptives
MTnrL = mean(finalnotrandomL$total)
MTrL = mean(finalrandomL$total)
SDTnrL = sd(finalnotrandomL$total)
SDTrL = sd(finalrandomL$total)

##t-test
TLt = t.test(finalnotrandomL$total, finalrandomL$total,
             var.equal = T,
             paired = F)

##effect size
TLd = d.ind.t(MTnrL, MTrL, SDTnrL, SDTrL, 
              length(finalnotrandomL$total), length(finalrandomL$total), a = .05)

##toster
##change cut off to the full number of questions
totcutoff = cutoff * noq

TLtost = TOSTtwo.raw(m1 = MTnrL, m2 = MTrL, sd1 = SDTnrL, sd2 = SDTrL,
                 n1 = length(finalnotrandomL$total), n2 = length(finalrandomL$total), 
                 low_eqbound = -totcutoff, high_eqbound = totcutoff, 
                 alpha = .05, var.equal = T)


##bayes factor
TLbf = ttestBF(finalnotrandomL$total, finalrandomL$total, paired = F)
```

```{r Ltable, echo = FALSE, results = 'asis'}
##make a blank table
Ltable = matrix(NA, nrow = 20, ncol = 7)

##create column names
colnames(Ltable) = c("Item","FL-R","FL-NR","M-R","SD-R","M-NR","SD-NR")

##stick in the information you need
Ltable[ , 1] = apa(1:20, 0)
Ltable[ , 2] = apa(FLrandomL$loadings[1:20], 3, F)
Ltable[ , 3] = apa(FLnotL$loadings[1:20], 3, F)
Ltable[ , 4] = apa(MrL, 3, F)
Ltable[ , 5] = apa(SDrL, 3, F)
Ltable[ , 6] = apa(MnrL, 3, F)
Ltable[ , 7] = apa(SDnrL, 3, F)

##print it (apa)
apa_table(
  Ltable
  , align = c("l", rep("c", 6))
  , caption = "Item Statistics for the LPQ Scale",
  note = "FL = Factor Loadings, M = Mean, SD = Standard Deviation, R = Random, NR = Not Random"
)

```

LPQ total scores were created by summing the items for each participant. The mean total score for randomized testing was *M* = `r apa(MTrL, 2, F)` (*SD* = `r apa(SDTrL, 2, F)`), and the mean for nonrandomized testing was *M* = `r apa(MTnrL, 2, F)` (*SD* = `r apa(SDTnrL, 2, F)`). An independent *t*-test indicated that the testing did not change total score, `r apa_print(TLt)$statistic`. The effect size for this difference was negligible, $d_{av}$ = `r apa(TLd$d, 2)` 95% CI [`r apa(TLd$dlow, 2)`, `r apa(ILd$dhigh, 2)`]. The TOST test indicated that the scores were withing a 5% (`r apa(totcutoff, 2)` points) change, lower: *t*(1627) = `r apa(TLtost$TOST_t1, 2)`, *p* < .001 and upper: *t*(1627) = `r apa(TLtost$TOST_t2, 2)`, *p* < .001. The BF results were in support of the null model, 0.06 ±0.04%.

# Discussion

PIL:
- cov matrices are different, decreasing variance when randomize
- factor loadings say no differences
- item means - they are different (p value, BF) but effect size is small and TOST is sig
- total scores say no differences

LPQ: 
- covariances are the same, decreasing variance, small correlation difference
- factor loadings are same
- item means are the same
- total scores are the same 

doesn't seem to be changing things, so you could go either way 
because we know that items can cause problems, then suggest randomizing so it's evened out at least 
so it's ok if one researcher randomized but another didn't they should still be comparable. 

\newpage

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}